Vocab len:  10032
model config: small_gpt_v9
model config: small_lstm_v4
student_checkpoint: ../tf_ckpts/word_sv_agreement_vp/online_pure_dstl_4_crs_slw_teacher_cl_lstm_em-256_h-256_d-2_hdrop-0.8_indrop-0.2_small_lstm_v4_af_tchr6_student_cl_bert_h-128_d-6_rdrop-0.4_adrop-0.6_indrop-0.2_small_gpt_v9_af_std6
Restored student from ../tf_ckpts/word_sv_agreement_vp/online_pure_dstl_4_crs_slw_teacher_cl_lstm_em-256_h-256_d-2_hdrop-0.8_indrop-0.2_small_lstm_v4_af_tchr6_student_cl_bert_h-128_d-6_rdrop-0.4_adrop-0.6_indrop-0.2_small_gpt_v9_af_std6/ckpt-60
  1/100 [..............................] - ETA: 6:52 - loss: 0.0624 - classification_loss: 0.0264 - sparse_categorical_accuracy: 1.0000  7/100 [=>............................] - ETA: 56s - loss: 0.0181 - classification_loss: 0.0043 - sparse_categorical_accuracy: 1.0000  12/100 [==>...........................] - ETA: 31s - loss: 0.0150 - classification_loss: 0.0027 - sparse_categorical_accuracy: 1.0000 18/100 [====>.........................] - ETA: 19s - loss: 0.0144 - classification_loss: 0.0025 - sparse_categorical_accuracy: 1.0000 23/100 [=====>........................] - ETA: 14s - loss: 0.0135 - classification_loss: 0.0020 - sparse_categorical_accuracy: 1.0000 29/100 [=======>......................] - ETA: 10s - loss: 0.1210 - classification_loss: 0.0557 - sparse_categorical_accuracy: 0.9655 35/100 [=========>....................] - ETA: 8s - loss: 0.1972 - classification_loss: 0.0939 - sparse_categorical_accuracy: 0.9429  41/100 [===========>..................] - ETA: 6s - loss: 0.2021 - classification_loss: 0.0963 - sparse_categorical_accuracy: 0.9268 46/100 [============>.................] - ETA: 5s - loss: 0.1884 - classification_loss: 0.0895 - sparse_categorical_accuracy: 0.9348 51/100 [==============>...............] - ETA: 4s - loss: 0.1800 - classification_loss: 0.0852 - sparse_categorical_accuracy: 0.9412 56/100 [===============>..............] - ETA: 3s - loss: 0.1704 - classification_loss: 0.0805 - sparse_categorical_accuracy: 0.9464 61/100 [=================>............] - ETA: 3s - loss: 0.1586 - classification_loss: 0.0746 - sparse_categorical_accuracy: 0.9508 66/100 [==================>...........] - ETA: 2s - loss: 0.1495 - classification_loss: 0.0700 - sparse_categorical_accuracy: 0.9545 71/100 [====================>.........] - ETA: 1s - loss: 0.1398 - classification_loss: 0.0651 - sparse_categorical_accuracy: 0.9577 76/100 [=====================>........] - ETA: 1s - loss: 0.1318 - classification_loss: 0.0611 - sparse_categorical_accuracy: 0.9605 81/100 [=======================>......] - ETA: 1s - loss: 0.1357 - classification_loss: 0.0631 - sparse_categorical_accuracy: 0.9506 86/100 [========================>.....] - ETA: 0s - loss: 0.1323 - classification_loss: 0.0614 - sparse_categorical_accuracy: 0.9535 91/100 [==========================>...] - ETA: 0s - loss: 0.1306 - classification_loss: 0.0606 - sparse_categorical_accuracy: 0.9560 96/100 [===========================>..] - ETA: 0s - loss: 0.1259 - classification_loss: 0.0582 - sparse_categorical_accuracy: 0.9583100/100 [==============================] - 5s 52ms/step - loss: 0.1266 - classification_loss: 0.0586 - sparse_categorical_accuracy: 0.9600
SpearmanrResult(correlation=0.2537596094612762, pvalue=0.32570795643929895)
(17,)
(17,)
(17,)
(17,)
###############Layer  0 #############
raw blankout
0.6090578540302202 0.2437115806087683
raw inputgrad
(17,) (17,)
-0.3303859946786551 0.40300754981587145
raw grad
(17,) (17,)
0.39643499112725683 0.3723249155658133
************joint blankout
0.27577526751776515 0.19700441735609206
joint grad
(17,) (17,)
0.2449742715936917 0.31422449436693267
joint inputgrad
(17,) (17,)
0.38237885771908836 0.33518963822769077
*************flow
0.27577526751776515 0.19700441735609206
flow grad
(17,) (17,)
0.2449742715936917 0.31422449436693267
flow inputgrad
(17,) (17,)
0.38237885771908836 0.33518963822769077
###############Layer  1 #############
raw blankout
0.24731574767128722 0.4667524978182336
raw inputgrad
(17,) (17,)
-0.24990240187829724 0.44520720953598114
raw grad
(17,) (17,)
0.19756526938843202 0.41145460239238413
************joint blankout
0.32404999188303235 0.23078527810923236
joint grad
(17,) (17,)
0.2785758424955853 0.31818139040053156
joint inputgrad
(17,) (17,)
0.3584823370811704 0.3546437543105801
*************flow
0.32094149403850725 0.22116059616035233
flow grad
(17,) (17,)
0.2751234308922969 0.31668930490467423
flow inputgrad
(17,) (17,)
0.36480807090693385 0.34587095822725633
###############Layer  2 #############
raw blankout
0.14434999097775592 0.46833303861249403
raw inputgrad
(17,) (17,)
-0.1705248562979801 0.4700508779496702
raw grad
(17,) (17,)
0.1646359538126011 0.4201892668072966
************joint blankout
0.37888718077697287 0.2654696405746093
joint grad
(17,) (17,)
0.3130996445261726 0.3308229517719255
joint inputgrad
(17,) (17,)
0.3149065158603734 0.37505406522637036
*************flow
0.43413422327981044 0.29166268332749046
flow grad
(17,) (17,)
0.336788418721444 0.3413321550994554
flow inputgrad
(17,) (17,)
0.2051941039217668 0.4521975591191132
###############Layer  3 #############
raw blankout
0.08376451177051172 0.45211594590191073
raw inputgrad
(17,) (17,)
-0.13887601303086142 0.47119124873203
raw grad
(17,) (17,)
0.16123350316510063 0.4225753975590337
************joint blankout
0.4265643576455219 0.2889343187094496
joint grad
(17,) (17,)
0.34122212623215276 0.3395994890491924
joint inputgrad
(17,) (17,)
0.26425717677336913 0.3996504585192518
*************flow
0.4883982850663176 0.2913299086650483
flow grad
(17,) (17,)
0.3883996720233715 0.3469732088699726
flow inputgrad
(17,) (17,)
0.13237695431281332 0.4647086401053744
###############Layer  4 #############
raw blankout
0.043083894783844695 0.45800690163911945
raw inputgrad
(17,) (17,)
-0.06285730807492766 0.4864002950515276
raw grad
(17,) (17,)
0.1841567245795241 0.4347823026495989
************joint blankout
0.4694033062814636 0.3051896807112794
joint grad
(17,) (17,)
0.36425528254233386 0.3468466265606555
joint inputgrad
(17,) (17,)
0.21013257136010344 0.42634567745748136
*************flow
0.5202129125572321 0.2863485099775262
flow grad
(17,) (17,)
0.4179150303676872 0.34654503073505344
flow inputgrad
(17,) (17,)
0.08689706497285221 0.46828355410603356
###############Layer  5 #############
raw blankout
-0.04020092994827064 0.47270793446407533
raw inputgrad
(17,) (17,)
-0.1266151068935084 0.4820053800957915
raw grad
(17,) (17,)
0.17955756122542296 0.43163044550149615
************joint blankout
0.5015773667663321 0.31218755701539247
joint grad
(17,) (17,)
0.3808635340682136 0.3508778114757854
joint inputgrad
(17,) (17,)
0.16272532463967043 0.44753454526565767
*************flow
0.5418013495593031 0.28067565439343384
flow grad
(17,) (17,)
0.43805645670008403 0.34499326738846026
flow inputgrad
(17,) (17,)
0.058090923666257455 0.46653046700635115
