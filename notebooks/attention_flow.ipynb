{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/samira/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from util import constants\n",
    "from util.config_util import get_model_params, get_task_params, get_train_params\n",
    "from tf2_models.trainer import Trainer\n",
    "from absl import app\n",
    "from absl import flags\n",
    "import pandas as pd\n",
    "\n",
    "from util.models import MODELS\n",
    "from util.tasks import TASKS\n",
    "from notebook_utils import *\n",
    "from attention_graph_util import *\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import matplotlib as mpl\n",
    "from scipy.stats import spearmanr\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:TFDS datasets with text encoding are deprecated and will be removed in a future version. Instead, you should use the plain text version and tokenize the text using `tensorflow_text` (See: https://www.tensorflow.org/tutorials/tensorflow_text/intro#tfdata_example)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab len:  10032\n"
     ]
    }
   ],
   "source": [
    "# Load Task: VP\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "with strategy.scope():\n",
    "    task_name = 'word_sv_agreement_vp'\n",
    "    chkpt_dir='../tf_ckpts'\n",
    "    task_params = get_task_params(batch_size=1)\n",
    "    task = TASKS[task_name](task_params, data_dir='../data')\n",
    "    cl_token = task.sentence_encoder().encode(constants.bos)\n",
    "    tokenizer = task.sentence_encoder()._tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model config: small_gpt_v9\n",
      "model config: small_lstm_v4\n",
      "student_checkpoint: ../tf_ckpts/word_sv_agreement_vp/online_dstl_6_crs_slw_teacher_cl_lstm_em-256_h-256_d-2_hdrop-0.8_indrop-0.2_small_lstm_v4_af_tchr5_student_cl_bert_h-128_d-6_rdrop-0.4_adrop-0.6_indrop-0.2_small_gpt_v9_af_std5\n",
      "Restored student from ../tf_ckpts/word_sv_agreement_vp/online_dstl_6_crs_slw_teacher_cl_lstm_em-256_h-256_d-2_hdrop-0.8_indrop-0.2_small_lstm_v4_af_tchr5_student_cl_bert_h-128_d-6_rdrop-0.4_adrop-0.6_indrop-0.2_small_gpt_v9_af_std5/ckpt-60\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 5s 46ms/step - loss: 0.1045 - classification_loss: 0.0948 - sparse_categorical_accuracy: 0.9700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.10447468552738429, 0.09482438, 0.97]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load and evaluate a model\n",
    "\n",
    "config = {'student_exp_name':'af_std5',\n",
    "        'teacher_exp_name':'af_tchr5',\n",
    "        'teacher_config':'small_lstm_v4',\n",
    "        'student_model':'cl_bert',\n",
    "        'teacher_model':'cl_lstm',\n",
    "        'student_config':'small_gpt_v9',\n",
    "        'distill_config':'dstl_6_crs_slw',\n",
    "        'distill_mode':'online',\n",
    "        'chkpt_dir':'../tf_ckpts',}\n",
    "\n",
    "hparams=get_model_params(task, config['student_model'], config['student_config'])    \n",
    "hparams.output_attentions = True\n",
    "hparams.output_embeddings = True\n",
    "hparams.output_hidden_states = True\n",
    "\n",
    "with strategy.scope():\n",
    "    model, ckpnt = get_student_model(config, task, hparams, cl_token)\n",
    "\n",
    "\n",
    "model.evaluate(task.valid_dataset, steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "99it [05:18,  3.23s/it]"
     ]
    }
   ],
   "source": [
    "all_gradient_scores = []\n",
    "all_inputgradient_scores = []\n",
    "all_examples_x = []\n",
    "all_examples_y = []\n",
    "all_examples_attentions = []\n",
    "all_examples_correct_probs = []\n",
    "all_examples_correct_index_probs_diff = []\n",
    "non_reshaped = []\n",
    "n_batches = 100\n",
    "prob_fn = task.get_probs_fn()\n",
    "for x, y in tqdm(task.valid_dataset):\n",
    "    \n",
    "    #Manually add cls token:\n",
    "    batch_size = len(x)\n",
    "    cl_token = tf.reshape(tf.convert_to_tensor(cl_token[0], dtype=tf.int64)[None], (-1,1))\n",
    "    cl_tokens = tf.tile(cl_token, (batch_size, 1))\n",
    "    x = tf.concat([cl_tokens, x], axis=-1)\n",
    "    \n",
    "    # Save examples\n",
    "    all_examples_x.extend(x)\n",
    "    all_examples_y.extend(y)\n",
    "    \n",
    "    # Get gradient scores\n",
    "    input_embeddings, input_shape, padding_mask, past = model.get_input_embeddings(x, add_cls=False, training=False)\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(input_embeddings)\n",
    "        outputs = model.call_with_embeddings(input_embeddings, input_shape, padding_mask, past)\n",
    "        logits = outputs[0]\n",
    "        probs = tf.nn.softmax(logits, axis=-1)\n",
    "        diff_probs = probs[:,0] - probs[:,1]\n",
    "        \n",
    "    grads = tape.gradient(diff_probs, input_embeddings)\n",
    "    grad_scores = tf.abs(tf.reduce_sum(grads, axis=-1))\n",
    "    input_grad_scores = tf.abs(tf.reduce_sum(tf.multiply(grads, input_embeddings), axis=-1))\n",
    "    \n",
    "    \n",
    "    all_gradient_scores.extend(grad_scores)\n",
    "    all_inputgradient_scores.extend(input_grad_scores)\n",
    "    \n",
    "    # Call the model to the get the logits and attentions\n",
    "    outputs = model.detailed_call(x, add_cls=False, training=False)\n",
    "    main_logits = outputs[0]\n",
    "    \n",
    "    # Get the probability of the correct class\n",
    "    main_probs = prob_fn(main_logits, y, 1)\n",
    "    main_diff_probs = main_probs[:,y[0]] #- main_probs[:,1]\n",
    "    \n",
    "    attentions_of_all_layers = outputs[6]\n",
    "\n",
    "    \n",
    "    # Reshape the attention matrix to: [batchsize, layers, heads, length, length]\n",
    "    attentions_of_all_layers = [att.numpy() for att in attentions_of_all_layers]\n",
    "    attentions_of_all_layers = np.transpose(np.asarray(attentions_of_all_layers), (1,0,2,3,4))\n",
    "    \n",
    "    # Save attentions and correct probs\n",
    "    all_examples_attentions.extend(attentions_of_all_layers)\n",
    "\n",
    "    \n",
    "    # Repeating examples and replacing one token at a time with unk\n",
    "    batch_size = tf.shape(x)[0]\n",
    "    max_len = x.shape[1]\n",
    "    \n",
    "    # Repeat each example 'max_len' times\n",
    "    extended_x = tf.reshape(tf.tile(x[:,None,...], (1,max_len-1, 1)),(-1,x.shape[-1]))\n",
    "    extended_y = tf.reshape(tf.tile(y[:,None],(1,max_len-1)),(-1,))\n",
    "    extended_diff_probs = tf.reshape(tf.tile(main_diff_probs[:,None],(1,max_len-1)),(-1,))\n",
    "    \n",
    "    # Create unk sequences and unk mask\n",
    "    unktoken = task.databuilder.sentence_encoder().encode(constants.unk)\n",
    "    unks = unktoken * tf.eye(max_len)[:-1]\n",
    "    unks = tf.cast(tf.tile(unks, (batch_size, 1)), dtype=tf.int64)\n",
    "    unk_mask =  tf.cast((unktoken - unks)/unktoken, dtype=tf.int64)\n",
    "  \n",
    "    # Replace one token in each repeatition with unk\n",
    "    extended_x = extended_x * unk_mask + unks\n",
    "    \n",
    "    # Get the new output\n",
    "    extended_logits = model(extended_x, training=False, add_cls=False)\n",
    "    extended_probs = prob_fn(extended_logits, extended_y, 1)\n",
    "\n",
    "    \n",
    "    extended_diff_probs = extended_probs[:,y[0]] # - extended_probs[:,1]\n",
    "    # Save the difference in the probability predicted for the correct class\n",
    "    diffs = abs(extended_diff_probs - main_diff_probs)\n",
    "    diffs = tf.reshape(diffs,(batch_size,-1,1))\n",
    "    all_examples_correct_index_probs_diff.extend(diffs)\n",
    "    \n",
    "    \n",
    "    n_batches -= 1\n",
    "    if n_batches <= 0:\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17,)\n",
      "(16, 1)\n",
      "(6, 8, 17, 17)\n"
     ]
    }
   ],
   "source": [
    "print(all_examples_x[0].shape)\n",
    "print(all_examples_correct_index_probs_diff[0].shape)\n",
    "print(all_examples_attentions[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_spearmanr(x, y):\n",
    "    \"\"\" `x`, `y` --> pd.Series\"\"\"\n",
    "    x = pd.Series(x)\n",
    "    y = pd.Series(y)\n",
    "    assert x.shape == y.shape\n",
    "    rx = x.rank(method='dense')\n",
    "    ry = y.rank(method='dense')\n",
    "    d = rx - ry\n",
    "    dsq = np.sum(np.square(d))\n",
    "    n = x.shape[0]\n",
    "    coef = 1. - (6. * dsq) / (n * (n**2 - 1.))\n",
    "    return [coef]\n",
    "\n",
    "def get_raw_att_relevance(full_att_mat, input_tokens, layer=-1):\n",
    "    cls_index = 0\n",
    "    raw_rel = full_att_mat[layer].sum(axis=0)[cls_index]/full_att_mat[layer].sum(axis=0)[cls_index].sum()\n",
    "    \n",
    "    return raw_rel\n",
    "\n",
    "\n",
    "def get_joint_relevance(full_att_mat, input_tokens, layer=-1):\n",
    "    att_sum_heads =  full_att_mat.sum(axis=1)/full_att_mat.shape[1]\n",
    "    joint_attentions = compute_joint_attention(att_sum_heads, add_residual=True)\n",
    "    relevance_attentions = joint_attentions[layer][0]\n",
    "    return relevance_attentions\n",
    "\n",
    "\n",
    "def get_flow_relevance(full_att_mat, input_tokens, layer):\n",
    "    \n",
    "    input_tokens = input_tokens\n",
    "    res_att_mat = full_att_mat.sum(axis=1)/full_att_mat.shape[1]\n",
    "    res_att_mat = res_att_mat + np.eye(res_att_mat.shape[1])[None,...]\n",
    "    res_att_mat = res_att_mat / res_att_mat.sum(axis=-1)[...,None]\n",
    "\n",
    "    res_adj_mat, res_labels_to_index = get_adjmat(mat=res_att_mat, input_tokens=input_tokens)\n",
    "    \n",
    "    A = res_adj_mat\n",
    "    res_G=nx.from_numpy_matrix(A, create_using=nx.DiGraph())\n",
    "    for i in np.arange(A.shape[0]):\n",
    "        for j in np.arange(A.shape[1]):\n",
    "            nx.set_edge_attributes(res_G, {(i,j): A[i,j]}, 'capacity')\n",
    "\n",
    "\n",
    "    output_nodes = ['L'+str(layer+1)+'_0']\n",
    "    input_nodes = []\n",
    "    for key in res_labels_to_index:\n",
    "        if res_labels_to_index[key] < full_att_mat.shape[-1]:\n",
    "            input_nodes.append(key)\n",
    "    \n",
    "    flow_values = compute_node_flow(res_G, res_labels_to_index, input_nodes, output_nodes, length=full_att_mat.shape[-1])\n",
    "    \n",
    "    length = full_att_mat.shape[-1]\n",
    "    final_layer_attention = flow_values[(layer+1)*length:,layer*length:(layer+1)*length]\n",
    "    cls_index = 0\n",
    "    relevance_attention_flow = final_layer_attention[cls_index]\n",
    "\n",
    "    return relevance_attention_flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      " 27%|██▋       | 27/100 [00:00<00:00, 267.18it/s]\u001b[A\n",
      " 52%|█████▏    | 52/100 [00:00<00:00, 259.68it/s]\u001b[A\n",
      " 76%|███████▌  | 76/100 [00:00<00:00, 252.80it/s]\u001b[A\n",
      "100%|██████████| 100/100 [00:00<00:00, 245.10it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      " 28%|██▊       | 28/100 [00:00<00:00, 265.32it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpearmanrResult(correlation=0.2389191550462824, pvalue=0.3557315679984201)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 53%|█████▎    | 53/100 [00:00<00:00, 259.69it/s]\u001b[A\n",
      "100%|██████████| 100/100 [00:00<00:00, 256.83it/s][A\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      " 28%|██▊       | 28/100 [00:00<00:00, 262.73it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 53%|█████▎    | 53/100 [00:00<00:00, 257.45it/s]\u001b[A\n",
      "100%|██████████| 100/100 [00:00<00:00, 253.90it/s][A\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      " 27%|██▋       | 27/100 [00:00<00:00, 266.72it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 51%|█████     | 51/100 [00:00<00:00, 257.51it/s]\u001b[A\n",
      "100%|██████████| 100/100 [00:00<00:00, 249.49it/s][A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(np.arange(len(all_examples_x))):\n",
    "        tokens = task.sentence_encoder().decode(all_examples_x[i]).split()\n",
    "        length = len(tokens)\n",
    "        all_gradient_scores[i] = all_gradient_scores[i][:length]\n",
    "        all_inputgradient_scores[i] = all_inputgradient_scores[i][:length]\n",
    "\n",
    "print(spearmanr(all_gradient_scores[0], all_inputgradient_scores[0]))\n",
    "\n",
    "all_examples_raw_relevance = {}\n",
    "for l in np.arange(5,6):\n",
    "    all_examples_raw_relevance[l] = []\n",
    "    for i in tqdm(np.arange(len(all_examples_x))):\n",
    "        tokens = task.sentence_encoder().decode(all_examples_x[i]).split()\n",
    "        length = len(tokens)\n",
    "        attention_relevance = get_raw_att_relevance(all_examples_attentions[i][...,:length, :length], tokens, layer=l)\n",
    "        all_examples_raw_relevance[l].append(np.asarray(attention_relevance))\n",
    "\n",
    "print(all_examples_raw_relevance[5][0].shape)\n",
    "\n",
    "\n",
    "all_examples_blankout_relevance = {}\n",
    "for l in np.arange(5,6):\n",
    "    all_examples_blankout_relevance[l] = []\n",
    "    for i in tqdm(np.arange(len(all_examples_x))):\n",
    "        tokens = task.sentence_encoder().decode(all_examples_x[i]).split()\n",
    "        length = len(tokens)\n",
    "        b_list = np.asarray(list(all_examples_correct_index_probs_diff[i].numpy().squeeze()) + [0])\n",
    "        all_examples_blankout_relevance[l].append(b_list)\n",
    "print(all_examples_blankout_relevance[5][0].shape)\n",
    "\n",
    "\n",
    "all_examples_joint_relevance = {}\n",
    "for l in np.arange(5,6):\n",
    "    all_examples_joint_relevance[l] = []\n",
    "    for i in tqdm(np.arange(len(all_examples_x))):\n",
    "        tokens = task.sentence_encoder().decode(all_examples_x[i]).split()\n",
    "        length = len(tokens)\n",
    "        attention_relevance = get_joint_relevance(all_examples_attentions[i][...,:length, :length], tokens, layer=l)\n",
    "        all_examples_joint_relevance[l].append(np.asarray(attention_relevance))\n",
    "print(all_examples_joint_relevance[5][0].shape)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0]\n",
      "SpearmanrResult(correlation=1.0, pvalue=0.0)\n",
      "[1.85310841e-04 1.36446953e-03 9.82026458e-01 0.00000000e+00]\n",
      "[0.19707008 0.21827809 0.42872978 0.15592205]\n",
      "17 17\n"
     ]
    }
   ],
   "source": [
    "print(my_spearmanr(all_examples_blankout_relevance[5][8], all_examples_joint_relevance[5][8]))\n",
    "print(spearmanr(all_examples_blankout_relevance[5][8], all_examples_joint_relevance[5][8]))\n",
    "\n",
    "print(all_examples_blankout_relevance[5][8])\n",
    "print(all_examples_joint_relevance[5][8])\n",
    "\n",
    "\n",
    "print(len(all_examples_blankout_relevance[5][0]), len(all_examples_joint_relevance[5][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|          | 1/100 [00:04<07:54,  4.80s/it]\u001b[A\n",
      "  2%|▏         | 2/100 [00:05<05:46,  3.54s/it]\u001b[A\n",
      "  4%|▍         | 4/100 [00:05<04:04,  2.54s/it]\u001b[A\n",
      "  5%|▌         | 5/100 [00:06<03:00,  1.90s/it]\u001b[A\n",
      "  8%|▊         | 8/100 [00:06<02:06,  1.37s/it]\u001b[A\n",
      " 10%|█         | 10/100 [00:06<01:28,  1.02it/s]\u001b[A\n",
      " 11%|█         | 11/100 [00:17<05:54,  3.98s/it]\u001b[A\n",
      " 13%|█▎        | 13/100 [00:17<04:03,  2.80s/it]\u001b[A\n",
      " 15%|█▌        | 15/100 [00:51<09:59,  7.05s/it]\u001b[A\n",
      " 16%|█▌        | 16/100 [00:52<07:04,  5.06s/it]\u001b[A\n",
      " 18%|█▊        | 18/100 [00:52<04:54,  3.59s/it]\u001b[A\n",
      " 20%|██        | 20/100 [00:52<03:22,  2.53s/it]\u001b[A\n",
      " 21%|██        | 21/100 [00:53<02:29,  1.89s/it]\u001b[A\n",
      " 23%|██▎       | 23/100 [00:53<01:49,  1.42s/it]\u001b[A\n",
      " 24%|██▍       | 24/100 [01:18<10:45,  8.50s/it]\u001b[A\n",
      " 25%|██▌       | 25/100 [01:18<07:29,  6.00s/it]\u001b[A\n",
      " 26%|██▌       | 26/100 [01:19<05:12,  4.23s/it]\u001b[A\n",
      " 28%|██▊       | 28/100 [01:24<04:34,  3.81s/it]\u001b[A\n",
      " 31%|███       | 31/100 [04:10<22:06, 19.23s/it]\u001b[A\n",
      " 32%|███▏      | 32/100 [04:10<15:18, 13.51s/it]\u001b[A\n",
      " 34%|███▍      | 34/100 [04:10<10:25,  9.48s/it]\u001b[A\n",
      " 35%|███▌      | 35/100 [04:11<07:26,  6.87s/it]\u001b[A\n",
      " 36%|███▌      | 36/100 [04:18<07:29,  7.02s/it]\u001b[A\n",
      " 39%|███▉      | 39/100 [04:19<05:01,  4.95s/it]\u001b[A\n",
      " 40%|████      | 40/100 [04:19<03:30,  3.51s/it]\u001b[A\n",
      " 41%|████      | 41/100 [04:19<02:31,  2.58s/it]\u001b[A\n",
      " 42%|████▏     | 42/100 [04:35<06:19,  6.55s/it]\u001b[A\n",
      " 44%|████▍     | 44/100 [04:35<04:20,  4.65s/it]\u001b[A\n",
      " 47%|████▋     | 47/100 [04:36<02:53,  3.27s/it]\u001b[A\n",
      " 48%|████▊     | 48/100 [04:36<02:03,  2.37s/it]\u001b[A\n",
      " 50%|█████     | 50/100 [04:45<02:30,  3.02s/it]\u001b[A\n",
      " 52%|█████▏    | 52/100 [04:45<01:44,  2.17s/it]\u001b[A\n",
      " 53%|█████▎    | 53/100 [04:51<02:32,  3.25s/it]\u001b[A\n",
      " 54%|█████▍    | 54/100 [04:51<01:46,  2.32s/it]\u001b[A\n",
      " 57%|█████▋    | 57/100 [04:52<01:14,  1.73s/it]\u001b[A\n",
      " 60%|██████    | 60/100 [05:43<04:09,  6.24s/it]\u001b[A\n",
      " 61%|██████    | 61/100 [05:43<02:58,  4.57s/it]\u001b[A\n",
      " 62%|██████▏   | 62/100 [05:44<02:06,  3.32s/it]\u001b[A\n",
      " 64%|██████▍   | 64/100 [05:44<01:25,  2.38s/it]\u001b[A\n",
      " 65%|██████▌   | 65/100 [05:53<02:31,  4.32s/it]\u001b[A\n",
      " 66%|██████▌   | 66/100 [05:54<01:55,  3.40s/it]\u001b[A\n",
      " 67%|██████▋   | 67/100 [05:54<01:20,  2.43s/it]\u001b[A\n",
      " 70%|███████   | 70/100 [05:55<00:54,  1.82s/it]\u001b[A\n",
      " 71%|███████   | 71/100 [05:57<00:51,  1.76s/it]\u001b[A\n",
      " 72%|███████▏  | 72/100 [05:59<00:52,  1.86s/it]\u001b[A\n",
      " 73%|███████▎  | 73/100 [06:00<00:38,  1.42s/it]\u001b[A\n",
      " 78%|███████▊  | 78/100 [06:00<00:22,  1.04s/it]\u001b[A\n",
      " 81%|████████  | 81/100 [06:16<00:43,  2.28s/it]\u001b[A\n",
      " 83%|████████▎ | 83/100 [06:34<01:14,  4.38s/it]\u001b[A\n",
      " 84%|████████▍ | 84/100 [06:35<00:51,  3.22s/it]\u001b[A\n",
      " 86%|████████▌ | 86/100 [06:36<00:32,  2.35s/it]\u001b[A\n",
      " 88%|████████▊ | 88/100 [06:36<00:19,  1.66s/it]\u001b[A\n",
      " 89%|████████▉ | 89/100 [09:25<09:32, 52.01s/it]\u001b[A\n",
      " 92%|█████████▏| 92/100 [09:39<05:02, 37.75s/it]\u001b[A\n",
      " 95%|█████████▌| 95/100 [09:39<02:12, 26.47s/it]\u001b[A\n",
      " 97%|█████████▋| 97/100 [09:44<00:57, 19.24s/it]\u001b[A\n",
      "100%|██████████| 100/100 [09:45<00:00,  5.86s/it][A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_examples_flow_relevance = {}\n",
    "for l in np.arange(5,6):\n",
    "    all_examples_flow_relevance[l] = []\n",
    "    for i in tqdm(np.arange(len(all_examples_x))):\n",
    "        tokens = task.sentence_encoder().decode(all_examples_x[i]).split()\n",
    "        length = len(tokens)\n",
    "        attention_relevance = get_flow_relevance(all_examples_attentions[i][...,:length, :length], tokens, layer=l)\n",
    "        all_examples_flow_relevance[l].append(np.asarray(attention_relevance))\n",
    "\n",
    "print(all_examples_flow_relevance[5][0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############Layer  5 #############\n",
      "raw blankout\n",
      "0.3167093330725704 0.4106444604701348\n",
      "raw inputgrad\n",
      "(17,) (17,)\n",
      "-0.1385616779444097 0.4352289444565447\n",
      "raw grad\n",
      "(17,) (17,)\n",
      "0.43226225077182684 0.4320430356223115\n",
      "************joint blankout\n",
      "0.7703635817931628 0.2244332513898077\n",
      "joint grad\n",
      "(17,) (17,)\n",
      "0.5426587720902303 0.3527589195396648\n",
      "joint inputgrad\n",
      "(17,) (17,)\n",
      "-0.05296976136270858 0.41151501688879366\n",
      "*************flow\n",
      "0.7638984109636371 0.18900946091084037\n",
      "flow grad\n",
      "(17,) (17,)\n",
      "0.5975126771142749 0.3145362531640321\n",
      "flow inputgrad\n",
      "(17,) (17,)\n",
      "-0.034039260018525745 0.33470291115685585\n"
     ]
    }
   ],
   "source": [
    "\n",
    "raw_sps_blank = []\n",
    "raw_sps_grad = []\n",
    "raw_sps_inputgrad = []\n",
    "\n",
    "joint_sps_blank = []\n",
    "joint_sps_grad = []\n",
    "joint_sps_inputgrad = []\n",
    "\n",
    "flow_sps_blank = []\n",
    "flow_sps_grad = []\n",
    "flow_sps_inputgrad = []\n",
    "\n",
    "\n",
    "for l in np.arange(5,6):\n",
    "    print(\"###############Layer \",l, \"#############\")\n",
    "    print('raw blankout')\n",
    "    for i in np.arange(len(all_examples_x)):\n",
    "        sp = my_spearmanr(all_examples_raw_relevance[l][i],all_examples_blankout_relevance[l][i])\n",
    "        if not math.isnan(sp[0]):\n",
    "            raw_sps_blank.append(sp[0])\n",
    "        else:\n",
    "            raw_sps_blank.append(0)\n",
    "        \n",
    "    print(np.mean(raw_sps_blank), np.std(raw_sps_blank))\n",
    "    \n",
    "    \n",
    "    print('raw inputgrad')\n",
    "    print(all_examples_raw_relevance[l][0].shape, all_inputgradient_scores[0].shape)\n",
    "    for i in np.arange(len(all_examples_x)):\n",
    "        sp = my_spearmanr(all_examples_raw_relevance[l][i],all_inputgradient_scores[i])\n",
    "        if not math.isnan(sp[0]):\n",
    "            raw_sps_inputgrad.append(sp[0])\n",
    "        else:\n",
    "            raw_sps_inputgrad.append(0)\n",
    "        \n",
    "    print(np.mean(raw_sps_inputgrad), np.std(raw_sps_inputgrad))\n",
    "    \n",
    "    print('raw grad')\n",
    "    print(all_examples_raw_relevance[l][0].shape, all_gradient_scores[0].shape)\n",
    "    for i in np.arange(len(all_examples_x)):\n",
    "        sp = my_spearmanr(all_examples_raw_relevance[l][i],all_gradient_scores[i])\n",
    "        if not math.isnan(sp[0]):\n",
    "            raw_sps_grad.append(sp[0])\n",
    "        else:\n",
    "            raw_sps_grad.append(0)\n",
    "        \n",
    "    print(np.mean(raw_sps_grad), np.std(raw_sps_grad))\n",
    "    \n",
    "    print('************joint blankout')\n",
    "    for i in np.arange(len(all_examples_x)):\n",
    "        sp = my_spearmanr(all_examples_joint_relevance[l][i],all_examples_blankout_relevance[l][i])\n",
    "        if not math.isnan(sp[0]):\n",
    "            joint_sps_blank.append(sp[0])\n",
    "        else:\n",
    "            joint_sps_blank.append(0)\n",
    "        \n",
    "    print(np.mean(joint_sps_blank), np.std(joint_sps_blank))\n",
    "    \n",
    "    print('joint grad')\n",
    "    print(all_examples_joint_relevance[l][0].shape, all_gradient_scores[0].shape)\n",
    "    for i in np.arange(len(all_examples_x)):\n",
    "        sp = my_spearmanr(all_examples_joint_relevance[l][i],all_gradient_scores[i])\n",
    "        if not math.isnan(sp[0]):\n",
    "            joint_sps_grad.append(sp[0])\n",
    "        else:\n",
    "            joint_sps_grad.append(0)\n",
    "        \n",
    "    print(np.mean(joint_sps_grad), np.std(joint_sps_grad))\n",
    "    \n",
    "    print('joint inputgrad')\n",
    "    print(all_examples_joint_relevance[l][0].shape, all_inputgradient_scores[0].shape)\n",
    "    for i in np.arange(len(all_examples_x)):\n",
    "        sp = my_spearmanr(all_examples_joint_relevance[l][i],all_inputgradient_scores[i])\n",
    "        if not math.isnan(sp[0]):\n",
    "            joint_sps_inputgrad.append(sp[0])\n",
    "        else:\n",
    "            joint_sps_inputgrad.append(0)\n",
    "        \n",
    "    print(np.mean(joint_sps_inputgrad), np.std(joint_sps_inputgrad))\n",
    "    \n",
    "    print('*************flow')\n",
    "    for i in np.arange(len(all_examples_x)):\n",
    "        sp = my_spearmanr(all_examples_flow_relevance[l][i],all_examples_blankout_relevance[l][i])\n",
    "        \n",
    "        if not math.isnan(sp[0]):\n",
    "            flow_sps_blank.append(sp[0])\n",
    "        else:\n",
    "            flow_sps_blank.append(0)\n",
    "        \n",
    "    print(np.mean(flow_sps_blank), np.std(flow_sps_blank))\n",
    "  \n",
    "    print('flow grad')\n",
    "    print(all_examples_joint_relevance[l][0].shape, all_gradient_scores[0].shape)\n",
    "    for i in np.arange(len(all_examples_x)):\n",
    "        sp = my_spearmanr(all_examples_flow_relevance[l][i],all_gradient_scores[i])\n",
    "        if not math.isnan(sp[0]):\n",
    "            flow_sps_grad.append(sp[0])\n",
    "        else:\n",
    "            flow_sps_grad.append(0)\n",
    "        \n",
    "    print(np.mean(flow_sps_grad), np.std(flow_sps_grad))\n",
    "    \n",
    "    print('flow inputgrad')\n",
    "    print(all_examples_joint_relevance[l][0].shape, all_inputgradient_scores[0].shape)\n",
    "    for i in np.arange(len(all_examples_x)):\n",
    "        sp = my_spearmanr(all_examples_flow_relevance[l][i],all_inputgradient_scores[i])\n",
    "        if not math.isnan(sp[0]):\n",
    "            flow_sps_inputgrad.append(sp[0])\n",
    "        else:\n",
    "            flow_sps_inputgrad.append(0)\n",
    "        \n",
    "    print(np.mean(flow_sps_inputgrad), np.std(flow_sps_inputgrad))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_examples_x))\n",
    "for i in np.arange(len(all_examples_x)):\n",
    "    print(all_examples_x[i].shape)\n",
    "    print(all_examples_flow_relevance[i].shape)\n",
    "    print(all_examples_raw_relevance[i].shape)\n",
    "    print(all_examples_joint_relevance[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean([my_spearmanr(all_examples_flow_relevance[5][i], all_examples_blankout_relevance[5][i]) for i in np.arange(len(all_examples_x))]))\n",
    "print(np.mean([my_spearmanr(all_examples_joint_relevance[5][i], all_examples_blankout_relevance[5][i]) for i in np.arange(len(all_examples_x))]))\n",
    "print(np.mean([my_spearmanr(all_examples_blankout_relevance[l][i], all_examples_blankout_relevance[l][i]) for i in np.arange(len(all_examples_x))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean([spearmanr(all_examples_blankout_relevance[l][i], all_examples_blankout_relevance[l][i]) for i in np.arange(len(all_examples_x))]))\n",
    "\n",
    "for i in np.arange(len(all_examples_x)):\n",
    "    print(all_examples_blankout_relevance[1][i])\n",
    "    sp = spearmanr(all_examples_blankout_relevance[1][i], all_examples_blankout_relevance[1][i])\n",
    "    print(sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([spearmanr(all_examples_blankout_relevance[l][i], all_examples_blankout_relevance[l][i])[0] for i in np.arange(len(all_examples_x))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = model\n",
    "\n",
    "sentences = []\n",
    "all_atts = []\n",
    "all_main_probs = []\n",
    "all_index_probs = []\n",
    "prob_fn = task.get_probs_fn()\n",
    "count = 0\n",
    "for x, y in iter(task.test_dataset):\n",
    "    max_len = x.shape[1]\n",
    "    all_outputs = model_1.detailed_call(x, training=False)\n",
    "    print(len(all_outputs))\n",
    "    main_logits = all_outputs[0]\n",
    "    attentions = all_outputs[6]\n",
    "    _attentions = [att.numpy() for att in attentions]\n",
    "    attentions = np.transpose(np.asarray(_attentions), (1,0,2,3,4))\n",
    "    main_probs = prob_fn(main_logits, y, 1)\n",
    "    batch_indexes = tf.range(len(y), dtype=tf.int64)\n",
    "    indexes = tf.concat([batch_indexes[:,None], y[:,None]], axis=1)\n",
    "    correct_main_probs = tf.gather_nd(main_probs, indexes).numpy()\n",
    "\n",
    "    sentences.append(task.databuilder.sentence_encoder().decode(x[0]))\n",
    "    all_atts.extend(attentions)\n",
    "    all_main_probs.extend(correct_main_probs)\n",
    "    all_index_probs.append([])\n",
    "    print(count, max_len)\n",
    "    # This loop can be optimized so that there is only one call...\n",
    "    new_xz = []\n",
    "    for i in np.arange(1,max_len-1):\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        unktoken = task.databuilder.sentence_encoder().encode(constants.unk)\n",
    "        unk = tf.reshape(tf.convert_to_tensor(unktoken, dtype=tf.int64)[None], (-1,1))\n",
    "        unks = tf.tile(unk, (batch_size, 1))\n",
    "        new_x = tf.concat([x[:,:i],unks, x[:,i+1:]], axis=-1)\n",
    "        new_xz.extend(new_x)\n",
    "    \n",
    "    new_x = np.asarray(new_xz)\n",
    "    logits = model_1(new_x, training=False)\n",
    "    probs = prob_fn(logits, y, 1)\n",
    "    \n",
    "    batch_indexes = tf.range(len(probs), dtype=tf.int64)\n",
    "    yz = tf.tile(y, (len(probs),))\n",
    "\n",
    "    indexes = tf.concat([batch_indexes[:,None], yz[:,None]], axis=1)\n",
    "    \n",
    "    correct_probs = tf.gather_nd(probs, indexes).numpy()\n",
    "    all_index_probs[-1].extend(abs(correct_main_probs - correct_probs))\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer:  0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'all_index_probs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-70a7c8ada968>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mf_corels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_pvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_index_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0maa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mall_index_probs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mbb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_flow_relevance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_atts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'<cls>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_index_probs' is not defined"
     ]
    }
   ],
   "source": [
    "def spearmanr(x, y):\n",
    "    \"\"\" `x`, `y` --> pd.Series\"\"\"\n",
    "    x = pd.Series(x)\n",
    "    y = pd.Series(y)\n",
    "    assert x.shape == y.shape\n",
    "    rx = x.rank(method='dense')\n",
    "    ry = y.rank(method='dense')\n",
    "    d = rx - ry\n",
    "    dsq = np.sum(np.square(d))\n",
    "    n = x.shape[0]\n",
    "    coef = 1. - (6. * dsq) / (n * (n**2 - 1.))\n",
    "    return coef\n",
    "\n",
    "def get_raw_att_relevance(full_att_mat, input_tokens, layer=-1):\n",
    "    cls_index = 0\n",
    "    return full_att_mat[layer].sum(axis=0)[cls_index]\n",
    "    \n",
    "\n",
    "def compute_node_flow(G, labels_to_index, input_nodes, output_nodes,length):\n",
    "    number_of_nodes = len(labels_to_index)\n",
    "    flow_values=np.zeros((number_of_nodes,number_of_nodes))\n",
    "    for key in output_nodes:\n",
    "        if key not in input_nodes:\n",
    "            current_layer = int(labels_to_index[key] / length)\n",
    "            pre_layer = current_layer - 1\n",
    "            u = labels_to_index[key]\n",
    "            for inp_node_key in input_nodes:\n",
    "                v = labels_to_index[inp_node_key]\n",
    "                flow_value = nx.maximum_flow_value(G,u,v)\n",
    "                flow_values[u][pre_layer*length+v ] = flow_value\n",
    "            flow_values[u] /= flow_values[u].sum()\n",
    "            \n",
    "    return flow_values\n",
    "\n",
    "def get_flow_relevance(full_att_mat, input_tokens, layer):\n",
    "    \n",
    "    res_att_mat = full_att_mat.sum(axis=1)/8\n",
    "    res_att_mat = res_att_mat + np.eye(res_att_mat.shape[1])[None,...]\n",
    "    res_att_mat = res_att_mat / res_att_mat.sum(axis=-1)[...,None]\n",
    "\n",
    "    res_adj_mat, res_labels_to_index = get_adjmat(mat=res_att_mat, input_tokens=input_tokens)\n",
    "    \n",
    "    A = res_adj_mat\n",
    "    res_G=nx.from_numpy_matrix(A, create_using=nx.DiGraph())\n",
    "    for i in np.arange(A.shape[0]):\n",
    "        for j in np.arange(A.shape[1]):\n",
    "            nx.set_edge_attributes(res_G, {(i,j): A[i,j]}, 'capacity')\n",
    "\n",
    "\n",
    "    output_nodes = []\n",
    "    input_nodes = []\n",
    "    for key in res_labels_to_index:\n",
    "        if 'L'+str(layer+1) in key:\n",
    "            output_nodes.append(key)\n",
    "        if res_labels_to_index[key] < full_att_mat.shape[-1]:\n",
    "            input_nodes.append(key)\n",
    "    \n",
    "    flow_values = compute_node_flow(res_G, res_labels_to_index, input_nodes, output_nodes, length=full_att_mat.shape[-1])\n",
    "    \n",
    "    n_layers = full_att_mat.shape[0]\n",
    "    length = full_att_mat.shape[-1]\n",
    "    final_layer_attention_raw = flow_values[(layer+1)*length:,layer*length:(layer+1)*length]\n",
    "    cls_index = 0\n",
    "    relevance_attention_raw = final_layer_attention_raw[cls_index]\n",
    "\n",
    "    return relevance_attention_raw\n",
    "    \n",
    "    \n",
    "def get_joint_relevance(full_att_mat, input_tokens, layer):\n",
    "    att_sum_heads =  full_att_mat.sum(axis=1)/8\n",
    "    joint_attentions = compute_joint_attention(att_sum_heads, add_residual=True)\n",
    "    relevance_attentions = joint_attentions[layer][0]\n",
    "    return relevance_attentions\n",
    "\n",
    "from tqdm import tqdm\n",
    "for layer in np.arange(6):\n",
    "    print(\"Layer: \", layer) \n",
    "#     r_corels, r_pvals = [], []\n",
    "#     for i in np.arange(len(all_index_probs)):\n",
    "#         aa = [0, 0]+all_index_probs[i]+[0]\n",
    "#         bb = get_raw_att_relevance(all_atts[i], ['<cls>']+sentences[i].split(), layer=layer)\n",
    "#         corel = spearmanr(aa,bb)\n",
    "#         r_corels.append(corel) \n",
    "\n",
    "#     print(\"r:\", np.mean(r_corels), np.std(r_corels))\n",
    "\n",
    "#     j_corels, j_pvals = [], []\n",
    "#     for i in np.arange(len(all_index_probs)):\n",
    "#         aa = [0, 0]+all_index_probs[i]+[0]\n",
    "#         bb = get_joint_relevance(all_atts[i], ['<cls>']+sentences[i].split(),layer=layer)\n",
    "#         corel = spearmanr(aa,bb)\n",
    "#         j_corels.append(corel) \n",
    "#     print(\"j:\", np.mean(j_corels), np.std(j_corels))\n",
    "\n",
    "    f_corels, f_pvals = [], []\n",
    "    for i in tqdm(np.arange(len(all_index_probs))):\n",
    "        aa = [0, 0]+all_index_probs[i]+[0]\n",
    "        bb = get_flow_relevance(all_atts[i], ['<cls>']+sentences[i].split(),layer=layer)\n",
    "        corel = spearmanr(aa,bb)\n",
    "        f_corels.append(corel) \n",
    "    print(\"f:\", np.mean(f_corels), np.std(f_corels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_outputs[6][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
