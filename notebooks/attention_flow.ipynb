{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from util import constants\n",
    "from util.config_util import get_model_params, get_task_params, get_train_params\n",
    "from tf2_models.trainer import Trainer\n",
    "from absl import app\n",
    "from absl import flags\n",
    "import pandas as pd\n",
    "\n",
    "from util.models import MODELS\n",
    "from util.tasks import TASKS\n",
    "from notebook_utils import *\n",
    "from attention_graph_util import *\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "Vocab len:  10032\n"
     ]
    }
   ],
   "source": [
    "# Load Task: VP\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "with strategy.scope():\n",
    "    task_name = 'word_sv_agreement_vp'\n",
    "    chkpt_dir='../tf_ckpts'\n",
    "    task_params = get_task_params(batch_size=10)\n",
    "    task = TASKS[task_name](task_params, data_dir='../data')\n",
    "    cl_token = task.sentence_encoder().encode(constants.bos)\n",
    "    tokenizer = task.sentence_encoder()._tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model config: small_gpt_v9\n",
      "{'embedding_dim': 128, 'resid_pdrop': 0.4, 'embd_pdrop': 0.2, 'attn_pdrop': 0.6, 'initializer_range': 0.05}\n",
      "model config: small_lstm_v4\n",
      "{'hidden_dim': 256, 'embedding_dim': 256, 'depth': 2, 'hidden_dropout_rate': 0.8, 'input_dropout_rate': 0.2, 'initializer_range': 0.1}\n",
      "student_checkpoint: ../tf_ckpts/word_sv_agreement_vp/online_dstl_6_crs_slw_teacher_cl_lstm_em-256_h-256_d-2_hdrop-0.8_indrop-0.2_small_lstm_v4_af_tchr2_student_cl_bert_h-128_d-6_rdrop-0.4_adrop-0.6_indrop-0.2_small_gpt_v9_af_std2\n",
      "Restored student from ../tf_ckpts/word_sv_agreement_vp/online_dstl_6_crs_slw_teacher_cl_lstm_em-256_h-256_d-2_hdrop-0.8_indrop-0.2_small_lstm_v4_af_tchr2_student_cl_bert_h-128_d-6_rdrop-0.4_adrop-0.6_indrop-0.2_small_gpt_v9_af_std2/ckpt-16\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 5s 55ms/step - loss: 0.1347 - classification_loss: 0.1243 - sparse_categorical_accuracy: 0.9520\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.13474531314335764, 0.12433353, 0.952]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load and evaluate a model\n",
    "\n",
    "config = {'student_exp_name':'af_std2',\n",
    "        'teacher_exp_name':'af_tchr2',\n",
    "        'teacher_config':'small_lstm_v4',\n",
    "        'student_model':'cl_bert',\n",
    "        'teacher_model':'cl_lstm',\n",
    "        'student_config':'small_gpt_v9',\n",
    "        'distill_config':'dstl_6_crs_slw',\n",
    "        'distill_mode':'online',\n",
    "        'chkpt_dir':'../tf_ckpts',}\n",
    "\n",
    "hparams=get_model_params(task, config['student_model'], config['student_config'])    \n",
    "hparams.output_attentions = True\n",
    "hparams.output_embeddings = True\n",
    "hparams.output_hidden_states = True\n",
    "\n",
    "with strategy.scope():\n",
    "    model, ckpnt = get_student_model(config, task, hparams, cl_token)\n",
    "\n",
    "\n",
    "model.evaluate(task.valid_dataset, steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_examples_x = []\n",
    "all_examples_y = []\n",
    "all_examples_attentions = []\n",
    "all_examples_correct_probs = []\n",
    "all_examples_correct_index_probs_diff = []\n",
    "non_reshaped = []\n",
    "n_batches = 10\n",
    "prob_fn = task.get_probs_fn()\n",
    "for x, y in iter(task.test_dataset):\n",
    "    # Save examples\n",
    "    all_examples_x.extend(x)\n",
    "    all_examples_y.extend(y)\n",
    "    \n",
    "    # Call the model to the get the logits and attentions\n",
    "    outputs = model.detailed_call(x, training=False)\n",
    "    main_logits = outputs[0]\n",
    "    \n",
    "    # Get the probability of the correct class\n",
    "    main_probs = prob_fn(main_logits, y, 1)\n",
    "    batch_indexes = tf.range(len(y), dtype=tf.int64)\n",
    "    indexes = tf.concat([batch_indexes[:,None], y[:,None]], axis=1)\n",
    "    correct_main_probs = tf.gather_nd(main_probs, indexes).numpy()\n",
    "    \n",
    "    attentions_of_all_layers = outputs[6]\n",
    "    \n",
    "    # Reshape the attention matrix to: [batchsize, layers, heads, length, length]\n",
    "    attentions_of_all_layers = [att.numpy() for att in attentions_of_all_layers]\n",
    "    attentions_of_all_layers = np.transpose(np.asarray(attentions_of_all_layers), (1,0,2,3,4))\n",
    "    \n",
    "    # Save attentions and correct probs\n",
    "    all_examples_attentions.extend(attentions_of_all_layers)\n",
    "    all_examples_correct_probs.extend(correct_main_probs)\n",
    "\n",
    "    \n",
    "    # Repeating examples and replacing one token at a time with unk\n",
    "    batch_size = tf.shape(x)[0]\n",
    "    max_len = x.shape[1]\n",
    "    \n",
    "    # Repeat each example 'max_len' times\n",
    "    extended_x = tf.reshape(tf.tile(x[:,None,...], (1,max_len, 1)),(-1,x.shape[-1]))\n",
    "    extended_y = tf.reshape(tf.tile(y[:,None],(1,max_len)),(-1,))\n",
    "    extened_correct_main_probs = tf.reshape(tf.tile(correct_main_probs[:,None],(1,max_len)),(-1,))\n",
    "    \n",
    "    # Create unk sequences and unk mask\n",
    "    unktoken = task.databuilder.sentence_encoder().encode(constants.unk)\n",
    "    unks = unktoken * tf.eye(max_len)\n",
    "    unks = tf.cast(tf.tile(unks, (batch_size, 1)), dtype=tf.int64)\n",
    "    unk_mask =  tf.cast((unktoken - unks)/unktoken, dtype=tf.int64)\n",
    "  \n",
    "    # Replace one token in each repeatition with unk\n",
    "    extended_x = extended_x * unk_mask + unks\n",
    "    \n",
    "    # Get the new output\n",
    "    extended_logits = model(extended_x, training=False)\n",
    "    extended_probs = prob_fn(extended_logits, extended_y, 1)\n",
    "    batch_indexes = tf.range(len(extended_y), dtype=tf.int64)\n",
    "    extended_indexes = tf.concat([batch_indexes[:,None], extended_y[:,None]], axis=1)\n",
    "    extended_correct_probs = tf.gather_nd(extended_probs, extended_indexes).numpy()\n",
    "    \n",
    "    # Save the difference in the probability predicted for the correct class\n",
    "    diffs = extened_correct_main_probs - extended_correct_probs\n",
    "    diffs = tf.reshape(diffs,(batch_size,-1,1))\n",
    "    all_examples_correct_index_probs_diff.extend(diffs)\n",
    "    \n",
    "    n_batches -= 1\n",
    "    if n_batches <= 0:\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spearmanr(x, y):\n",
    "    \"\"\" `x`, `y` --> pd.Series\"\"\"\n",
    "    x = pd.Series(x)\n",
    "    y = pd.Series(y)\n",
    "    assert x.shape == y.shape\n",
    "    rx = x.rank(method='dense')\n",
    "    ry = y.rank(method='dense')\n",
    "    d = rx - ry\n",
    "    dsq = np.sum(np.square(d))\n",
    "    n = x.shape[0]\n",
    "    coef = 1. - (6. * dsq) / (n * (n**2 - 1.))\n",
    "    return coef\n",
    "\n",
    "def get_raw_att_relevance(full_att_mat, input_tokens, layer=-1):\n",
    "    cls_index = 0\n",
    "    return full_att_mat[layer].sum(axis=0)[cls_index]\n",
    "\n",
    "\n",
    "def get_joint_relevance(full_att_mat, input_tokens, layer=-1):\n",
    "    att_sum_heads =  full_att_mat.sum(axis=1)/8\n",
    "    joint_attentions = compute_joint_attention(att_sum_heads, add_residual=True)\n",
    "    relevance_attentions = joint_attentions[layer][0]\n",
    "    return relevance_attentions\n",
    "\n",
    "\n",
    "def get_flow_relevance(full_att_mat, input_tokens, layer):\n",
    "    \n",
    "    res_att_mat = full_att_mat.sum(axis=1)/full_att_mat.shape[1]\n",
    "    res_att_mat = res_att_mat + np.eye(res_att_mat.shape[1])[None,...]\n",
    "    res_att_mat = res_att_mat / res_att_mat.sum(axis=-1)[...,None]\n",
    "\n",
    "    res_adj_mat, res_labels_to_index = get_adjmat(mat=res_att_mat, input_tokens=input_tokens)\n",
    "    \n",
    "    A = res_adj_mat\n",
    "    res_G=nx.from_numpy_matrix(A, create_using=nx.DiGraph())\n",
    "    for i in np.arange(A.shape[0]):\n",
    "        for j in np.arange(A.shape[1]):\n",
    "            nx.set_edge_attributes(res_G, {(i,j): A[i,j]}, 'capacity')\n",
    "\n",
    "\n",
    "    output_nodes = []\n",
    "    input_nodes = []\n",
    "    for key in res_labels_to_index:\n",
    "        if 'L'+str(layer+1) in key:\n",
    "            output_nodes.append(key)\n",
    "        if res_labels_to_index[key] < full_att_mat.shape[-1]:\n",
    "            input_nodes.append(key)\n",
    "    \n",
    "    flow_values = compute_node_flow(res_G, res_labels_to_index, input_nodes, output_nodes, length=full_att_mat.shape[-1])\n",
    "    \n",
    "    n_layers = full_att_mat.shape[0]\n",
    "    length = full_att_mat.shape[-1]\n",
    "    final_layer_attention = flow_values[(layer+1)*length:,layer*length:(layer+1)*length]\n",
    "    cls_index = 0\n",
    "    relevance_attention_raw = final_layer_attention[cls_index]\n",
    "\n",
    "    return relevance_attention_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_examples_blankout_relevance = []\n",
    "for i in np.arange(len(all_examples_x)):\n",
    "    tokens = task.sentence_encoder().decode(all_examples_x[i]).split()\n",
    "    length = len(tokens)\n",
    "    all_examples_blankout_relevance.append(all_examples_correct_index_probs_diff[i].numpy().squeeze()[:length])\n",
    "\n",
    "all_examples_joint_relevance = []\n",
    "for i in np.arange(len(all_examples_x)):\n",
    "    tokens = task.sentence_encoder().decode(all_examples_x[i]).split()\n",
    "    length = len(tokens)\n",
    "    attention_relevance = get_joint_relevance(all_examples_attentions[i][...,:length, :length], tokens, layer=5)\n",
    "    all_examples_joint_relevance.append(np.asarray(attention_relevance))\n",
    "    \n",
    "    \n",
    "all_examples_raw_relevance = []\n",
    "for i in np.arange(len(all_examples_x)):\n",
    "    tokens = task.sentence_encoder().decode(all_examples_x[i]).split()\n",
    "    length = len(tokens)\n",
    "    attention_relevance = get_raw_att_relevance(all_examples_attentions[i][...,:length, :length], tokens, layer=5)\n",
    "    all_examples_raw_relevance.append(np.asarray(attention_relevance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_examples_flow_relevance = []\n",
    "for i in np.arange(len(all_examples_x)):\n",
    "    tokens = task.sentence_encoder().decode(all_examples_x[i]).split()\n",
    "    length = len(tokens)\n",
    "    attention_relevance = get_flow_relevance(all_examples_attentions[i][...,:length, :length], tokens, layer=5)\n",
    "    all_examples_flow_relevance.append(np.asarray(attention_relevance))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6522755415915037\n",
      "-0.5016929969496945\n",
      "-0.6297435308303443\n"
     ]
    }
   ],
   "source": [
    "print(np.mean([spearmanr(all_examples_flow_relevance[i], all_examples_joint_relevance[i]) for i in np.arange(len(all_examples_x))]))\n",
    "print(np.mean([spearmanr(all_examples_flow_relevance[i], all_examples_raw_relevance[i]) for i in np.arange(len(all_examples_x))]))\n",
    "print(np.mean([spearmanr(all_examples_joint_relevance[i], all_examples_raw_relevance[i]) for i in np.arange(len(all_examples_x))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0880811794975221\n",
      "-0.11830827359925346\n",
      "-0.11830827359925346\n"
     ]
    }
   ],
   "source": [
    "print(np.mean([spearmanr(all_examples_flow_relevance[i], all_examples_blankout_relevance[i]) for i in np.arange(len(all_examples_x))]))\n",
    "print(np.mean([spearmanr(all_examples_joint_relevance[i], all_examples_blankout_relevance[i]) for i in np.arange(len(all_examples_x))]))\n",
    "print(np.mean([spearmanr(all_examples_joint_relevance[i], all_examples_blankout_relevance[i]) for i in np.arange(len(all_examples_x))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 1)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
