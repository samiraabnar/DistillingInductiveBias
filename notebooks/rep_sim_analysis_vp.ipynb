{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from util import constants\n",
    "from util.config_util import get_model_params, get_task_params, get_train_params\n",
    "from tf2_models.trainer import Trainer\n",
    "from absl import app\n",
    "from absl import flags\n",
    "import numpy as np\n",
    "from util.models import MODELS\n",
    "from util.tasks import TASKS\n",
    "from notebook_utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reps(inputs, model, index=1, layer=None):\n",
    "    \"\"\"\n",
    "    If Model is LSTM:\n",
    "        1: final_rnn_outputs, \n",
    "        2: hidden_activation (for all layers, including input embeddings)\n",
    "    \"\"\"\n",
    "    outputs = model.detailed_call(inputs)\n",
    "    reps = outputs[index]\n",
    "    \n",
    "    if layer is not None:\n",
    "        reps = reps[layer]\n",
    "    \n",
    "    return reps\n",
    "\n",
    "def normalized_pairwisedot_product_sim(reps1, reps2):    \n",
    "    reps1 = reps1 / tf.norm(reps1, axis=-1)[...,None]\n",
    "    reps2 = reps2 / tf.norm(reps2, axis=-1)[...,None]\n",
    "    \n",
    "    pw_dot_product = tf.cast(tf.matmul(reps1, reps2, transpose_b=True), dtype=tf.float32) \n",
    "\n",
    "    p_max = tf.reduce_max(pw_dot_product, axis=-1)\n",
    "    p_min =  tf.reduce_min(pw_dot_product, axis=-1)\n",
    "    \n",
    "    \n",
    "    #pw_dot_product = (pw_dot_product  - p_max) / (p_max - p_min)\n",
    "    return pw_dot_product\n",
    "\n",
    "\n",
    "def normalized_dot_product_sim(reps1, reps2, padding_mask):\n",
    "    #normalize reps:\n",
    "    reps1 = reps1 / tf.norm(reps1, axis=-1)[...,None]\n",
    "    reps2 = reps2 / tf.norm(reps2, axis=-1)[...,None]\n",
    "    \n",
    "    norm1 = tf.norm(reps1, axis=-1)\n",
    "    norm2 = tf.norm(reps2, axis=-1)\n",
    "\n",
    "    # Elementwise multiplication\n",
    "    dot_product = tf.multiply(reps1, reps2)\n",
    "    \n",
    "    # Sum over last axis to get the dot product similarity between corresponding pairs\n",
    "    dot_product = tf.reduce_sum(dot_product, axis=-1)\n",
    "    dot_product = tf.multiply(dot_product,padding_mask[:,0])\n",
    "    \n",
    "    return dot_product\n",
    "    \n",
    "    \n",
    "def second_order_rep_sim(reps1, reps2, padding_mask):\n",
    "    \n",
    "    sims1 = normalized_pairwisedot_product_sim(reps1, reps1)\n",
    "    sims2 = normalized_pairwisedot_product_sim(reps2, reps2)\n",
    "    \n",
    "    padding_mask = tf.ones((reps1.shape[0],1))\n",
    "    so_sims = normalized_dot_product_sim(sims1, sims2, padding_mask) * padding_mask[:,0]\n",
    "    mean_sim = tf.reduce_sum(so_sims) / tf.reduce_sum(padding_mask)\n",
    "    \n",
    "    return mean_sim, so_sims\n",
    "\n",
    "def compare_models(inputs, model1, model2, index1=1, index2=1,layer1=None, layer2=None, padding_symbol=None):\n",
    "    reps1 = get_reps(inputs, model1)\n",
    "    reps2 = get_reps(inputs, model2)\n",
    "\n",
    "    \n",
    "    reps1 = tf.reshape(reps1, (-1, tf.shape(reps1)[-1]))\n",
    "    reps2 = tf.reshape(reps2, (-1, tf.shape(reps2)[-1]))\n",
    "    \n",
    "    if padding_symbol is not None:\n",
    "        padding_mask = tf.cast(1.0 - (inputs == padding_symbol), dtype=tf.float32)\n",
    "        padding_mask = tf.reshape(reps2, (-1,1))\n",
    "    else:\n",
    "        padding_mask = tf.ones((tf.shape(reps1)[0]))\n",
    "    \n",
    "    similarity_measures = second_order_rep_sim(reps1, reps2, padding_mask=padding_mask)\n",
    "    \n",
    "    return similarity_measures\n",
    "\n",
    "def compare_reps(reps1, reps2, padding_symbol=None, inputs=None):\n",
    "    reps1 = tf.reshape(reps1, (-1, tf.shape(reps1)[-1]))\n",
    "    reps2 = tf.reshape(reps2, (-1, tf.shape(reps2)[-1]))\n",
    "    \n",
    "    if padding_symbol is not None:\n",
    "        padding_mask = tf.cast(1.0 - (inputs == padding_symbol), dtype=tf.float32)\n",
    "        padding_mask = tf.reshape(reps2, (-1,1))\n",
    "    else:\n",
    "        padding_mask = tf.ones((tf.shape(reps1)[0],1))\n",
    "        \n",
    "    \n",
    "    similarity_measures = second_order_rep_sim(reps1, reps2, padding_mask)\n",
    "    \n",
    "    return similarity_measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab len:  10032\n"
     ]
    }
   ],
   "source": [
    "task_name = 'word_sv_agreement_vp'\n",
    "chkpt_dir='../tf_ckpts'\n",
    "task = TASKS[task_name](get_task_params(), data_dir='../data')\n",
    "cl_token = task.databuilder.sentence_encoder().encode(constants.bos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model config: small_lstm_v4\n",
      "{'hidden_dim': 256, 'embedding_dim': 256, 'depth': 2, 'hidden_dropout_rate': 0.8, 'input_dropout_rate': 0.2, 'initializer_range': 0.1}\n",
      "model config: small_lstm_v4\n",
      "{'hidden_dim': 256, 'embedding_dim': 256, 'depth': 2, 'hidden_dropout_rate': 0.8, 'input_dropout_rate': 0.2, 'initializer_range': 0.1}\n",
      "student_checkpoint: ../tf_ckpts/word_sv_agreement_vp/offline_pure_dstl_4_crs_slw_teacher_cl_lstm_em-256_h-256_d-2_hdrop-0.8_indrop-0.2_small_lstm_v4_0.001_samira_offlineteacher_v11_student_cl_lstm_em-256_h-256_d-2_hdrop-0.8_indrop-0.2_small_lstm_v4_lisa_fd131\n",
      "Restored student from ../tf_ckpts/word_sv_agreement_vp/offline_pure_dstl_4_crs_slw_teacher_cl_lstm_em-256_h-256_d-2_hdrop-0.8_indrop-0.2_small_lstm_v4_0.001_samira_offlineteacher_v11_student_cl_lstm_em-256_h-256_d-2_hdrop-0.8_indrop-0.2_small_lstm_v4_lisa_fd131/ckpt-60\n",
      "model config: small_lstm_v4\n",
      "{'hidden_dim': 256, 'embedding_dim': 256, 'depth': 2, 'hidden_dropout_rate': 0.8, 'input_dropout_rate': 0.2, 'initializer_range': 0.1}\n",
      "Restored student from ../tf_ckpts/word_sv_agreement_vp/cl_lstm_em-256_h-256_d-2_hdrop-0.8_indrop-0.2_small_lstm_v4_0.001_samira_offlineteacher_v11/ckpt-60\n"
     ]
    }
   ],
   "source": [
    "config={'student_exp_name':'lisa_fd131',\n",
    "    'teacher_exp_name':'0.001_samira_offlineteacher_v11',\n",
    "    'teacher_config':'small_lstm_v4',\n",
    "    'task_name':'word_sv_agreement_vp',\n",
    "    'student_model':'cl_lstm',\n",
    "    'teacher_model':'cl_lstm',\n",
    "    'student_config':'small_lstm_v4',\n",
    "    'distill_config':'pure_dstl_4_crs_slw',\n",
    "    'distill_mode':'offline',\n",
    "    'chkpt_dir':'../tf_ckpts',\n",
    "       }\n",
    "\n",
    "std_hparams=get_model_params(task, config['student_model'], config['student_config'])\n",
    "std_hparams.output_attentions = True\n",
    "std_hparams.output_embeddings = True\n",
    "std_hparams.output_hidden_states = True\n",
    "\n",
    "model1, _ = get_student_model(config, task, std_hparams, cl_token)\n",
    "\n",
    "tchr_hparams=get_model_params(task, config['teacher_model'], config['teacher_config'])\n",
    "tchr_hparams.output_attentions = True\n",
    "tchr_hparams.output_embeddings = True\n",
    "tchr_hparams.output_hidden_states = True\n",
    "\n",
    "model2, _ = get_teacher_model(config, task, tchr_hparams, cl_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab len:  10032\n",
      "model config: very_big_gpt_v10\n",
      "{'embedding_dim': 512, 'resid_pdrop': 0.4, 'embd_pdrop': 0.2, 'attn_pdrop': 0.6, 'initializer_range': 0.05}\n",
      "model config: lstm_drop31_v2\n",
      "{'hidden_dim': 512, 'embedding_dim': 512, 'depth': 2, 'hidden_dropout_rate': 0.3, 'input_dropout_rate': 0.2}\n",
      "student_checkpoint: ../tf_ckpts/word_sv_agreement_lm/offline_dstl_6_crs_slw_teacher_lm_lstm_shared_emb_em-512_h-512_d-2_hdrop-0.3_indrop-0.2_lstm_drop31_v2_0.001_lisa_crs_fst_offlineteacher_v20_student_lm_gpt2_h-512_d-6_rdrop-0.4_adrop-0.6_indrop-0.2_very_big_gpt_v10_lisa_fd432\n",
      "Restored student from ../tf_ckpts/word_sv_agreement_lm/offline_dstl_6_crs_slw_teacher_lm_lstm_shared_emb_em-512_h-512_d-2_hdrop-0.3_indrop-0.2_lstm_drop31_v2_0.001_lisa_crs_fst_offlineteacher_v20_student_lm_gpt2_h-512_d-6_rdrop-0.4_adrop-0.6_indrop-0.2_very_big_gpt_v10_lisa_fd432/ckpt-60\n",
      "model config: lstm_drop31_v2\n",
      "{'hidden_dim': 512, 'embedding_dim': 512, 'depth': 2, 'hidden_dropout_rate': 0.3, 'input_dropout_rate': 0.2}\n",
      "Restored student from ../tf_ckpts/word_sv_agreement_lm/lm_lstm_shared_emb_em-512_h-512_d-2_hdrop-0.3_indrop-0.2_lstm_drop31_v2_0.001_lisa_crs_fst_offlineteacher_v20/ckpt-60\n"
     ]
    }
   ],
   "source": [
    "task_name = 'word_sv_agreement_lm'\n",
    "chkpt_dir='../tf_ckpts'\n",
    "task = TASKS[task_name](get_task_params(), data_dir='../data')\n",
    "cl_token = task.databuilder.sentence_encoder().encode(constants.bos)\n",
    "\n",
    "config={'student_exp_name':'lisa_fd432',\n",
    "    'teacher_exp_name':'0.001_lisa_crs_fst_offlineteacher_v20',\n",
    "    'teacher_config':'lstm_drop31_v2',\n",
    "    'task_name':'word_sv_agreement_lm',\n",
    "    'student_model':'lm_gpt2',\n",
    "    'teacher_model':'lm_lstm_shared_emb',\n",
    "    'student_config':'very_big_gpt_v10',\n",
    "    'distill_config':'dstl_6_crs_slw',\n",
    "    'distill_mode':'offline',\n",
    "    'chkpt_dir':'../tf_ckpts',\n",
    "       }\n",
    "\n",
    "std_hparams=get_model_params(task, config['student_model'], config['student_config'])\n",
    "std_hparams.output_attentions = True\n",
    "std_hparams.output_embeddings = True\n",
    "std_hparams.output_hidden_states = True\n",
    "\n",
    "model1, _ = get_student_model(config, task, std_hparams, cl_token)\n",
    "\n",
    "tchr_hparams=get_model_params(task, config['teacher_model'], config['teacher_config'])\n",
    "tchr_hparams.output_attentions = True\n",
    "tchr_hparams.output_embeddings = True\n",
    "tchr_hparams.output_hidden_states = True\n",
    "\n",
    "model2, _ = get_teacher_model(config, task, tchr_hparams, cl_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "tf.Tensor(0.9217837, shape=(), dtype=float32)\n",
      "tf.Tensor(0.9217837, shape=(), dtype=float32)\n",
      "tf.Tensor(1.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for inputs, labels in task.valid_dataset:\n",
    "    reps1 = get_reps(inputs, model1, index=1, layer=None)\n",
    "    reps2 = get_reps(inputs, model2, index=1, layer=None)\n",
    "    mean, all = compare_reps(reps1, reps1)\n",
    "    print(mean)\n",
    "    mean, all = compare_models(inputs, model1, model1)\n",
    "    print(mean)\n",
    "    mean, all = compare_reps(reps1, reps2)\n",
    "    print(mean)\n",
    "    mean, all = compare_reps(reps2, reps1)\n",
    "    print(mean)\n",
    "    mean, all = compare_reps(reps2, reps2)\n",
    "    print(mean)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model2.detailed_call(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inputs, labels in task.valid_dataset:\n",
    "    reps1 = get_reps(inputs, model1, index=1, layer=None)\n",
    "    reps2 = get_reps(inputs, model2, index=1, layer=None)\n",
    "    mean, all = compare_reps(reps1, reps1, padding_symbol=0, inputs=inputs)\n",
    "    print(mean)\n",
    "    mean, all = compare_models(inputs, model1, model1, padding_symbol=0, inputs=inputs)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
