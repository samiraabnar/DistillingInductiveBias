{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from datetime import datetime   # date stamp the log directory\n",
    "import json  # for saving and loading hyperparameters\n",
    "import os, sys, re\n",
    "import time\n",
    "\n",
    "# Get logger that has already been created in config.py\n",
    "import daiquiri\n",
    "logger = daiquiri.getLogger(__name__)\n",
    "\n",
    "import absl\n",
    "import absl.logging as logging\n",
    "gfile = tf.io.gfile\n",
    "flags = absl.app.flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Need this line for flags to work with Jupyter\n",
    "# https://github.com/tensorflow/tensorflow/issues/17702\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# HYPERPARAMETERS\n",
    "#------------------------------------------------------------------------------\n",
    "# set to 64 according to authors (https://openreview.net/forum?id=HJWLfGWRb)\n",
    "flags.DEFINE_integer('batch_size', 64, 'batch size in total across all gpus') \n",
    "flags.DEFINE_integer('epoch', 2000, 'epoch')\n",
    "flags.DEFINE_integer('iter_routing', 3, 'number of iterations')\n",
    "flags.DEFINE_integer('num_gpus', 1, 'number of GPUs')\n",
    "flags.DEFINE_float('epsilon', 1e-9, 'epsilon')\n",
    "flags.DEFINE_float('lrn_rate', 3e-3, 'learning rate to use in Adam optimiser')\n",
    "flags.DEFINE_float('val_prop', 0.1, \n",
    "                   'proportion of test dataset to use for validation')\n",
    "flags.DEFINE_boolean('weight_reg', False, \n",
    "                     'train with regularization of weights')\n",
    "flags.DEFINE_string('norm', 'norm2', 'norm type')\n",
    "flags.DEFINE_integer('num_threads', 8, \n",
    "                     'number of parallel calls in the input pipeline')\n",
    "flags.DEFINE_string('dataset', 'smallNORB', \n",
    "                    '''dataset name: currently only \"smallNORB\" supported, feel\n",
    "                    free to add your own''')\n",
    "flags.DEFINE_float('final_lambda', 0.01, 'final lambda in EM routing')\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# ARCHITECTURE PARAMETERS\n",
    "#------------------------------------------------------------------------------\n",
    "flags.DEFINE_integer('A', 64, 'number of channels in output from ReLU Conv1')\n",
    "flags.DEFINE_integer('B', 8, 'number of capsules in output from PrimaryCaps')\n",
    "flags.DEFINE_integer('C', 16, 'number of channels in output from ConvCaps1')\n",
    "flags.DEFINE_integer('D', 16, 'number of channels in output from ConvCaps2')\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# ENVIRONMENT SETTINGS\n",
    "#------------------------------------------------------------------------------\n",
    "flags.DEFINE_string('mode', 'train', 'train, validate, or test')\n",
    "flags.DEFINE_string('name', '', 'name of experiment in log directory')\n",
    "flags.DEFINE_boolean('reset', False, 'clear the train or test log directory')\n",
    "flags.DEFINE_string('debugger', None, \n",
    "                    '''set to host of TensorBoard debugger e.g. \"dccxc180:8886 \n",
    "                    or dccxl015:8770\"''')\n",
    "flags.DEFINE_boolean('profile', False, \n",
    "                     '''get runtime statistics to display inTensorboard e.g. \n",
    "                     compute time''')\n",
    "flags.DEFINE_string('load_dir', None, \n",
    "                    '''directory containing train or test checkpoints to \n",
    "                    continue from''')\n",
    "flags.DEFINE_string('ckpt_name', None, \n",
    "                    '''None to load the latest ckpt; all to load all ckpts in \n",
    "                      dir; name to load specific ckpt''')\n",
    "flags.DEFINE_string('params_path', None, 'path to JSON containing parameters')\n",
    "\n",
    "LOCAL_STORAGE = '../'\n",
    "flags.DEFINE_string('storage', LOCAL_STORAGE, \n",
    "                    'directory where logs and data are stored')\n",
    "flags.DEFINE_string('db_name', 'capsules_ex1', \n",
    "                    'Name of the DB for mongo for sacred')\n",
    "\n",
    "# Parse flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# DIRECTORIES\n",
    "#------------------------------------------------------------------------------\n",
    "def setup_train_directories():\n",
    "  \n",
    "  # Set log directory\n",
    "  date_stamp = datetime.now().strftime('%Y%m%d')\n",
    "  save_dir = os.path.join(absl.app.flags.FLAGS.storage, 'logs/',\n",
    "              absl.app.flags.FLAGS.dataset)\n",
    "  train_dir = '{}/{}_{}/train'.format(save_dir, date_stamp, FLAGS.name)\n",
    "\n",
    "  # Clear the train log directory\n",
    "  if FLAGS.reset is True and gfile.exists(train_dir):\n",
    "    gfile.remove(train_dir)\n",
    "\n",
    "  # Create train directory\n",
    "  if not gfile.exists(train_dir):\n",
    "    gfile.makedirs(train_dir)\n",
    "\n",
    "  # Set summary directory\n",
    "  train_summary_dir = os.path.join(train_dir, 'summary')\n",
    "\n",
    "  # Create summary directory\n",
    "  if not gfile.exists(train_summary_dir):\n",
    "    gfile.makedirs(train_summary_dir)\n",
    "    \n",
    "  return train_dir, train_summary_dir\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# SETUP LOGGER\n",
    "#------------------------------------------------------------------------------\n",
    "def setup_logger(logger_dir, name=\"logger\"):\n",
    "  os.environ['TZ'] = 'Africa/Johannesburg'\n",
    "  time.tzset()\n",
    "  daiquiri_formatter = daiquiri.formatter.ColorFormatter(\n",
    "      fmt= \"%(asctime)s %(color)s%(levelname)s: %(message)s%(color_stop)s\",\n",
    "      datefmt=\"%Y-%m-%d %H:%M:%S\")\n",
    "  logger_path = os.path.join(logger_dir, name)\n",
    "  daiquiri.setup(level=logging.INFO, outputs=(\n",
    "      daiquiri.output.Stream(formatter=daiquiri_formatter),\n",
    "      daiquiri.output.File(logger_path,formatter=daiquiri_formatter),\n",
    "     ))\n",
    "  # To access the logger from other files, just put this line at the top:\n",
    "  # logger = daiquiri.getLogger(__name__)\n",
    "\n",
    "  \n",
    "#------------------------------------------------------------------------------\n",
    "# LOAD OR SAVE HYPERPARAMETERS\n",
    "#------------------------------------------------------------------------------\n",
    "def load_or_save_hyperparams(train_dir=None):\n",
    "     \n",
    "  # Load parameters from file\n",
    "  # params_path is given in the case that run a new training using existing \n",
    "  # parameters\n",
    "  # load_dir is given in the case of testing or continuing training \n",
    "  if FLAGS.params_path or FLAGS.load_dir:\n",
    "\n",
    "    if FLAGS.params_path:\n",
    "      params_path = os.path.abspath(FLAGS.params_path)\n",
    "    elif FLAGS.load_dir:\n",
    "      params_path = os.path.join(FLAGS.load_dir, \"train\", \n",
    "                     \"params\", \"params.json\")\n",
    "      params_path = os.path.abspath(params_path)\n",
    "\n",
    "    with open(params_path, 'r') as params_file:\n",
    "      params = json.load(params_file)\n",
    "      \n",
    "      # Get list of flags that were specifically set in command line\n",
    "      cl_args = sys.argv[1:]\n",
    "      specified_flags = [re.search('--(.*)=', s).group(1) for s in cl_args]\n",
    "      \n",
    "      for name, value in params.items():\n",
    "        # ignore flags that were specifically set./run in command line\n",
    "        if name in specified_flags:\n",
    "          pass\n",
    "        else:\n",
    "          FLAGS.__flags[name].value = value \n",
    "    logger.info(\"Loaded parameters from file: {}\".format(params_path))\n",
    "\n",
    "  # Save parameters to file\n",
    "  elif FLAGS.mode == 'train': \n",
    "    params_dir_path = os.path.join(train_dir, \"params\")\n",
    "    os.makedirs(params_dir_path, exist_ok=True)\n",
    "    params_file_path = os.path.join(params_dir_path, \"params.json\")\n",
    "    params = FLAGS.flag_values_dict()\n",
    "    params_json = json.dumps(params, indent=4, separators=(',', ':'))\n",
    "    with open(params_file_path, 'w') as params_file:\n",
    "      params_file.write(params_json)\n",
    "    logger.info(\"Parameters saved to file: {}\".format(params_file_path))\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# FACTORIES FOR DATASET\n",
    "#------------------------------------------------------------------------------\n",
    "def get_dataset_path(dataset_name: str): \n",
    "  options = {'smallNORB': 'data/smallNORB/tfrecord'}\n",
    "  path = FLAGS.storage + options[dataset_name]\n",
    "  return path\n",
    "\n",
    "\n",
    "def get_dataset_size_train(dataset_name: str):\n",
    "  options = {'mnist': 55000, \n",
    "             'smallNORB': 23400 * 2,\n",
    "             'fashion_mnist': 55000, \n",
    "             'cifar10': 50000, \n",
    "             'cifar100': 50000}\n",
    "  return options[dataset_name]\n",
    "\n",
    "\n",
    "def get_dataset_size_test(dataset_name: str):\n",
    "  options = {'mnist': 10000, \n",
    "             'smallNORB': 23400 * 2,\n",
    "             'fashion_mnist': 10000, \n",
    "             'cifar10': 10000, \n",
    "             'cifar100': 10000}\n",
    "  return options[dataset_name]\n",
    "\n",
    "\n",
    "def get_dataset_size_validate(dataset_name: str):\n",
    "  options = {'smallNORB': 23400 * 2}\n",
    "  return options[dataset_name]\n",
    "\n",
    "\n",
    "def get_num_classes(dataset_name: str):\n",
    "  options = {'mnist': 10, \n",
    "             'smallNORB': 5, \n",
    "             'fashion_mnist': 10, \n",
    "             'cifar10': 10, \n",
    "             'cifar100': 100}\n",
    "  return options[dataset_name]\n",
    "\n",
    "\n",
    "# import data_pipeline_norb as data_norb\n",
    "def get_create_inputs(dataset_name: str, mode=\"train\"):\n",
    "  \n",
    "  if mode == \"train\":\n",
    "    is_train = True\n",
    "  else:\n",
    "    is_train = False\n",
    "    \n",
    "  path = get_dataset_path(dataset_name)\n",
    "  \n",
    "  options = {'smallNORB': \n",
    "         lambda: create_inputs_norb(path, is_train)}\n",
    "  return options[dataset_name]\n",
    "\n",
    "\n",
    "def get_dataset_architecture(dataset_name: str):\n",
    "  options = {'smallNORB': build_arch_smallnorb}\n",
    "  return options[dataset_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_routing_map(child_space, k, s):\n",
    "  \"\"\"Generate TFRecord for train and test datasets from .mat files.\n",
    "  \n",
    "  Create a binary map where the rows are capsules in the lower layer (children)\n",
    "  and the columns are capsules in the higher layer (parents). The binary map \n",
    "  shows which children capsules are connected to which parent capsules along the   spatial dimension.\n",
    "  \n",
    "  Author:\n",
    "    Ashley Gritzman 19/10/2018     \n",
    "  Args: \n",
    "    child_space: spatial dimension of lower capsule layer\n",
    "    k: kernel size\n",
    "    s: stride    \n",
    "  Returns:\n",
    "    binmap: \n",
    "      A 2D numpy matrix containing mapping between children capsules along the \n",
    "      rows, and parent capsules along the columns.\n",
    "      (child_space^2, parent_space^2)\n",
    "      (7*7, 5*5)\n",
    "  \"\"\"\n",
    "  \n",
    "  parent_space = int((child_space - k)/s + 1)\n",
    "  binmap = np.zeros((child_space**2, parent_space**2))\n",
    "  for r in range(parent_space):\n",
    "    for c in range(parent_space):\n",
    "      p_idx = r*parent_space + c\n",
    "      for i in range(k):\n",
    "        # c_idx stand for child_index; p_idx is parent_index\n",
    "        c_idx = r*s*child_space + c*s + child_space*i\n",
    "        binmap[(c_idx):(c_idx + k), p_idx] = 1\n",
    "  return binmap\n",
    "\n",
    "\n",
    "def kernel_tile(input, kernel, stride):\n",
    "  \"\"\"Tile the children poses/activations so that the children for each parent occur in one axis.\n",
    "  \n",
    "  Author:\n",
    "    Ashley Gritzman 19/10/2018\n",
    "  Args: \n",
    "    input: \n",
    "      tensor of child poses or activations\n",
    "      poses (N, child_space, child_space, i, 4, 4) -> (64, 7, 7, 8, 4, 4)\n",
    "      activations (N, child_space, child_space, i, 1) -> (64, 7, 7, 8, 16) \n",
    "    kernel: \n",
    "    stride: \n",
    "  Returns:\n",
    "    tiled: \n",
    "      (N, parent_space, parent_space, kh*kw, i, 16 or 1)\n",
    "      (64, 5, 5, 9, 8, 16 or 1)\n",
    "    child_parent_matrix:\n",
    "      A 2D numpy matrix containing mapping between children capsules along the \n",
    "      rows, and parent capsules along the columns.\n",
    "      (child_space^2, parent_space^2)\n",
    "      (7*7, 5*5)\n",
    "  \"\"\"\n",
    "  \n",
    "  input_shape = input.get_shape()\n",
    "  batch_size   = int(input_shape[0])\n",
    "  spatial_size = int(input_shape[1])\n",
    "  n_capsules   = int(input_shape[3])\n",
    "  parent_spatial_size = int((spatial_size - kernel)/stride + 1)\n",
    "  \n",
    "  # Check that dim 1 and 2 correspond to the spatial size\n",
    "  assert input_shape[1] == input_shape[2]\n",
    "  \n",
    "  # Check if we have poses or activations\n",
    "  if len(input_shape) > 5: \n",
    "    # Poses\n",
    "    size = input_shape[4]*input_shape[5]\n",
    "  else:\n",
    "    # Activations\n",
    "    size = 1\n",
    "  \n",
    "  # Matrix showing which children map to which parent. Children are rows, \n",
    "  # parents are columns.\n",
    "  child_parent_matrix = create_routing_map(spatial_size, kernel, stride)\n",
    "  \n",
    "  # Convert from np to tf\n",
    "  #child_parent_matrix = tf.constant(child_parent_matrix)\n",
    "\n",
    "  # Each row contains the children belonging to one parent\n",
    "  child_to_parent_idx = group_children_by_parent(child_parent_matrix)\n",
    "  \n",
    "  # Spread out spatial dimension of children\n",
    "  input = tf.reshape(input, [batch_size, spatial_size*spatial_size, -1])\n",
    "  \n",
    "  # Select which children go to each parent capsule\n",
    "  tiled = tf.gather(input, child_to_parent_idx, axis=1)\n",
    "  \n",
    "  tiled = tf.squeeze(tiled)\n",
    "  tiled = tf.reshape(tiled, [batch_size, parent_spatial_size, parent_spatial_size, kernel*kernel, n_capsules, -1])\n",
    "  \n",
    "  return tiled, child_parent_matrix\n",
    "\n",
    "\n",
    "def compute_votes(poses_i, o, regularizer, tag=False):\n",
    "  \"\"\"Compute the votes by multiplying input poses by transformation matrix.\n",
    "  \n",
    "  Multiply the poses of layer i by the transform matrix to compute the votes for \n",
    "  layer j.\n",
    "  \n",
    "  Author:\n",
    "    Ashley Gritzman 19/10/2018\n",
    "    \n",
    "  Credit: \n",
    "    Suofei Zhang's implementation on GitHub, \"Matrix-Capsules-EM-Tensorflow\"\n",
    "    https://github.com/www0wwwjs1/Matrix-Capsules-EM-Tensorflow\n",
    "    \n",
    "  Args: \n",
    "    poses_i: \n",
    "      poses in layer i tiled according to the kernel\n",
    "      (N*OH*OW, kh*kw*i, 16)\n",
    "      (64*5*5, 9*8, 16) \n",
    "    o: number of output capsules, also called \"parent_caps\"\n",
    "    regularizer:    \n",
    "    \n",
    "  Returns:\n",
    "    votes: \n",
    "      (N*OH*OW, kh*kw*i, o, 16)\n",
    "      (64*5*5, 9*8, 32, 16)\n",
    "  \"\"\"\n",
    "  \n",
    "  batch_size = int(poses_i.get_shape()[0]) # 64*5*5\n",
    "  kh_kw_i = int(poses_i.get_shape()[1]) # 9*8\n",
    "  \n",
    "  # (64*5*5, 9*8, 16) -> (64*5*5, 9*8, 1, 4, 4)\n",
    "  output = tf.reshape(poses_i, shape=[batch_size, kh_kw_i, 1, 4, 4])\n",
    "  \n",
    "  # the output of capsule is miu, the mean of a Gaussian, and activation, the \n",
    "  # sum of probabilities it has no relationship with the absolute values of w \n",
    "  # and votes using weights with bigger stddev helps numerical stability\n",
    "  w = slim.model_variable('w', shape=[1, kh_kw_i, o, 4, 4], \n",
    "                          dtype=tf.float32, \n",
    "                          initializer=tf.truncated_normal_initializer(\n",
    "                            mean=0.0, \n",
    "                            stddev=1.0), #1.0\n",
    "                          regularizer=regularizer)\n",
    "  \n",
    "  # (1, 9*8, 32, 4, 4) -> (64*5*5, 9*8, 32, 4, 4)\n",
    "  w = tf.tile(w, [batch_size, 1, 1, 1, 1])\n",
    "  \n",
    "  # (64*5*5, 9*8, 1, 4, 4) -> (64*5*5, 9*8, 32, 4, 4)\n",
    "  output = tf.tile(output, [1, 1, o, 1, 1])\n",
    "  \n",
    "  # (64*5*5, 9*8, 32, 4, 4) x (64*5*5, 9*8, 32, 4, 4) \n",
    "  # -> (64*5*5, 9*8, 32, 4, 4)\n",
    "  mult = tf.matmul(output, w)\n",
    "  \n",
    "  # (64*5*5, 9*8, 32, 4, 4) -> (64*5*5, 9*8, 32, 16)\n",
    "  votes = tf.reshape(mult, [batch_size, kh_kw_i, o, 16])\n",
    "  \n",
    "  # tf.summary.histogram('w', w) \n",
    "\n",
    "  return votes\n",
    "\n",
    "\n",
    "def group_children_by_parent(bin_routing_map):\n",
    "  \"\"\"Groups children capsules by parent capsule.\n",
    "  \n",
    "  Rearrange the bin_routing_map so that each row represents one parent capsule,   and the entries in the row are indexes of the children capsules that route to   that parent capsule. This mapping is only along the spatial dimension, each \n",
    "  child capsule along in spatial dimension will actually contain many capsules,   e.g. 32. The grouping that we are doing here tell us about the spatial \n",
    "  routing, e.g. if the lower layer is 7x7 in spatial dimension, with a kernel of \n",
    "  3 and stride of 1, then the higher layer will be 5x5 in the spatial dimension. \n",
    "  So this function will tell us which children from the 7x7=49 lower capsules \n",
    "  map to each of the 5x5=25 higher capsules. One child capsule can be in several \n",
    "  different parent capsules, children in the corners will only belong to one \n",
    "  parent, but children towards the center will belong to several with a maximum   of kernel*kernel (e.g. 9), but also depending on the stride.\n",
    "  \n",
    "  Author:\n",
    "    Ashley Gritzman 19/10/2018\n",
    "  Args: \n",
    "    bin_routing_map: \n",
    "      binary routing map with children as rows and parents as columns\n",
    "  Returns:\n",
    "    children_per_parents: \n",
    "      parents are rows, and the indexes in the row are which children belong to       that parent\n",
    "  \"\"\"\n",
    "  \n",
    "  tmp = np.where(np.transpose(bin_routing_map))\n",
    "  children_per_parent = np.reshape(tmp[1],[bin_routing_map.shape[1], -1])\n",
    "  \n",
    "  return children_per_parent\n",
    "\n",
    "\n",
    "def init_rr(spatial_routing_matrix, child_caps, parent_caps):\n",
    "  \"\"\"Initialise routing weights.\n",
    "  \n",
    "  Initialise routing weights taking into accout spatial position of child \n",
    "  capsules. Child capsules in the corners only go to one parent capsule, while \n",
    "  those in the middle can go to kernel*kernel capsules.\n",
    "  \n",
    "  Author:\n",
    "    Ashley Gritzman 19/10/2018\n",
    "    \n",
    "  Args: \n",
    "    spatial_routing_matrix: \n",
    "      A 2D numpy matrix containing mapping between children capsules along the \n",
    "      rows, and parent capsules along the columns.\n",
    "      (child_space^2, parent_space^2)\n",
    "      (7*7, 5*5)\n",
    "    child_caps: number of child capsules along depth dimension\n",
    "    parent_caps: number of parent capsules along depth dimension\n",
    "    \n",
    "  Returns:\n",
    "    rr_initial: \n",
    "      initial routing weights\n",
    "      (1, parent_space, parent_space, kk, child_caps, parent_caps)\n",
    "      (1, 5, 5, 9, 8, 32)\n",
    "  \"\"\"\n",
    "\n",
    "  # Get spatial dimension of parent & child\n",
    "  parent_space_2 = int(spatial_routing_matrix.shape[1])\n",
    "  parent_space = int(np.sqrt(parent_space_2))\n",
    "  child_space_2 = int(spatial_routing_matrix.shape[0])\n",
    "  child_space = int(np.sqrt(child_space_2))\n",
    "\n",
    "  # Count the number of parents that each child belongs to\n",
    "  parents_per_child = np.sum(spatial_routing_matrix, axis=1, keepdims=True)\n",
    "\n",
    "  # Divide the vote of each child by the number of parents that it belongs to\n",
    "  # If the striding causes the filter not to fit, it will result in some  \n",
    "  # \"dropped\" child capsules, which effectively means child capsules that do not \n",
    "  # have any parents. This would create a divide by 0 scenario, so need to add \n",
    "  # 1e-9 to prevent NaNs.\n",
    "  rr_initial = (spatial_routing_matrix \n",
    "                / (parents_per_child * parent_caps + 1e-9))\n",
    "\n",
    "  # Convert the sparse matrix to be compatible with votes.\n",
    "  # This is done by selecting the child capsules belonging to each parent, which \n",
    "  # is achieved by selecting the non-zero values down each column. Need the \n",
    "  # combination of two transposes so that order is correct when reshaping\n",
    "  mask = spatial_routing_matrix.astype(bool)\n",
    "  rr_initial = rr_initial.T[mask.T]\n",
    "  rr_initial = np.reshape(rr_initial, [parent_space, parent_space, -1])\n",
    "\n",
    "  # Copy values across depth dimensions\n",
    "  # i.e. the number of child_caps and the number of parent_caps\n",
    "  # (5, 5, 9) -> (5, 5, 9, 8, 32)\n",
    "  rr_initial = rr_initial[..., np.newaxis, np.newaxis]\n",
    "  rr_initial = np.tile(rr_initial, [1, 1, 1, child_caps, parent_caps])\n",
    "  \n",
    "  # Add one mode dimension for batch size\n",
    "  rr_initial = np.expand_dims(rr_initial, 0)\n",
    "  \n",
    "  # Check the total of the routing weights is equal to the number of child \n",
    "  # capsules\n",
    "  # child_space * child_space * child_caps (minus the dropped ones)\n",
    "  dropped_child_caps = np.sum(np.sum(spatial_routing_matrix, axis=1) < 1e-9)\n",
    "  effective_child_cap = ((child_space*child_space - dropped_child_caps) \n",
    "                         * child_caps)\n",
    "  \n",
    "  sum_routing_weights = np.sum(rr_initial)\n",
    "  \n",
    "#   assert_op = tf.assert_less(\n",
    "#       np.abs(sum_routing_weights - effective_child_cap), 1e-9)\n",
    "#   with tf.control_dependencies([assert_op]):\n",
    "#     return rr_initial\n",
    "  \n",
    "  assert np.abs(sum_routing_weights - effective_child_cap) < 1e-3\n",
    "  \n",
    "  return rr_initial\n",
    "\n",
    "\n",
    "def to_sparse(probs, spatial_routing_matrix, sparse_filler=tf.math.log(1e-20)):\n",
    "  \"\"\"Convert probs tensor to sparse along child_space dimension.\n",
    "  \n",
    "  Consider a probs tensor of shape (64, 6, 6, 3*3, 32, 16). \n",
    "  (batch_size, parent_space, parent_space, kernel*kernel, child_caps, \n",
    "  parent_caps)\n",
    "  The tensor contains the probability of each child capsule belonging to a \n",
    "  particular parent capsule. We want to be able to sum the total probabilities \n",
    "  for a single child capsule to all the parent capsules. So we need to convert \n",
    "  the 3*3 spatial locations have been condensed, into a sparse format across\n",
    "  all child spatial location e.g. 14*14. \n",
    "  \n",
    "  Since we are working in log space, we must replace the zeros that come about \n",
    "  during sparse with log(0). The 'sparse_filler' option allows us to specify the \n",
    "  number to use to fill.\n",
    "  \n",
    "  Author:\n",
    "    Ashley Gritzman 01/11/2018\n",
    "    \n",
    "  Args: \n",
    "    probs: \n",
    "      tensor of log probabilities of each child capsule belonging to a \n",
    "      particular parent capsule\n",
    "      (batch_size, parent_space, parent_space, kernel*kernel, child_caps, \n",
    "      parent_caps)\n",
    "      (64, 5, 5, 3*3, 32, 16)\n",
    "    spatial_routing_matrix: \n",
    "      binary routing map with children as rows and parents as columns\n",
    "    sparse_filler: \n",
    "      the number to use to fill in the sparse locations instead of zero\n",
    "      \n",
    "  Returns:\n",
    "    sparse: \n",
    "      the sparse representation of the probs tensor in log space\n",
    "      (batch_size, parent_space, parent_space, child_space*child_space, \n",
    "      child_caps, parent_caps)\n",
    "      (64, 5, 5, 7*7, 32, 16)\n",
    "  \"\"\"\n",
    "  \n",
    "  # Get shapes of probs\n",
    "  shape = probs.get_shape().as_list()\n",
    "  batch_size = shape[0]\n",
    "  parent_space = shape[1]\n",
    "  kk = shape[3]\n",
    "  child_caps = shape[4]\n",
    "  parent_caps = shape[5]\n",
    "  \n",
    "  # Get spatial dimesion of child capsules\n",
    "  child_space_2 = int(spatial_routing_matrix.shape[0])\n",
    "  parent_space_2 = int(spatial_routing_matrix.shape[1])\n",
    "  \n",
    "  # Unroll the probs along the spatial dimension\n",
    "  # e.g. (64, 6, 6, 3*3, 8, 32) -> (64, 6*6, 3*3, 8, 32)\n",
    "  probs_unroll = tf.reshape(\n",
    "      probs, \n",
    "      [batch_size, parent_space_2, kk, child_caps, parent_caps])\n",
    "  \n",
    "  # Each row contains the children belonging to one parent\n",
    "  child_to_parent_idx = group_children_by_parent(spatial_routing_matrix)\n",
    "\n",
    "  # Create an index mapping each capsule to the correct sparse location\n",
    "  # Each element of the index must contain [batch_position, \n",
    "  # parent_space_position, child_sparse_position]\n",
    "  # E.g. [63, 24, 49] maps image 63, parent space 24, sparse position 49\n",
    "  child_sparse_idx = child_to_parent_idx\n",
    "  child_sparse_idx = child_sparse_idx[np.newaxis,...]\n",
    "  child_sparse_idx = np.tile(child_sparse_idx, [batch_size,1,1])\n",
    "\n",
    "  parent_idx = np.arange(parent_space_2)\n",
    "  parent_idx = np.reshape(parent_idx,[-1,1])\n",
    "  parent_idx = np.repeat(parent_idx, kk)\n",
    "  parent_idx = np.tile(parent_idx, batch_size)\n",
    "  parent_idx = np.reshape(parent_idx,[batch_size,parent_space_2,kk])\n",
    "\n",
    "  batch_idx = np.arange(batch_size)\n",
    "  batch_idx = np.reshape(batch_idx, [-1,1])\n",
    "  batch_idx = np.tile(batch_idx, parent_space_2*kk)\n",
    "  batch_idx = np.reshape(batch_idx, [batch_size,parent_space_2,kk])\n",
    "\n",
    "  # Combine the 3 coordinates\n",
    "  indices = np.stack((batch_idx, parent_idx, child_sparse_idx),axis=3)\n",
    "  indices = tf.constant(indices)\n",
    "\n",
    "  # Convert each spatial location to sparse\n",
    "  shape = [batch_size, parent_space_2, child_space_2, child_caps, parent_caps]\n",
    "  sparse = tf.scatter_nd(indices, probs_unroll, shape)\n",
    "  \n",
    "  # scatter_nd pads the output with zeros, but since we are operating\n",
    "  # in log space, we need to replace 0 with log(0), or log(1e-9)\n",
    "  zeros_in_log = tf.ones_like(sparse, dtype=tf.float32) * sparse_filler\n",
    "  sparse = tf.where(tf.equal(sparse, 0.0), zeros_in_log, sparse)\n",
    "  \n",
    "  # Reshape\n",
    "  # (64, 5*5, 7*7, 8, 32) -> (64, 6, 6, 14*14, 8, 32)\n",
    "  sparse = tf.reshape(sparse, [batch_size, parent_space, parent_space, child_space_2, child_caps, parent_caps])\n",
    "  \n",
    "  # Checks\n",
    "  # 1. Shape\n",
    "  assert sparse.get_shape().as_list() == [batch_size, parent_space, parent_space, child_space_2, child_caps, parent_caps]\n",
    "  \n",
    "  # This check no longer holds since we have replaced zeros with log(1e-9), so \n",
    "  # the total of dense and sparse no longer match.\n",
    "  # 2. Total of dense and sparse must be the same\n",
    "#   pct_delta = tf.abs(\n",
    "#     (tf.reduce_sum(probs) - tf.reduce_sum(sparse))\n",
    "#     /tf.reduce_sum(probs))\n",
    "\n",
    "#   assert_op = tf.assert_less(\n",
    "#       pct_delta, \n",
    "#       1e-4, \n",
    "#       message=\"in fn to_sparse: total of probs and sparse are different\", \n",
    "#       data=[pct_delta, tf.reduce_sum(probs), tf.reduce_sum(sparse)])\n",
    "#   with tf.control_dependencies([assert_op]):\n",
    "#      sparse = tf.identity(sparse)\n",
    "  \n",
    "  return sparse\n",
    "  \n",
    "  \n",
    "def normalise_across_parents(probs_sparse, spatial_routing_matrix):\n",
    "  \"\"\"Normalise across all parent capsules including spatial and depth.\n",
    "  \n",
    "  Consider a sparse matrix of probabilities (1, 5, 5, 49, 8, 32)  \n",
    "  (batch_size, parent_space, parent_space, child_space*child_space, child_caps,   parent_caps)  \n",
    "  For one child capsule, we need to normalise across all parent capsules that \n",
    "  receive output from that child. This includes the depth of parent capsules, \n",
    "  and the spacial dimension od parent capsules. In the example matrix of \n",
    "  probabilities above this would mean normalising across [1, 2, 5] or \n",
    "  [parent_space, parent_space, parent_caps]. \n",
    "  \n",
    "  Author:\n",
    "    Ashley Gritzman 05/11/2018\n",
    "  Args: \n",
    "    probs_sparse: \n",
    "      the sparse representation of the probs matrix, not in log\n",
    "      (batch_size, parent_space, parent_space, child_space*child_space, \n",
    "      child_caps, parent_caps) \n",
    "      (64, 5, 5, 49, 8, 32)\n",
    "             \n",
    "  Returns:\n",
    "    rr_updated: \n",
    "      softmax across all parent capsules, same shape as input\n",
    "      (batch_size, parent_space, parent_space, child_space*child_space, \n",
    "      child_caps, parent_caps) \n",
    "      (64, 5, 5, 49, 8, 32)\n",
    "  \"\"\"\n",
    "  \n",
    "  # e.g. (1, 5, 5, 49, 8, 32)\n",
    "  # (batch_size, parent_space, parent_space, child_space*child_space, child_caps, parent_caps) \n",
    "  shape = probs_sparse.get_shape().as_list()\n",
    "  batch_size = shape[0]\n",
    "  parent_space = shape[1]\n",
    "  child_space_2 = shape[3]  # squared\n",
    "  child_caps = shape[4]\n",
    "  parent_caps = shape[5]\n",
    "  \n",
    "  rr_updated = probs_sparse/(tf.reduce_sum(probs_sparse, \n",
    "                                           axis=[1,2,5], \n",
    "                                           keepdims=True) + 1e-9)\n",
    "  \n",
    "  # Checks\n",
    "  # 1. Shape\n",
    "  assert (rr_updated.get_shape().as_list() \n",
    "          == [batch_size, parent_space, parent_space, child_space_2, \n",
    "              child_caps, parent_caps])\n",
    "  \n",
    "  # 2. Total of routing weights must equal number of child capsules minus \n",
    "  # dropped ones. \n",
    "  # Because of numerical issues it is not likely that the routing weights will \n",
    "  # equal the calculated number of capsules, so we check that it is within a \n",
    "  # certain percent.\n",
    "  dropped_child_caps = np.sum(np.sum(spatial_routing_matrix, axis=1) < 1e-9)\n",
    "  #effective_child_caps = (child_space_2 - dropped_child_caps) * child_caps * \n",
    "  # batch_size\n",
    "  effective_child_caps = (child_space_2 - dropped_child_caps) * child_caps\n",
    "  effective_child_caps = tf.to_double(effective_child_caps)\n",
    "  \n",
    "  sum_routing_weights = tf.reduce_sum(tf.to_double(rr_updated), \n",
    "                                      axis=[1,2,3,4,5])\n",
    "  \n",
    "  pct_delta = tf.abs((effective_child_caps - sum_routing_weights) \n",
    "                     / effective_child_caps)\n",
    "\n",
    "#   assert_op = tf.assert_less(\n",
    "#       pct_delta, \n",
    "#       tf.to_double(0.01), \n",
    "#       message=\"\"\"function normalise_across_parents: total of routing weights \n",
    "#               not equal to number of child capsules\"\"\",\n",
    "#       data=[pct_delta, sum_routing_weights, effective_child_caps, \n",
    "#             tf.reduce_min(sum_routing_weights)], \n",
    "#       summarize=10)\n",
    "#   with tf.control_dependencies([assert_op]):\n",
    "#       rr_updated = tf.identity(rr_updated)\n",
    "  \n",
    "  return rr_updated\n",
    "\n",
    "\n",
    "def softmax_across_parents(probs_sparse, spatial_routing_matrix):\n",
    "  \"\"\"Softmax across all parent capsules including spatial and depth.\n",
    "  \n",
    "  Consider a sparse matrix of probabilities (1, 5, 5, 49, 8, 32)  \n",
    "  (batch_size, parent_space, parent_space, child_space*child_space, child_caps,   parent_caps)  \n",
    "  For one child capsule, we need to normalise across all parent capsules that \n",
    "  receive output from that child. This includes the depth of parent capsules, \n",
    "  and the spacial dimension od parent capsules. In the example matrix of \n",
    "  probabilities above this would mean normalising across [1, 2, 5] or \n",
    "  [parent_space, parent_space, parent_caps]. But the softmax function \n",
    "  `tf.nn.softmax` can only operate across one axis, so we need to reshape the \n",
    "  matrix such that we can combine paret_space and parent_caps into one axis. \n",
    "  \n",
    "  Author:\n",
    "    Ashley Gritzman 05/11/2018\n",
    "    \n",
    "  Args: \n",
    "    probs_sparse: \n",
    "      the sparse representation of the probs matrix, in log\n",
    "      (batch_size, parent_space, parent_space, child_space*child_space, \n",
    "      child_caps, parent_caps) \n",
    "      (64, 5, 5, 49, 8, 32)\n",
    "             \n",
    "  Returns:\n",
    "    rr_updated: \n",
    "      softmax across all parent capsules, same shape as input\n",
    "      (batch_size, parent_space, parent_space, child_space*child_space, \n",
    "      child_caps, parent_caps) \n",
    "      (64, 5, 5, 49, 8, 32)\n",
    "  \"\"\"\n",
    "  \n",
    "  # e.g. (1, 5, 5, 49, 8, 32)\n",
    "  # (batch_size, parent_space, parent_space, child_space*child_space, \n",
    "  # child_caps, parent_caps) \n",
    "  shape = probs_sparse.get_shape().as_list()\n",
    "  batch_size = shape[0]\n",
    "  parent_space = shape[1]\n",
    "  child_space_2 = shape[3]  # squared\n",
    "  child_caps = shape[4]\n",
    "  parent_caps = shape[5]\n",
    "  \n",
    "  # Move parent space dimensions, and parent depth dimension to end\n",
    "  # (1, 5, 5, 49, 8, 32)  -> (1, 49, 4, 5, 5, 3)\n",
    "  sparse = tf.transpose(probs_sparse, perm=[0,3,4,1,2,5])\n",
    "  \n",
    "  # Combine parent \n",
    "  # (1, 49, 4, 75)\n",
    "  sparse = tf.reshape(sparse, [batch_size, child_space_2, child_caps, -1])\n",
    "  \n",
    "  # Perform softmax across parent capsule dimension\n",
    "  parent_softmax = tf.nn.softmax(sparse, axis=-1)\n",
    "  \n",
    "  # Uncombine parent space and depth\n",
    "  # (1, 49, 4, 5, 5, 3)\n",
    "  parent_softmax = tf.reshape(\n",
    "    parent_softmax, \n",
    "    [batch_size, child_space_2, child_caps, parent_space, parent_space, \n",
    "     parent_caps])\n",
    "  \n",
    "  # Return to original order\n",
    "  # (1, 5, 5, 49, 8, 32)\n",
    "  parent_softmax = tf.transpose(parent_softmax, perm=[0,3,4,1,2,5])\n",
    "  \n",
    "  # Softmax across the parent capsules actually gives us the updated routing \n",
    "  # weights\n",
    "  rr_updated = parent_softmax\n",
    "  \n",
    "  # Checks\n",
    "  # 1. Shape\n",
    "  assert (rr_updated.get_shape().as_list() \n",
    "          == [batch_size, parent_space, parent_space, child_space_2, \n",
    "              child_caps, parent_caps])\n",
    "  \n",
    "  # 2. Check the total of the routing weights is equal to the number of child \n",
    "  # capsules\n",
    "  # Note: during convolution some child capsules may be dropped if the \n",
    "  # convolution doesn't fit nicely. So in the sparse form of child capsules, the   # dropped capsules will be 0 everywhere. When we do a softmax, these capsules\n",
    "  # will then be given a value, so when we check the total child capsules we \n",
    "  # need to include these. But these will then be excluded when we convert back   # to dense so it's not a problem. \n",
    "  total_child_caps = tf.to_float(child_space_2 * child_caps * batch_size)\n",
    "  sum_routing_weights = tf.round(tf.reduce_sum(rr_updated))\n",
    "  \n",
    "#   assert_op = tf.assert_equal(\n",
    "#       sum_routing_weights, \n",
    "#       total_child_caps,\n",
    "#       message=\"\"\"in fn softmax_across_parents: sum_routing_weights and \n",
    "#               effective_child_caps are different\"\"\")\n",
    "#   with tf.control_dependencies([assert_op]):\n",
    "#      rr_updated = tf.identity(rr_updated)\n",
    "  \n",
    "  return rr_updated   \n",
    "\n",
    "\n",
    "def to_dense(sparse, spatial_routing_matrix):\n",
    "  \"\"\"Convert sparse back to dense along child_space dimension.\n",
    "  \n",
    "  Consider a sparse probs tensor of shape (64, 5, 5, 49, 8, 32).\n",
    "  (batch_size, parent_space, parent_space, child_space*child_space, child_caps,\n",
    "  parent_caps) \n",
    "  The tensor contains all child capsules at every parent spatial location, but \n",
    "  if the child does not route to the parent then it is just zero at that spot.\n",
    "  Now we want to get back to the dense representation:\n",
    "  (64, 5, 5, 49, 8, 32) -> (64, 5, 5, 9, 8, 32)\n",
    "  \n",
    "  Author:\n",
    "    Ashley Gritzman 05/11/2018\n",
    "  Args: \n",
    "    sparse: \n",
    "      the sparse representation of the probs tensor\n",
    "      (batch_size, parent_space, parent_space, child_space*child_space, \n",
    "      child_caps, parent_caps) \n",
    "      (64, 5, 5, 49, 8, 32)\n",
    "    spatial_routing_matrix: \n",
    "      binary routing map with children as rows and parents as columns\n",
    "      \n",
    "  Returns:\n",
    "    dense: \n",
    "      the dense representation of the probs tensor\n",
    "      (batch_size, parent_space, parent_space, kk, child_caps, parent_caps) \n",
    "      (64, 5, 5, 9, 8, 32)\n",
    "  \"\"\"\n",
    "  \n",
    "  # Get shapes of probs\n",
    "  shape = sparse.get_shape().as_list()\n",
    "  batch_size = shape[0]\n",
    "  parent_space = shape[1]\n",
    "  child_space_2 = shape[3] #squared\n",
    "  child_caps = shape[4]\n",
    "  parent_caps = shape[5]\n",
    "  \n",
    "  # Calculate kernel size by adding up column of spatial routing matrix\n",
    "  kk = int(np.sum(spatial_routing_matrix[:,0]))\n",
    "  \n",
    "  # Unroll parent spatial dimensions\n",
    "  # (64, 5, 5, 49, 8, 32) -> (64, 5*5, 49, 8, 32)\n",
    "  sparse_unroll = tf.reshape(sparse, [batch_size, parent_space*parent_space, \n",
    "                                      child_space_2, child_caps, parent_caps])\n",
    "  \n",
    "  \n",
    "  # Apply boolean_mask on axis 1 and 2\n",
    "  # sparse_unroll: (64, 5*5, 49, 8, 32)\n",
    "  # spatial_routing_matrix: (49, 25) -> (25, 49)\n",
    "  # dense: (64, 5*5, 49, 8, 32) -> (64, 5*5*9, 8, 32)\n",
    "  dense = tf.boolean_mask(sparse_unroll, \n",
    "                          tf.transpose(spatial_routing_matrix), axis=1)\n",
    "  \n",
    "  # Reshape\n",
    "  dense = tf.reshape(dense, [batch_size, parent_space, parent_space, kk, \n",
    "                             child_caps, parent_caps])    \n",
    "  \n",
    "  # Checks\n",
    "  # 1. Shape\n",
    "  assert (dense.get_shape().as_list() \n",
    "          == [batch_size, parent_space, parent_space, kk, child_caps, \n",
    "              parent_caps])\n",
    "  \n",
    "#   # 2. Total of dense and sparse must be the same\n",
    "#   delta = tf.abs(tf.reduce_sum(dense, axis=[3]) \n",
    "#                  - tf.reduce_sum(sparse, axis=[3]))\n",
    "#   assert_op = tf.assert_less(\n",
    "#       delta, \n",
    "#       1e-6,\n",
    "#       message=\"in fn to_dense: total of dense and sparse are different\",\n",
    "#       data=[tf.reduce_sum(dense,[1,2,3,4,5]), \n",
    "#             tf.reduce_sum(sparse,[1,2,3,4,5]), \n",
    "#             tf.reduce_sum(dense),tf.reduce_sum(sparse)],\n",
    "#       summarize=10)\n",
    "#   with tf.control_dependencies([assert_op]):\n",
    "#      dense = tf.identity(dense)\n",
    "      \n",
    "  return dense  \n",
    "\n",
    "\n",
    "def logits_one_vs_rest(logits, positive_class = 0):\n",
    "  \"\"\"Return the logit from the positive class and the maximum logit from the \n",
    "  other classes.\n",
    "  \n",
    "  This function is used to prepare the logits from a multi class classifier to \n",
    "  be used for binary classification. The logits from the positive class are \n",
    "  placed in column 0. The maximum logit from the remaining classes is placed in   column 1.\n",
    "  \n",
    "  Author:\n",
    "    Ashley Gritzman 04/12/2018\n",
    "  Args: \n",
    "    logits_all: logits from multiple classes\n",
    "    positive_class: the index of the positive class\n",
    "  Returns:\n",
    "    logits_one_vs_rest: \n",
    "      logits from positive class in column 0, and maximum logits of other \n",
    "      classes in column 1  \n",
    "  \"\"\"\n",
    "  \n",
    "  logits_positive = tf.reshape(logits[:,positive_class], [-1,1])\n",
    "  \n",
    "  logits_rest = tf.concat([logits[:,:positive_class], \n",
    "                           logits[:,(positive_class+1):]], axis=1)\n",
    "  logits_rest_max = tf.reduce_max(logits_rest, axis=1, keepdims=True)\n",
    "\n",
    "  logits_one_vs_rest = tf.concat([logits_positive, logits_rest_max], axis=1)\n",
    "  \n",
    "  return logits_one_vs_rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def em_routing(votes_ij, activations_i, batch_size, spatial_routing_matrix):\n",
    "  \"\"\"The EM routing between input capsules (i) and output capsules (j).\n",
    "  \n",
    "  See Hinton et al. \"Matrix Capsules with EM Routing\" for detailed description \n",
    "  of EM routing.\n",
    "  \n",
    "  Author:\n",
    "    Ashley Gritzman 19/10/2018\n",
    "  Definitions:\n",
    "    N -> number of samples in batch\n",
    "    OH -> output height\n",
    "    OW -> output width\n",
    "    kh -> kernel height\n",
    "    kw -> kernel width\n",
    "    kk -> kh * kw\n",
    "    i -> number of input capsules, also called \"child_caps\"\n",
    "    o -> number of output capsules, also called \"parent_caps\"\n",
    "    child_space -> spatial dimensions of input capsule layer i\n",
    "    parent_space -> spatial dimensions of output capsule layer j\n",
    "    n_channels -> number of channels in pose matrix (usually 4x4=16)\n",
    "  Args: \n",
    "    votes_ij: \n",
    "      votes from capsules in layer i to capsules in layer j\n",
    "      For conv layer:\n",
    "        (N*OH*OW, kh*kw*i, o, 4x4)\n",
    "        (64*6*6, 9*8, 32, 16)\n",
    "      For FC layer:\n",
    "        The kernel dimensions are equal to the spatial dimensions of the input \n",
    "        layer i, and the spatial dimensions of the output layer j are 1x1.\n",
    "        (N*1*1, child_space*child_space*i, o, 4x4)\n",
    "        (64, 4*4*16, 5, 16)\n",
    "    activations_i: \n",
    "      activations of capsules in layer i (L)\n",
    "      (N*OH*OW, kh*kw*i, 1)\n",
    "      (64*6*6, 9*8, 1)\n",
    "    batch_size: \n",
    "    spatial_routing_matrix: \n",
    "  Returns:\n",
    "    poses_j: \n",
    "      poses of capsules in layer j (L+1)\n",
    "      (N, OH, OW, o, 4x4) \n",
    "      (64, 6, 6, 32, 16)\n",
    "    activations_j: \n",
    "      activations of capsules in layer j (L+1)\n",
    "      (N, OH, OW, o, 1)\n",
    "      (64, 6, 6, 32, 1)\n",
    "  \"\"\"\n",
    "  \n",
    "  #----- Dimensions -----#\n",
    "  \n",
    "  # Get dimensions needed to do conversions\n",
    "  N = batch_size\n",
    "  votes_shape = votes_ij.get_shape().as_list()\n",
    "  OH = np.sqrt(int(votes_shape[0]) / N)\n",
    "  OH = int(OH)\n",
    "  OW = np.sqrt(int(votes_shape[0]) / N)\n",
    "  OW = int(OW)\n",
    "  kh_kw_i = int(votes_shape[1])\n",
    "  o = int(votes_shape[2])\n",
    "  n_channels = int(votes_shape[3])\n",
    "  \n",
    "  # Calculate kernel size by adding up column of spatial routing matrix\n",
    "  # Do this before conventing the spatial_routing_matrix to tf\n",
    "  kk = int(np.sum(spatial_routing_matrix[:,0]))\n",
    "  \n",
    "  parent_caps = o\n",
    "  child_caps = int(kh_kw_i/kk)\n",
    "  \n",
    "  rt_mat_shape = spatial_routing_matrix.shape\n",
    "  child_space_2 = rt_mat_shape[0]\n",
    "  child_space = int(np.sqrt(child_space_2))\n",
    "  parent_space_2 = rt_mat_shape[1]\n",
    "  parent_space = int(np.sqrt(parent_space_2))\n",
    "   \n",
    "  \n",
    "  #----- Reshape Inputs -----#\n",
    "\n",
    "  # conv: (N*OH*OW, kh*kw*i, o, 4x4) -> (N, OH, OW, kh*kw*i, o, 4x4)\n",
    "  # FC: (N, child_space*child_space*i, o, 4x4) -> (N, 1, 1, child_space*child_space*i, output_classes, 4x4)\n",
    "  votes_ij = tf.reshape(votes_ij, [N, OH, OW, kh_kw_i, o, n_channels]) \n",
    "  \n",
    "  # (N*OH*OW, kh*kw*i, 1) -> (N, OH, OW, kh*kw*i, o, n_channels)\n",
    "  #              (24, 6, 6, 288, 1, 1)\n",
    "  activations_i = tf.reshape(activations_i, [N, OH, OW, kh_kw_i, 1, 1])\n",
    "  \n",
    "\n",
    "  #----- Betas -----#\n",
    "\n",
    "  \"\"\"\n",
    "  # Initialization from Jonathan Hui [1]:\n",
    "  beta_v_hui = tf.get_variable(\n",
    "    name='beta_v', \n",
    "    shape=[1, 1, 1, o], \n",
    "    dtype=tf.float32,\n",
    "    initializer=tf.contrib.layers.xavier_initializer())\n",
    "  beta_a_hui = tf.get_variable(\n",
    "    name='beta_a', \n",
    "    shape=[1, 1, 1, o], \n",
    "    dtype=tf.float32,\n",
    "    initializer=tf.contrib.layers.xavier_initializer())\n",
    "                              \n",
    "  # AG 21/11/2018: \n",
    "  # Tried to find std according to Hinton's comments on OpenReview \n",
    "  # https://openreview.net/forum?id=HJWLfGWRb&noteId=r1lQjCAChm\n",
    "  # Hinton: \"We used truncated_normal_initializer and set the std so that at the \n",
    "  # start of training half of the capsules in each layer are active and half \n",
    "  # inactive (for the Primary Capsule layer where the activation is not computed \n",
    "  # through routing we use different std for activation convolution weights & \n",
    "  # for pose parameter convolution weights).\"\n",
    "  # \n",
    "  # std beta_v seems to control the spread of activations\n",
    "  # To try and achieve what Hinton said about half active and half not active,\n",
    "  # I change the std values and check the histogram/distributions in \n",
    "  # Tensorboard\n",
    "  # to try and get a good spread across all values. I couldn't get this working\n",
    "  # nicely.\n",
    "  beta_v_hui = slim.model_variable(\n",
    "    name='beta_v', \n",
    "    shape=[1, 1, 1, 1, o, 1], \n",
    "    dtype=tf.float32,\n",
    "    initializer=tf.truncated_normal_initializer(mean=0.0, stddev=10.0))\n",
    "  \"\"\"\n",
    "  beta_a = slim.model_variable(\n",
    "    name='beta_a', \n",
    "    shape=[1, 1, 1, 1, o, 1], \n",
    "    dtype=tf.float32, \n",
    "    initializer=tf.truncated_normal_initializer(mean=-1000.0, stddev=500.0))\n",
    "  \n",
    "  # AG 04/10/2018: using slim.variable to create instead of tf.get_variable so \n",
    "  # that they get correctly placed on the CPU instead of GPU in the multi-gpu \n",
    "  # version.\n",
    "  # One beta per output capsule type\n",
    "  # (1, 1, 1, 1, 32, 1)\n",
    "  # (N, OH, OH, i, o, n_channels)\n",
    "  beta_v = slim.model_variable(\n",
    "    name='beta_v', \n",
    "    shape=[1, 1, 1, 1, o, 1], \n",
    "    dtype=tf.float32,            \n",
    "    initializer=tf.contrib.layers.xavier_initializer(),\n",
    "    regularizer=None)\n",
    "  \"\"\"\n",
    "  beta_a = slim.model_variable(\n",
    "    name='beta_a', \n",
    "    shape=[1, 1, 1, 1, o, 1], \n",
    "    dtype=tf.float32, \n",
    "    initializer=tf.contrib.layers.xavier_initializer(),\n",
    "    regularizer=None)\n",
    "  \"\"\"\n",
    "\n",
    "  with tf.variable_scope(\"em_routing\") as scope:\n",
    "    # Initialise routing assignments\n",
    "    # rr (1, 6, 6, 9, 8, 16) \n",
    "    #  (1, parent_space, parent_space, kk, child_caps, parent_caps)\n",
    "    rr = utl.init_rr(spatial_routing_matrix, child_caps, parent_caps)\n",
    "    \n",
    "    # Need to reshape (1, 6, 6, 9, 8, 16) -> (1, 6, 6, 9*8, 16, 1)\n",
    "    rr = np.reshape(\n",
    "      rr, \n",
    "      [1, parent_space, parent_space, kk*child_caps, parent_caps, 1])\n",
    "    \n",
    "    # Convert rr from np to tf\n",
    "    rr = tf.constant(rr, dtype=tf.float32)\n",
    "    \n",
    "    for it in range(FLAGS.iter_routing):  \n",
    "      # AG 17/09/2018: modified schedule for inverse_temperature (lambda) based\n",
    "      # on Hinton's response to questions on OpenReview.net: \n",
    "      # https://openreview.net/forum?id=HJWLfGWRb\n",
    "      # \"the formula we used for lambda is:\n",
    "      # lambda = final_lambda * (1 - tf.pow(0.95, tf.cast(i + 1, tf.float32)))\n",
    "      # where 'i' is the routing iteration (range is 0-2). Final_lambda is set \n",
    "      # to 0.01.\"\n",
    "      # final_lambda = 0.01\n",
    "      final_lambda = FLAGS.final_lambda\n",
    "      inverse_temperature = (final_lambda * \n",
    "                             (1 - tf.pow(0.95, tf.cast(it + 1, tf.float32))))\n",
    "\n",
    "      # AG 26/06/2018: added var_j\n",
    "      activations_j, mean_j, stdv_j, var_j = m_step(\n",
    "        rr, \n",
    "        votes_ij, \n",
    "        activations_i, \n",
    "        beta_v, beta_a, \n",
    "        inverse_temperature=inverse_temperature)\n",
    "      \n",
    "      # We skip the e_step call in the last iteration because we only need to \n",
    "      # return the a_j and the mean from the m_stp in the last iteration to \n",
    "      # compute the output capsule activation and pose matrices  \n",
    "      if it < FLAGS.iter_routing - 1:\n",
    "        rr = e_step(votes_ij, \n",
    "                    activations_j, \n",
    "                    mean_j, \n",
    "                    stdv_j, \n",
    "                    var_j, \n",
    "                    spatial_routing_matrix)\n",
    "\n",
    "    # pose: (N, OH, OW, o, 4 x 4) via squeeze mean_j (24, 6, 6, 32, 16)\n",
    "    poses_j = tf.squeeze(mean_j, axis=-3, name=\"poses\")\n",
    "\n",
    "    # activation: (N, OH, OW, o, 1) via squeeze o_activation is \n",
    "    # [24, 6, 6, 32, 1]\n",
    "    activations_j = tf.squeeze(activations_j, axis=-3, name=\"activations\")\n",
    "\n",
    "  return poses_j, activations_j\n",
    "\n",
    "\n",
    "def m_step(rr, votes, activations_i, beta_v, beta_a, inverse_temperature):\n",
    "  \"\"\"The m-step in EM routing between input capsules (i) and output capsules \n",
    "  (j).\n",
    "  \n",
    "  Compute the activations of the output capsules (j), and the Gaussians for the\n",
    "  pose of the output capsules (j).\n",
    "  See Hinton et al. \"Matrix Capsules with EM Routing\" for detailed description \n",
    "  of m-step.\n",
    "  \n",
    "  Author:\n",
    "    Ashley Gritzman 19/10/2018\n",
    "    \n",
    "  Args: \n",
    "    rr: \n",
    "      assignment weights between capsules in layer i and layer j\n",
    "      (N, OH, OW, kh*kw*i, o, 1)\n",
    "      (64, 6, 6, 9*8, 16, 1)\n",
    "    votes_ij: \n",
    "      votes from capsules in layer i to capsules in layer j\n",
    "      For conv layer:\n",
    "        (N, OH, OW, kh*kw*i, o, 4x4)\n",
    "        (64, 6, 6, 9*8, 32, 16)\n",
    "      For FC layer:\n",
    "        The kernel dimensions are equal to the spatial dimensions of the input \n",
    "        layer i, and\n",
    "        the spatial dimensions of the output layer j are 1x1.\n",
    "        (N, 1, 1, child_space*child_space*i, output_classes, 4x4)\n",
    "        (64, 1, 1, 4*4*16, 5, 16)\n",
    "    activations_i: \n",
    "      activations of capsules in layer i (L)\n",
    "      (N, OH, OW, kh*kw*i, o, n_channels)\n",
    "      (24, 6, 6, 288, 1, 1)\n",
    "    beta_v: \n",
    "      Trainable parameters in computing cost \n",
    "      (1, 1, 1, 1, 32, 1)\n",
    "    beta_a: \n",
    "      Trainable parameters in computing next level activation \n",
    "      (1, 1, 1, 1, 32, 1)\n",
    "    inverse_temperature: lambda, increase over each iteration by the caller\n",
    "    \n",
    "  Returns:\n",
    "    activations_j: \n",
    "      activations of capsules in layer j (L+1)\n",
    "      (N, OH, OW, 1, o, 1)\n",
    "      (64, 6, 6, 1, 32, 1)\n",
    "    mean_j: \n",
    "      mean of each channel in capsules of layer j (L+1)\n",
    "      (N, OH, OW, 1, o, n_channels)\n",
    "      (24, 6, 6, 1, 32, 16)\n",
    "    stdv_j: \n",
    "      standard deviation of each channel in capsules of layer j (L+1)\n",
    "      (N, OH, OW, 1, o, n_channels)\n",
    "      (24, 6, 6, 1, 32, 16)\n",
    "    var_j: \n",
    "      variance of each channel in capsules of layer j (L+1)\n",
    "      (N, OH, OW, 1, o, n_channels)\n",
    "      (24, 6, 6, 1, 32, 16)\n",
    "  \"\"\"\n",
    "\n",
    "  with tf.variable_scope(\"m_step\") as scope:\n",
    "    \n",
    "    rr_prime = rr * activations_i\n",
    "    rr_prime = tf.identity(rr_prime, name=\"rr_prime\")\n",
    "\n",
    "    # rr_prime_sum: sum over all input capsule i\n",
    "    rr_prime_sum = tf.reduce_sum(rr_prime, \n",
    "                                 axis=-3, \n",
    "                                 keepdims=True, \n",
    "                                 name='rr_prime_sum')\n",
    "    \n",
    "    # AG 13/12/2018: normalise amount of information\n",
    "    # The amount of information given to parent capsules is very different for \n",
    "    # the final \"class-caps\" layer. Since all the spatial capsules give output \n",
    "    # to just a few class caps, they receive a lot more information than the \n",
    "    # convolutional layers. So in order for lambda and beta_v/beta_a settings to \n",
    "    # apply to this layer, we must normalise the amount of information.\n",
    "    # activ from convcaps1 to convcaps2 (64*5*5, 144, 16, 1) 144/16 = 9 info\n",
    "    # (N*OH*OW, kh*kw*i, o, 1)\n",
    "    # activ from convcaps2 to classcaps (64, 1, 1, 400, 5, 1) 400/5 = 80 info\n",
    "    # (N, 1, 1, IH*IW*i, n_classes, 1)\n",
    "    child_caps = float(rr_prime.get_shape().as_list()[-3])\n",
    "    parent_caps = float(rr_prime.get_shape().as_list()[-2])\n",
    "    ratio_child_to_parent =  child_caps/parent_caps\n",
    "    layer_norm_factor = 100/ratio_child_to_parent\n",
    "    # logger.info(\"ratio_child_to_parent: {}\".format(ratio_child_to_parent))\n",
    "    # rr_prime_sum = rr_prime_sum/ratio_child_to_parent\n",
    "\n",
    "    # mean_j: (24, 6, 6, 1, 32, 16)\n",
    "    mean_j_numerator = tf.reduce_sum(rr_prime * votes, \n",
    "                                     axis=-3, \n",
    "                                     keepdims=True, \n",
    "                                     name=\"mean_j_numerator\")\n",
    "    mean_j = tf.div(mean_j_numerator, \n",
    "                    rr_prime_sum + FLAGS.epsilon, \n",
    "                    name=\"mean_j\")\n",
    "    \n",
    "    #----- AG 26/06/2018 START -----#\n",
    "    # Use variance instead of standard deviation, because the sqrt seems to \n",
    "    # cause NaN gradients during backprop.\n",
    "    # See original implementation from Suofei below\n",
    "    var_j_numerator = tf.reduce_sum(rr_prime * tf.square(votes - mean_j), \n",
    "                                    axis=-3, \n",
    "                                    keepdims=True, \n",
    "                                    name=\"var_j_numerator\")\n",
    "    var_j = tf.div(var_j_numerator, \n",
    "                   rr_prime_sum + FLAGS.epsilon, \n",
    "                   name=\"var_j\")\n",
    "    \n",
    "    # Set the minimum variance (note: variance should always be positive)\n",
    "    # This should allow me to remove the FLAGS.epsilon safety from log and div \n",
    "    # that follow\n",
    "    #var_j = tf.maximum(var_j, FLAGS.epsilon)\n",
    "    #var_j = var_j + FLAGS.epsilon\n",
    "    \n",
    "    ###################\n",
    "    #var_j = var_j + 1e-5\n",
    "    var_j = tf.identity(var_j + 1e-9, name=\"var_j_epsilon\")\n",
    "    ###################\n",
    "    \n",
    "    # Compute the stdv, but it shouldn't actually be used anywhere\n",
    "    # stdv_j = tf.sqrt(var_j)\n",
    "    stdv_j = None\n",
    "    \n",
    "    ######## layer_norm_factor\n",
    "    cost_j_h = (beta_v + 0.5*tf.math.log(var_j)) * rr_prime_sum * layer_norm_factor\n",
    "    cost_j_h = tf.identity(cost_j_h, name=\"cost_j_h\")\n",
    "    \n",
    "    # ----- END ----- #\n",
    "    \n",
    "    \"\"\"\n",
    "    # Original from Suofei (reference [3] at top)\n",
    "    # stdv_j: (24, 6, 6, 1, 32, 16)\n",
    "    stdv_j = tf.sqrt(\n",
    "      tf.reduce_sum(\n",
    "        rr_prime * tf.square(votes - mean_j), axis=-3, keepdims=True\n",
    "      ) / rr_prime_sum,\n",
    "      name=\"stdv_j\"\n",
    "    )\n",
    "    # cost_j_h: (24, 6, 6, 1, 32, 16)\n",
    "    cost_j_h = (beta_v + tf.log(stdv_j + FLAGS.epsilon)) * rr_prime_sum\n",
    "    \"\"\"\n",
    "    \n",
    "    # cost_j: (24, 6, 6, 1, 32, 1)\n",
    "    # activations_j_cost = (24, 6, 6, 1, 32, 1)\n",
    "    # yg: This is done for numeric stability.\n",
    "    # It is the relative variance between each channel determined which one \n",
    "    # should activate.\n",
    "    cost_j = tf.reduce_sum(cost_j_h, axis=-1, keepdims=True, name=\"cost_j\")\n",
    "    #cost_j_mean = tf.reduce_mean(cost_j, axis=-2, keepdims=True)\n",
    "    #cost_j_stdv = tf.sqrt(\n",
    "    #  tf.reduce_sum(\n",
    "    #    tf.square(cost_j - cost_j_mean), axis=-2, keepdims=True\n",
    "    #  ) / cost_j.get_shape().as_list()[-2]\n",
    "    #)\n",
    "    \n",
    "    # AG 17/09/2018: trying to remove normalisation\n",
    "    # activations_j_cost = beta_a + (cost_j_mean - cost_j) / (cost_j_stdv)\n",
    "    activations_j_cost = tf.identity(beta_a - cost_j, \n",
    "                                     name=\"activations_j_cost\")\n",
    "\n",
    "    # (24, 6, 6, 1, 32, 1)\n",
    "    activations_j = tf.sigmoid(inverse_temperature * activations_j_cost,\n",
    "                               name=\"sigmoid\")\n",
    "    \n",
    "    # AG 26/06/2018: added var_j to return\n",
    "    return activations_j, mean_j, stdv_j, var_j\n",
    "\n",
    "  \n",
    "# AG 26/06/2018: added var_j\n",
    "def e_step(votes_ij, activations_j, mean_j, stdv_j, var_j, spatial_routing_matrix):\n",
    "  \"\"\"The e-step in EM routing between input capsules (i) and output capsules (j).\n",
    "  \n",
    "  Update the assignment weights using in routung. The output capsules (j) \n",
    "  compete for the input capsules (i).\n",
    "  See Hinton et al. \"Matrix Capsules with EM Routing\" for detailed description \n",
    "  of e-step.\n",
    "  \n",
    "  Author:\n",
    "    Ashley Gritzman 19/10/2018\n",
    "    \n",
    "  Args: \n",
    "    votes_ij: \n",
    "      votes from capsules in layer i to capsules in layer j\n",
    "      For conv layer:\n",
    "        (N, OH, OW, kh*kw*i, o, 4x4)\n",
    "        (64, 6, 6, 9*8, 32, 16)\n",
    "      For FC layer:\n",
    "        The kernel dimensions are equal to the spatial dimensions of the input \n",
    "        layer i, and the spatial dimensions of the output layer j are 1x1.\n",
    "        (N, 1, 1, child_space*child_space*i, output_classes, 4x4)\n",
    "        (64, 1, 1, 4*4*16, 5, 16)\n",
    "    activations_j: \n",
    "      activations of capsules in layer j (L+1)\n",
    "      (N, OH, OW, 1, o, 1)\n",
    "      (64, 6, 6, 1, 32, 1)\n",
    "    mean_j: \n",
    "      mean of each channel in capsules of layer j (L+1)\n",
    "      (N, OH, OW, 1, o, n_channels)\n",
    "      (24, 6, 6, 1, 32, 16)\n",
    "    stdv_j: \n",
    "      standard deviation of each channel in capsules of layer j (L+1)\n",
    "      (N, OH, OW, 1, o, n_channels)\n",
    "      (24, 6, 6, 1, 32, 16)\n",
    "    var_j: \n",
    "      variance of each channel in capsules of layer j (L+1)\n",
    "      (N, OH, OW, 1, o, n_channels)\n",
    "      (24, 6, 6, 1, 32, 16)\n",
    "    spatial_routing_matrix: ???\n",
    "    \n",
    "  Returns:\n",
    "    rr: \n",
    "      assignment weights between capsules in layer i and layer j\n",
    "      (N, OH, OW, kh*kw*i, o, 1)\n",
    "      (64, 6, 6, 9*8, 16, 1)\n",
    "  \"\"\"\n",
    "  \n",
    "  with tf.variable_scope(\"e_step\") as scope:\n",
    "    \n",
    "    # AG 26/06/2018: changed stdv_j to var_j\n",
    "    o_p_unit0 = - tf.reduce_sum(\n",
    "      tf.square(votes_ij - mean_j, name=\"num\") / (2 * var_j), \n",
    "      axis=-1, \n",
    "      keepdims=True, \n",
    "      name=\"o_p_unit0\")\n",
    "    \n",
    "    o_p_unit2 = - 0.5 * tf.reduce_sum(\n",
    "      tf.math.log(2*np.pi * var_j), \n",
    "      axis=-1, \n",
    "      keepdims=True, \n",
    "      name=\"o_p_unit2\"\n",
    "    )\n",
    "\n",
    "    # (24, 6, 6, 288, 32, 1)\n",
    "    o_p = o_p_unit0 + o_p_unit2\n",
    "    zz = tf.math.log(activations_j + FLAGS.epsilon) + o_p\n",
    "    \n",
    "    # AG 13/11/2018: New implementation of normalising across parents\n",
    "    #----- Start -----#\n",
    "    zz_shape = zz.get_shape().as_list()\n",
    "    batch_size = zz_shape[0]\n",
    "    parent_space = zz_shape[1]\n",
    "    kh_kw_i = zz_shape[3]\n",
    "    parent_caps = zz_shape[4]\n",
    "    kk = int(np.sum(spatial_routing_matrix[:,0]))\n",
    "    child_caps = int(kh_kw_i / kk)\n",
    "    \n",
    "    zz = tf.reshape(zz, [batch_size, parent_space, parent_space, kk, \n",
    "                         child_caps, parent_caps])\n",
    "    \n",
    "    \"\"\"\n",
    "    # In un-log space\n",
    "    with tf.variable_scope(\"to_sparse_unlog\") as scope:\n",
    "      zz_unlog = tf.exp(zz)\n",
    "      #zz_sparse_unlog = utl.to_sparse(zz_unlog, spatial_routing_matrix, \n",
    "      # sparse_filler=1e-15)\n",
    "      zz_sparse_unlog = utl.to_sparse(\n",
    "          zz_unlog, \n",
    "          spatial_routing_matrix, \n",
    "          sparse_filler=0.0)\n",
    "      # maybe this value should be even lower 1e-15\n",
    "      zz_sparse_log = tf.log(zz_sparse_unlog + 1e-15) \n",
    "      zz_sparse = zz_sparse_log\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # In log space\n",
    "    with tf.variable_scope(\"to_sparse_log\") as scope:\n",
    "      # Fill the sparse matrix with the smallest value in zz (at least -100)\n",
    "      sparse_filler = tf.minimum(tf.reduce_min(zz), -100)\n",
    "#       sparse_filler = -100\n",
    "      zz_sparse = utl.to_sparse(\n",
    "          zz, \n",
    "          spatial_routing_matrix, \n",
    "          sparse_filler=sparse_filler)\n",
    "  \n",
    "    \n",
    "    with tf.variable_scope(\"softmax_across_parents\") as scope:\n",
    "      rr_sparse = utl.softmax_across_parents(zz_sparse, spatial_routing_matrix)\n",
    "    \n",
    "    with tf.variable_scope(\"to_dense\") as scope:\n",
    "      rr_dense = utl.to_dense(rr_sparse, spatial_routing_matrix)\n",
    "      \n",
    "    rr = tf.reshape(\n",
    "        rr_dense, \n",
    "        [batch_size, parent_space, parent_space, kh_kw_i, parent_caps, 1])\n",
    "    #----- End -----#\n",
    "\n",
    "    # AG 02/11/2018\n",
    "    # In response to a question on OpenReview, Hinton et al. wrote the \n",
    "    # following:\n",
    "    # \"The gradient flows through EM algorithm. We do not use stop gradient. A \n",
    "    # routing of 3 is like a 3 layer network where the weights of layers are \n",
    "    # shared.\"\n",
    "    # https://openreview.net/forum?id=HJWLfGWRb&noteId=S1eo2P1I3Q\n",
    "    \n",
    "    return rr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_caps(activation_in, \n",
    "              pose_in, \n",
    "              kernel, \n",
    "              stride, \n",
    "              ncaps_out, \n",
    "              name='conv_caps', \n",
    "              weights_regularizer=None):\n",
    "  \"\"\"Convolutional capsule layer.\n",
    "  \n",
    "  \"The routing procedure is used between each adjacent pair of capsule layers. \n",
    "  For convolutional capsules, each capsule in layer L + 1 sends feedback only to \n",
    "  capsules within its receptive field in layer L. Therefore each convolutional \n",
    "  instance of a capsule in layer L receives at most kernel size X kernel size \n",
    "  feedback from each capsule type in layer L + 1. The instances closer to the \n",
    "  border of the image receive fewer feedbacks with corner ones receiving only \n",
    "  one feedback per capsule type in layer L + 1.\"\n",
    "  \n",
    "  See Hinton et al. \"Matrix Capsules with EM Routing\" for detailed description \n",
    "  convolutional capsule layer.\n",
    "  \n",
    "  Author:\n",
    "    Ashley Gritzman 27/11/2018\n",
    "    \n",
    "  Args: \n",
    "    activation_in:\n",
    "      (batch_size, child_space, child_space, child_caps, 1)\n",
    "      (64, 7, 7, 8, 1) \n",
    "    pose_in:\n",
    "      (batch_size, child_space, child_space, child_caps, 16)\n",
    "      (64, 7, 7, 8, 16) \n",
    "    kernel: \n",
    "    stride: \n",
    "    ncaps_out: depth dimension of parent capsules\n",
    "    \n",
    "  Returns:\n",
    "    activation_out: \n",
    "      (batch_size, parent_space, parent_space, parent_caps, 1)\n",
    "      (64, 5, 5, 32, 1)\n",
    "    pose_out:\n",
    "      (batch_size, parent_space, parent_space, parent_caps, 16)\n",
    "      (64, 5, 5, 32, 16)\n",
    "  \"\"\"\n",
    "  \n",
    "  with tf.variable_scope(name) as scope:\n",
    "    \n",
    "    # Get shapes\n",
    "    shape = pose_in.get_shape().as_list()\n",
    "    batch_size = shape[0]\n",
    "    child_space = shape[1]\n",
    "    child_space_2 = int(child_space**2)\n",
    "    child_caps = shape[3]\n",
    "    parent_space = int(np.floor((child_space-kernel)/stride + 1))\n",
    "    parent_space_2 = int(parent_space**2)\n",
    "    parent_caps = ncaps_out\n",
    "    kernel_2 = int(kernel**2)\n",
    "    \n",
    "    with tf.variable_scope('votes') as scope:\n",
    "      # Tile poses and activations\n",
    "      # (64, 7, 7, 8, 16)  -> (64, 5, 5, 9, 8, 16)\n",
    "      pose_tiled, spatial_routing_matrix = utl.kernel_tile(\n",
    "          pose_in, \n",
    "          kernel=kernel, \n",
    "          stride=stride)\n",
    "      activation_tiled, _ = utl.kernel_tile(\n",
    "          activation_in, \n",
    "          kernel=kernel, \n",
    "          stride=stride)\n",
    "\n",
    "      # Check dimensions of spatial_routing_matrix\n",
    "      assert spatial_routing_matrix.shape == (child_space_2, parent_space_2)\n",
    "\n",
    "      # Unroll along batch_size and parent_space_2\n",
    "      # (64, 5, 5, 9, 8, 16) -> (64*5*5, 9*8, 16)\n",
    "      pose_unroll = tf.reshape(\n",
    "          pose_tiled, \n",
    "          shape=[batch_size * parent_space_2, kernel_2 * child_caps, 16])\n",
    "      activation_unroll = tf.reshape(\n",
    "          activation_tiled, \n",
    "          shape=[batch_size * parent_space_2, kernel_2 * child_caps, 1])\n",
    "      \n",
    "      # (64*5*5, 9*8, 16) -> (64*5*5, 9*8, 32, 16)\n",
    "      votes = utl.compute_votes(\n",
    "          pose_unroll, \n",
    "          parent_caps, \n",
    "          weights_regularizer, \n",
    "          tag=True)\n",
    "      logger.info(name + ' votes shape: {}'.format(votes.get_shape()))\n",
    "\n",
    "    with tf.variable_scope('routing') as scope:\n",
    "      # votes (64*5*5, 9*8, 32, 16)\n",
    "      # activations (64*5*5, 9*8, 1)\n",
    "      # pose_out: (N, OH, OW, o, 4x4)\n",
    "      # activation_out: (N, OH, OW, o, 1)\n",
    "      pose_out, activation_out = em.em_routing(votes, \n",
    "                           activation_unroll, \n",
    "                           batch_size, \n",
    "                           spatial_routing_matrix)\n",
    "  \n",
    "    logger.info(name + ' pose_out shape: {}'.format(pose_out.get_shape()))\n",
    "    logger.info(name + ' activation_out shape: {}'\n",
    "                .format(activation_out.get_shape()))\n",
    "\n",
    "    tf.summary.histogram(name + \"activation_out\", activation_out)\n",
    "  \n",
    "  return activation_out, pose_out\n",
    "\n",
    "\n",
    "def fc_caps(activation_in, \n",
    "            pose_in, \n",
    "            ncaps_out, \n",
    "            name='class_caps', \n",
    "            weights_regularizer=None):\n",
    "  \"\"\"Fully connected capsule layer.\n",
    "  \n",
    "  \"The last layer of convolutional capsules is connected to the final capsule \n",
    "  layer which has one capsule per output class.\" We call this layer 'fully \n",
    "  connected' because it fits these characteristics, although Hinton et al. do \n",
    "  not use this teminology in the paper.\n",
    "  \n",
    "  See Hinton et al. \"Matrix Capsules with EM Routing\" for detailed description.\n",
    "  \n",
    "  Author:\n",
    "    Ashley Gritzman 27/11/2018\n",
    "    \n",
    "  Args: \n",
    "    activation_in:\n",
    "      (batch_size, child_space, child_space, child_caps, 1)\n",
    "      (64, 7, 7, 8, 1) \n",
    "    pose_in:\n",
    "      (batch_size, child_space, child_space, child_caps, 16)\n",
    "      (64, 7, 7, 8, 16) \n",
    "    ncaps_out: number of class capsules\n",
    "    name: \n",
    "    weights_regularizer:\n",
    "    \n",
    "  Returns:\n",
    "    activation_out: \n",
    "      score for each output class\n",
    "      (batch_size, ncaps_out)\n",
    "      (64, 5)\n",
    "    pose_out:\n",
    "      pose for each output class capsule\n",
    "      (batch_size, ncaps_out, 16)\n",
    "      (64, 5, 16)\n",
    "  \"\"\"\n",
    "  \n",
    "  with tf.variable_scope(name) as scope:\n",
    "    \n",
    "    # Get shapes\n",
    "    shape = pose_in.get_shape().as_list()\n",
    "    batch_size = shape[0]\n",
    "    child_space = shape[1]\n",
    "    child_caps = shape[3]\n",
    "\n",
    "    with tf.variable_scope('v') as scope:\n",
    "      # In the class_caps layer, we apply same multiplication to every spatial \n",
    "      # location, so we unroll along the batch and spatial dimensions\n",
    "      # (64, 5, 5, 32, 16) -> (64*5*5, 32, 16)\n",
    "      pose = tf.reshape(\n",
    "          pose_in, \n",
    "          shape=[batch_size * child_space * child_space, child_caps, 16])\n",
    "      activation = tf.reshape(\n",
    "          activation_in, \n",
    "          shape=[batch_size * child_space * child_space, child_caps, 1], \n",
    "          name=\"activation\")\n",
    "\n",
    "      # (64*5*5, 32, 16) -> (65*5*5, 32, 5, 16)\n",
    "      votes = utl.compute_votes(pose, ncaps_out, weights_regularizer)\n",
    "\n",
    "      # (65*5*5, 32, 5, 16)\n",
    "      assert (\n",
    "        votes.get_shape() == \n",
    "        [batch_size * child_space * child_space, child_caps, ncaps_out, 16])\n",
    "      logger.info('class_caps votes original shape: {}'\n",
    "                  .format(votes.get_shape()))\n",
    "\n",
    "    with tf.variable_scope('coord_add') as scope:\n",
    "      # (64*5*5, 32, 5, 16)\n",
    "      votes = tf.reshape(\n",
    "          votes, \n",
    "          [batch_size, child_space, child_space, child_caps, ncaps_out, \n",
    "           votes.shape[-1]])\n",
    "      votes = coord_addition(votes)\n",
    "\n",
    "    with tf.variable_scope('routing') as scope:\n",
    "      # Flatten the votes:\n",
    "      # Combine the 4 x 4 spacial dimensions to appear as one spacial dimension       # with many capsules.\n",
    "      # [64*5*5, 16, 5, 16] -> [64, 5*5*16, 5, 16]\n",
    "      votes_flat = tf.reshape(\n",
    "          votes, \n",
    "          shape=[batch_size, child_space * child_space * child_caps, \n",
    "                 ncaps_out, votes.shape[-1]])\n",
    "      activation_flat = tf.reshape(\n",
    "          activation, \n",
    "          shape=[batch_size, child_space * child_space * child_caps, 1])\n",
    "      \n",
    "      spatial_routing_matrix = utl.create_routing_map(child_space=1, k=1, s=1)\n",
    "\n",
    "      logger.info('class_caps votes in to routing shape: {}'\n",
    "            .format(votes_flat.get_shape()))\n",
    "      \n",
    "      pose_out, activation_out = em.em_routing(votes_flat, \n",
    "                           activation_flat, \n",
    "                           batch_size, \n",
    "                           spatial_routing_matrix)\n",
    "\n",
    "    activation_out = tf.squeeze(activation_out, name=\"activation_out\")\n",
    "    pose_out = tf.squeeze(pose_out, name=\"pose_out\")\n",
    "\n",
    "    logger.info('class_caps activation shape: {}'\n",
    "                .format(activation_out.get_shape()))\n",
    "    logger.info('class_caps pose shape: {}'.format(pose_out.get_shape()))\n",
    "\n",
    "    tf.summary.histogram(\"activation_out\", activation_out)\n",
    "      \n",
    "  return activation_out, pose_out\n",
    "\n",
    "  \n",
    "def coord_addition(votes):\n",
    "  \"\"\"Coordinate addition for connecting the last convolutional capsule layer to   the final layer.\n",
    "  \n",
    "  \"When connecting the last convolutional capsule layer to the final layer we do \n",
    "  not want to throw away information about the location of the convolutional \n",
    "  capsules but we also want to make use of the fact that all capsules of the \n",
    "  same type are extracting the same entity at different positions. We therefore   share the transformation matrices between different positions of the same \n",
    "  capsule type and add the scaled coordinate (row, column) of the center of the   receptive field of each capsule to the first two elements of the right-hand \n",
    "  column of its vote matrix. We refer to this technique as Coordinate Addition.   This should encourage the shared final transformations to produce values for \n",
    "  those two elements that represent the fine position of the entity relative to   the center of the capsule’s receptive field.\"\n",
    "  \n",
    "  In Suofei's implementation, they add x and y coordinates as two new dimensions   to the pose matrix i.e. from 16 to 18 dimensions. The paper seems to say that   the values are added to existing dimensions.\n",
    "  \n",
    "  See Hinton et al. \"Matrix Capsules with EM Routing\" for detailed description \n",
    "  coordinate addition.  \n",
    "  \n",
    "  Author:\n",
    "    Ashley Gritzman 27/11/2018\n",
    "    \n",
    "  Credit:\n",
    "    Based on Jonathan Hui's implementation:\n",
    "    https://jhui.github.io/2017/11/14/Matrix-Capsules-with-EM-routing-\n",
    "    Capsule-Network/\n",
    "    \n",
    "  Args: \n",
    "    votes:\n",
    "      (batch_size, child_space, child_space, child_caps, n_output_capsules, 16)\n",
    "      (64, 5, 5, 32, 5, 16) \n",
    "      \n",
    "  Returns:\n",
    "    votes: \n",
    "      same size as input, with coordinate encoding added to first two elements \n",
    "      of right hand column of vote matrix\n",
    "      (batch_size, parent_space, parent_space, parent_caps, 1)\n",
    "      (64, 5, 5, 32, 16)\n",
    "  \"\"\"\n",
    "  \n",
    "  # get spacial dimension of votes\n",
    "  height = votes.get_shape().as_list()[1]\n",
    "  width = votes.get_shape().as_list()[2]\n",
    "  dims = votes.get_shape().as_list()[-1]\n",
    "  \n",
    "  # Generate offset coordinates\n",
    "  # The difference here is that the coordinate won't be exactly in the middle of \n",
    "  # the receptive field, but will be evenly spread out\n",
    "  w_offset_vals = (np.arange(width) + 0.50)/float(width)\n",
    "  h_offset_vals = (np.arange(height) + 0.50)/float(height)\n",
    "  \n",
    "  w_offset = np.zeros([width, dims]) # (5, 16)\n",
    "  w_offset[:,3] = w_offset_vals\n",
    "  # (1, 1, 5, 1, 1, 16)\n",
    "  w_offset = np.reshape(w_offset, [1, 1, width, 1, 1, dims]) \n",
    "  \n",
    "  h_offset = np.zeros([height, dims])\n",
    "  h_offset[:,7] = h_offset_vals\n",
    "  # (1, 5, 1, 1, 1, 16)\n",
    "  h_offset = np.reshape(h_offset, [1, height, 1, 1, 1, dims]) \n",
    "  \n",
    "  # Combine w and h offsets using broadcasting\n",
    "  # w is (1, 1, 5, 1, 1, 16)\n",
    "  # h is (1, 5, 1, 1, 1, 16)\n",
    "  # together (1, 5, 5, 1, 1, 16)\n",
    "  offset = w_offset + h_offset\n",
    "  \n",
    "  # Convent from numpy to tensor\n",
    "  offset = tf.constant(offset, dtype=tf.float32)\n",
    "    \n",
    "  votes = tf.add(votes, offset, name=\"votes_with_coord_add\")\n",
    "  \n",
    "  return votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# LOSS FUNCTIONS\n",
    "#------------------------------------------------------------------------------\n",
    "def spread_loss(scores, y):\n",
    "  \"\"\"Spread loss.\n",
    "  \n",
    "  \"In order to make the training less sensitive to the initialization and \n",
    "  hyper-parameters of the model, we use “spread loss” to directly maximize the \n",
    "  gap between the activation of the target class (a_t) and the activation of the \n",
    "  other classes. If the activation of a wrong class, a_i, is closer than the \n",
    "  margin, m, to at then it is penalized by the squared distance to the margin.\"\n",
    "  \n",
    "  See Hinton et al. \"Matrix Capsules with EM Routing\" equation (3).\n",
    "  \n",
    "  Author:\n",
    "    Ashley Gritzman 19/10/2018  \n",
    "  Credit:\n",
    "    Adapted from Suofei Zhang's implementation on GitHub, \"Matrix-Capsules-\n",
    "    EM-Tensorflow\"\n",
    "    https://github.com/www0wwwjs1/Matrix-Capsules-EM-Tensorflow  \n",
    "  Args: \n",
    "    scores: \n",
    "      scores for each class \n",
    "      (batch_size, num_class)\n",
    "    y: \n",
    "      index of true class \n",
    "      (batch_size, 1)  \n",
    "  Returns:\n",
    "    loss: \n",
    "      mean loss for entire batch\n",
    "      (scalar)\n",
    "  \"\"\"\n",
    "  \n",
    "  with tf.variable_scope('spread_loss') as scope:\n",
    "    batch_size = int(scores.get_shape()[0])\n",
    "\n",
    "    # AG 17/09/2018: modified margin schedule based on response of authors to \n",
    "    # questions on OpenReview.net: \n",
    "    # https://openreview.net/forum?id=HJWLfGWRb\n",
    "    # \"The margin that we set is: \n",
    "    # margin = 0.2 + .79 * tf.sigmoid(tf.minimum(10.0, step / 50000.0 - 4))\n",
    "    # where step is the training step. We trained with batch size of 64.\"\n",
    "    global_step = tf.to_float(tf.train.get_global_step())\n",
    "    m_min = 0.2\n",
    "    m_delta = 0.79\n",
    "    m = (m_min \n",
    "         + m_delta * tf.sigmoid(tf.minimum(10.0, global_step / 50000.0 - 4)))\n",
    "\n",
    "    num_class = int(scores.get_shape()[-1])\n",
    "\n",
    "    y = tf.one_hot(y, num_class, dtype=tf.float32)\n",
    "    \n",
    "    # Get the score of the target class\n",
    "    # (64, 1, 5)\n",
    "    scores = tf.reshape(scores, shape=[batch_size, 1, num_class])\n",
    "    # (64, 5, 1)\n",
    "    y = tf.expand_dims(y, axis=2)\n",
    "    # (64, 1, 5)*(64, 5, 1) = (64, 1, 1)\n",
    "    at = tf.matmul(scores, y)\n",
    "    \n",
    "    # Compute spread loss, paper eq (3)\n",
    "    loss = tf.square(tf.maximum(0., m - (at - scores)))\n",
    "    \n",
    "    # Sum losses for all classes\n",
    "    # (64, 1, 5)*(64, 5, 1) = (64, 1, 1)\n",
    "    # e.g loss*[1 0 1 1 1]\n",
    "    loss = tf.matmul(loss, 1. - y)\n",
    "    \n",
    "    # Compute mean\n",
    "    loss = tf.reduce_mean(loss)\n",
    "\n",
    "  return loss\n",
    "\n",
    "\n",
    "def cross_ent_loss(logits, y):\n",
    "  \"\"\"Cross entropy loss.\n",
    "  \n",
    "  Author:\n",
    "    Ashley Gritzman 06/05/2019  \n",
    "  Args: \n",
    "    logits: \n",
    "      logits for each class \n",
    "      (batch_size, num_class)\n",
    "    y: \n",
    "      index of true class \n",
    "      (batch_size, 1)  \n",
    "  Returns:\n",
    "    loss: \n",
    "      mean loss for entire batch\n",
    "      (scalar)\n",
    "  \"\"\"\n",
    "  loss = tf.losses.sparse_softmax_cross_entropy(labels=y, logits=logits)\n",
    "  loss = tf.reduce_mean(loss)\n",
    "\n",
    "  return loss\n",
    "\n",
    "\n",
    "\n",
    "def total_loss(scores, y):\n",
    "  \"\"\"total_loss = spread_loss + regularization_loss.\n",
    "  \n",
    "  If the flag to regularize is set, the the total loss is the sum of the spread   loss and the regularization loss.\n",
    "  \n",
    "  Author:\n",
    "    Ashley Gritzman 19/10/2018  \n",
    "  Credit:\n",
    "    Adapted from Suofei Zhang's implementation on GitHub, \"Matrix-Capsules-\n",
    "    EM-Tensorflow\"\n",
    "    https://github.com/www0wwwjs1/Matrix-Capsules-EM-Tensorflow  \n",
    "  Args: \n",
    "    scores: \n",
    "      scores for each class \n",
    "      (batch_size, num_class)\n",
    "    y: \n",
    "      index of true class \n",
    "      (batch_size, 1)  \n",
    "  Returns:\n",
    "    total_loss: \n",
    "      mean total loss for entire batch\n",
    "      (scalar)\n",
    "  \"\"\"\n",
    "  \n",
    "  with tf.variable_scope('total_loss') as scope:\n",
    "    # spread loss\n",
    "    sprd_loss = spread_loss(scores, y)\n",
    "\n",
    "    if FLAGS.weight_reg:\n",
    "      # Regularization\n",
    "      regularization = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "      reg_loss = tf.add_n(regularization)\n",
    "      total_loss = sprd_loss + reg_loss\n",
    "      tf.summary.scalar('spread_loss', sprd_loss)\n",
    "      tf.summary.scalar('regularization_loss', reg_loss)\n",
    "    else:\n",
    "      # No regularization\n",
    "      total_loss = sprd_loss\n",
    "      tf.summary.scalar('spread_loss', sprd_loss)\n",
    "\n",
    "  return total_loss \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_arch_smallnorb(input, is_train: bool, num_classes: int):\n",
    "  \n",
    "  logger.info('input shape: {}'.format(input.get_shape()))\n",
    "  batch_size = int(input.get_shape()[0])\n",
    "  spatial_size = int(input.get_shape()[1])\n",
    "\n",
    "  # xavier initialization is necessary here to provide higher stability\n",
    "  # initializer = tf.truncated_normal_initializer(mean=0.0, stddev=0.01)\n",
    "  # instead of initializing bias with constant 0, a truncated normal \n",
    "  # initializer is exploited here for higher stability\n",
    "  bias_initializer = tf.truncated_normal_initializer(mean=0.0, stddev=0.01) \n",
    "\n",
    "  # AG 13/11/2018\n",
    "  # In response to a question on OpenReview, Hinton et al. wrote the \n",
    "  # following:\n",
    "  # \"We use a weight decay loss with a small factor of .0000002 rather than \n",
    "  # the reconstruction loss.\"\n",
    "  # https://openreview.net/forum?id=HJWLfGWRb&noteId=rJeQnSsE3X\n",
    "  weights_regularizer = tf.contrib.layers.l2_regularizer(0.0000002)\n",
    "\n",
    "  # weights_initializer=initializer,\n",
    "  with slim.arg_scope([slim.conv2d], \n",
    "    trainable = is_train, \n",
    "    biases_initializer = bias_initializer,\n",
    "    weights_regularizer = weights_regularizer):\n",
    "    \n",
    "    #----- Batch Norm -----#\n",
    "    output = slim.batch_norm(\n",
    "        input, \n",
    "        center=False, \n",
    "        is_training=is_train, \n",
    "        trainable=is_train)\n",
    "    \n",
    "    #----- Convolutional Layer 1 -----#\n",
    "    with tf.variable_scope('relu_conv1') as scope:\n",
    "      output = slim.conv2d(output, \n",
    "      num_outputs=FLAGS.A, \n",
    "      kernel_size=[5, 5], \n",
    "      stride=2, \n",
    "      padding='SAME', \n",
    "      scope=scope, \n",
    "      activation_fn=tf.nn.relu)\n",
    "      \n",
    "      spatial_size = int(output.get_shape()[1])\n",
    "      assert output.get_shape() == [batch_size, spatial_size, spatial_size, \n",
    "                                    FLAGS.A]\n",
    "      logger.info('relu_conv1 output shape: {}'.format(output.get_shape()))\n",
    "    \n",
    "    #----- Primary Capsules -----#\n",
    "    with tf.variable_scope('primary_caps') as scope:\n",
    "      pose = slim.conv2d(output, \n",
    "      num_outputs=FLAGS.B * 16, \n",
    "      kernel_size=[1, 1], \n",
    "      stride=1, \n",
    "      padding='VALID', \n",
    "      scope='pose', \n",
    "      activation_fn=None)\n",
    "      activation = slim.conv2d(\n",
    "          output, \n",
    "          num_outputs=FLAGS.B, \n",
    "          kernel_size=[1, 1], \n",
    "          stride=1, \n",
    "          padding='VALID', \n",
    "          scope='activation', \n",
    "          activation_fn=tf.nn.sigmoid)\n",
    "\n",
    "      spatial_size = int(pose.get_shape()[1])\n",
    "      pose = tf.reshape(pose, shape=[batch_size, spatial_size, spatial_size, \n",
    "                                     FLAGS.B, 16], name='pose')\n",
    "      activation = tf.reshape(\n",
    "          activation, \n",
    "          shape=[batch_size, spatial_size, spatial_size, FLAGS.B, 1], \n",
    "          name=\"activation\")\n",
    "      \n",
    "      assert pose.get_shape() == [batch_size, spatial_size, spatial_size, \n",
    "                                  FLAGS.B, 16]\n",
    "      assert activation.get_shape() == [batch_size, spatial_size, spatial_size,\n",
    "                                        FLAGS.B, 1]\n",
    "      logger.info('primary_caps pose shape: {}'.format(pose.get_shape()))\n",
    "      logger.info('primary_caps activation shape {}'\n",
    "                  .format(activation.get_shape()))\n",
    "      \n",
    "      tf.summary.histogram(\"activation\", activation)\n",
    "    \n",
    "    #----- Conv Caps 1 -----#\n",
    "    # activation_in: (64, 7, 7, 8, 1) \n",
    "    # pose_in: (64, 7, 7, 16, 16) \n",
    "    # activation_out: (64, 5, 5, 32, 1)\n",
    "    # pose_out: (64, 5, 5, 32, 16)\n",
    "    activation, pose = lyr.conv_caps(\n",
    "        activation_in = activation, \n",
    "        pose_in = pose, \n",
    "        kernel = 3, \n",
    "        stride = 2, \n",
    "        ncaps_out = FLAGS.C, \n",
    "        name = 'lyr.conv_caps1', \n",
    "        weights_regularizer = weights_regularizer)\n",
    "    \n",
    "    #----- Conv Caps 2 -----#\n",
    "    # activation_in: (64, 7, 7, 8, 1) \n",
    "    # pose_in: (64, 7, 7, 16, 1) \n",
    "    # activation_out: (64, 5, 5, 32, 1)\n",
    "    # pose_out: (64, 5, 5, 32, 16)\n",
    "    activation, pose = lyr.conv_caps(\n",
    "        activation_in = activation, \n",
    "        pose_in = pose, \n",
    "        kernel = 3, \n",
    "        stride = 1, \n",
    "        ncaps_out = FLAGS.D, \n",
    "        name = 'lyr.conv_caps2', \n",
    "        weights_regularizer = weights_regularizer)\n",
    "    \n",
    "    #----- Class Caps -----#\n",
    "    # activation_in: (64, 5, 5, 32, 1)\n",
    "    # pose_in: (64, 5, 5, 32, 16)\n",
    "    # activation_out: (64, 5)\n",
    "    # pose_out: (64, 5, 16) \n",
    "    activation_out, pose_out = lyr.fc_caps(\n",
    "        activation_in = activation,\n",
    "        pose_in = pose,\n",
    "        ncaps_out = num_classes,\n",
    "        name = 'class_caps',\n",
    "        weights_regularizer = weights_regularizer)\n",
    "    \n",
    "  return {'scores': activation_out, 'pose_out': pose_out}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parser(serialized_example):\n",
    "  \"\"\"Parse smallNORB example from tfrecord.\n",
    "  \n",
    "  Author:\n",
    "    Ashley Gritzman 15/11/2018\n",
    "  Args: \n",
    "    serialized_example: serialized example from tfrecord  \n",
    "  Returns:\n",
    "    img: image\n",
    "    lab: label\n",
    "    cat: \n",
    "      category\n",
    "      the instance in the category (0 to 9)\n",
    "    elv: \n",
    "      elevation\n",
    "      the elevation (0 to 8, which mean cameras are 30, \n",
    "      35,40,45,50,55,60,65,70 degrees from the horizontal respectively)\n",
    "    azi: \n",
    "      azimuth\n",
    "      the azimuth (0,2,4,...,34, multiply by 10 to get the azimuth in \n",
    "      degrees)\n",
    "    lit: \n",
    "      lighting\n",
    "      the lighting condition (0 to 5)\n",
    "  \"\"\"\n",
    "\n",
    "  features = tf.parse_single_example(\n",
    "    serialized_example, \n",
    "    features={\n",
    "      'img_raw': tf.FixedLenFeature([], tf.string),\n",
    "      'label': tf.FixedLenFeature([], tf.int64),\n",
    "      'category': tf.FixedLenFeature([], tf.int64), \n",
    "      'elevation': tf.FixedLenFeature([], tf.int64), \n",
    "      'azimuth': tf.FixedLenFeature([], tf.int64), \n",
    "      'lighting': tf.FixedLenFeature([], tf.int64),\n",
    "     })\n",
    "\n",
    "  img = tf.decode_raw(features['img_raw'], tf.float64)\n",
    "  img = tf.reshape(img, [96, 96, 1])\n",
    "  img = tf.cast(img, tf.float32)  # * (1. / 255) # left unnormalized\n",
    "\n",
    "  lab = tf.cast(features['label'], tf.int32)\n",
    "  cat = tf.cast(features['category'], tf.int32)\n",
    "  elv = tf.cast(features['elevation'], tf.int32)\n",
    "  azi = tf.cast(features['azimuth'], tf.int32)\n",
    "  lit = tf.cast(features['lighting'], tf.int32)\n",
    "\n",
    "  return img, lab, cat, elv, azi, lit\n",
    "\n",
    "\n",
    "def _train_preprocess(img, lab, cat, elv, azi, lit):\n",
    "  \"\"\"Preprocessing for training.\n",
    "  \n",
    "  Preprocessing from Hinton et al. (2018) \"Matrix capsules with EM routing.\"\n",
    "  Hinton2018: \"We downsample smallNORB to 48 × 48 pixels and normalize each \n",
    "  image to have zero mean and unit variance. During training, we randomly crop \n",
    "  32 × 32 patches and add random brightness and contrast to the cropped images.\n",
    "  During test, we crop a 32 × 32 patch from the center of the image and \n",
    "  achieve...\"\n",
    "  \n",
    "  Author:\n",
    "    Ashley Gritzman 15/11/2018\n",
    "  Args: \n",
    "    img: this fn only works on the image\n",
    "    lab, cat, elv, azi, lit: allow these to pass through  \n",
    "  Returns:\n",
    "    img: image processed\n",
    "    lab, cat, elv, azi, lit: allow these to pass through   \n",
    "  \"\"\"\n",
    "  \n",
    "  img = img / 255.\n",
    "  img = tf.image.resize_images(img, [48, 48])\n",
    "  img = tf.image.per_image_standardization(img)\n",
    "  img = tf.random_crop(img, [32, 32, 1])\n",
    "  img = tf.image.random_brightness(img, max_delta = 2.0)\n",
    "  #original 0.5, 1.5\n",
    "  img = tf.image.random_contrast(img, lower=0.5, upper=1.5) \n",
    "  \n",
    "  # Original\n",
    "  # image = tf.image.random_brightness(image, max_delta=32. / 255.)\n",
    "  # image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n",
    "  # image = tf.image.resize_images(image, [48, 48])\n",
    "  # image = tf.random_crop(image, [32, 32, 1])\n",
    "\n",
    "  return img, lab, cat, elv, azi, lit\n",
    "\n",
    "\n",
    "def _val_preprocess(img, lab, cat, elv, azi, lit):\n",
    "  \"\"\"Preprocessing for validation/testing.\n",
    "  \n",
    "  Preprocessing from Hinton et al. (2018) \"Matrix capsules with EM routing.\" \n",
    "  Hinton2018: \"We downsample smallNORB to 48 × 48 pixels and normalize each \n",
    "  image to have zero mean and unit variance. During training, we randomly crop \n",
    "  32 × 32 patches and add random brightness and contrast to the cropped \n",
    "  images. During test, we crop a 32 × 32 patch from the center of the image \n",
    "  and achieve...\"\n",
    "  \n",
    "  Author:\n",
    "    Ashley Gritzman 15/11/2018\n",
    "  Args: \n",
    "    img: this fn only works on the image\n",
    "    lab, cat, elv, azi, lit: allow these to pass through  \n",
    "  Returns:\n",
    "    img: image processed\n",
    "    lab, cat, elv, azi, lit: allow these to pass through   \n",
    "  \"\"\"\n",
    "  \n",
    "  img = img / 255.\n",
    "  img = tf.image.resize_images(img, [48, 48])\n",
    "  img = tf.image.per_image_standardization(img)\n",
    "  img = tf.slice(img, [8, 8, 0], [32, 32, 1])\n",
    "  \n",
    "  # Original\n",
    "  # image = tf.image.resize_images(image, [48, 48])\n",
    "  # image = tf.slice(image, [8, 8, 0], [32, 32, 1])\n",
    "\n",
    "  return img, lab, cat, elv, azi, lit\n",
    "  \n",
    "\n",
    "def input_fn(path, is_train: bool):\n",
    "  \"\"\"Input pipeline for smallNORB using tf.data.\n",
    "  \n",
    "  Author:\n",
    "    Ashley Gritzman 15/11/2018\n",
    "  Args: \n",
    "    is_train:  \n",
    "  Returns:\n",
    "    dataset: image tf.data.Dataset \n",
    "  \"\"\"\n",
    "\n",
    "  import re\n",
    "  if is_train:\n",
    "    CHUNK_RE = re.compile(r\"train.*\\.tfrecords\")\n",
    "  else:\n",
    "    CHUNK_RE = re.compile(r\"test.*\\.tfrecords\")\n",
    "\n",
    "  chunk_files = [os.path.join(path, fname)\n",
    "           for fname in os.listdir(path)\n",
    "           if CHUNK_RE.match(fname)]\n",
    "  \n",
    "  print(\"path:\", path)\n",
    "  # 1. create the dataset\n",
    "  dataset = tf.data.TFRecordDataset(chunk_files)\n",
    "  \n",
    "  # 2. map with the actual work (preprocessing, augmentation…) using multiple \n",
    "  # parallel calls\n",
    "  dataset = dataset.map(_parser, num_parallel_calls=4)\n",
    "  if is_train:\n",
    "    dataset = dataset.map(_train_preprocess, \n",
    "                          num_parallel_calls=FLAGS.num_threads)\n",
    "  else:\n",
    "    dataset = dataset.map(_val_preprocess, \n",
    "                          num_parallel_calls=FLAGS.num_threads)\n",
    "  \n",
    "  # 3. shuffle (with a big enough buffer size)\n",
    "  # In response to a question on OpenReview, Hinton et al. wrote the \n",
    "  # following:\n",
    "  # https://openreview.net/forum?id=HJWLfGWRb&noteId=rJgxonoNnm\n",
    "  # \"We did not have any special ordering of training batches and we random \n",
    "  # shuffle. In terms of TF batch:\n",
    "  # capacity=2000 + 3 * batch_size, ensures a minimum amount of shuffling of \n",
    "  # examples. min_after_dequeue=2000.\"\n",
    "  capacity = 2000 + 3 * FLAGS.batch_size\n",
    "  dataset = dataset.shuffle(buffer_size = capacity)\n",
    "    \n",
    "  # 4. batch\n",
    "  dataset = dataset.batch(FLAGS.batch_size, drop_remainder=True)\n",
    "  \n",
    "  # 5. repeat\n",
    "  dataset = dataset.repeat(count=FLAGS.epoch)\n",
    "  \n",
    "  # 6. prefetch\n",
    "  dataset = dataset.prefetch(1)\n",
    "  \n",
    "  return dataset\n",
    "\n",
    "\n",
    "def create_inputs_norb(path, is_train: bool):\n",
    "  \"\"\"Get a batch from the input pipeline.\n",
    "  \n",
    "  Author:\n",
    "    Ashley Gritzman 15/11/2018\n",
    "  Args: \n",
    "    is_train:  \n",
    "  Returns:\n",
    "    img, lab, cat, elv, azi, lit: \n",
    "  \"\"\"\n",
    "  \n",
    "  # Create batched dataset\n",
    "  dataset = input_fn(path, is_train)\n",
    "  \n",
    "  # Create one-shot iterator\n",
    "  iterator = dataset.make_one_shot_iterator()\n",
    "  \n",
    "  img, lab, cat, elv, azi, lit = iterator.get_next()\n",
    "  \n",
    "  output_dict = {'image': img,\n",
    "           'label': lab,\n",
    "           'category': cat,\n",
    "           'elevation': elv,\n",
    "           'azimuth': azi,\n",
    "           'lighting': lit}\n",
    "  \n",
    "  return output_dict\n",
    "\n",
    "\n",
    "def plot_smallnorb(is_train=True, samples_per_class=5):\n",
    "  \"\"\"Plot examples from the smallNORB dataset.\n",
    "  \n",
    "  Execute this command in a Jupyter Notebook.\n",
    "  \n",
    "  Author:\n",
    "    Ashley Gritzman 18/04/2019\n",
    "  Args: \n",
    "    is_train: True for the training dataset, False for the test dataset\n",
    "    samples_per_class: number of samples images per class\n",
    "  Returns:\n",
    "    None\n",
    "  \"\"\"\n",
    "  \n",
    "  # To plot pretty figures\n",
    "  import matplotlib.pyplot as plt\n",
    "  plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "  plt.rcParams['image.interpolation'] = 'nearest'\n",
    "  plt.rcParams['image.cmap'] = 'gray'\n",
    "  \n",
    "  from config import get_dataset_path\n",
    "  path = get_dataset_path(\"smallNORB\")\n",
    "  \n",
    "  CLASSES = ['animal', 'human', 'airplane', 'truck', 'car']\n",
    "\n",
    "  # Get batch from data queue. Batch size is FLAGS.batch_size, which is then \n",
    "  # divided across multiple GPUs\n",
    "  input_dict = create_inputs_norb(path, is_train=is_train)\n",
    "  with tf.Session() as sess:\n",
    "    input_dict = sess.run(input_dict)\n",
    "    \n",
    "  img_bch = input_dict['image']\n",
    "  lab_bch = input_dict['label']\n",
    "  cat_bch = input_dict['category']\n",
    "  elv_bch = input_dict['elevation']\n",
    "  azi_bch = input_dict['azimuth']\n",
    "  lit_bch = input_dict['lighting']\n",
    "  \n",
    "  num_classes = len(CLASSES)\n",
    "\n",
    "  fig = plt.figure(figsize=(num_classes * 2, samples_per_class * 2))\n",
    "  fig.suptitle(\"category, elevation, azimuth, lighting\")  \n",
    "  for y, cls in enumerate(CLASSES):\n",
    "    idxs = np.flatnonzero(lab_bch == y)\n",
    "    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
    "    for i, idx in enumerate(idxs):\n",
    "      plt_idx = i * num_classes + y + 1\n",
    "      plt.subplot(samples_per_class, num_classes, plt_idx)\n",
    "      #plt.imshow(img_bch[idx].astype('uint8').squeeze())\n",
    "      plt.imshow(np.squeeze(img_bch[idx]))\n",
    "      plt.xticks([], [])\n",
    "      plt.yticks([], [])\n",
    "      plt.xlabel(\"{}, {}, {},{}\".format(cat_bch[idx], elv_bch[idx], \n",
    "                        azi_bch[idx], lit_bch[idx]))\n",
    "      # plt.axis('off')\n",
    "\n",
    "      if i == 0:\n",
    "        plt.title(cls)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "  \"\"\"Run training and validation.\n",
    "  \n",
    "  1. Build graphs\n",
    "      1.1 Training graph to run on multiple GPUs\n",
    "      1.2 Validation graph to run on multiple GPUs\n",
    "  2. Configure sessions\n",
    "      2.1 Train\n",
    "      2.2 Validate\n",
    "  3. Main loop\n",
    "      3.1 Train\n",
    "      3.2 Write summary\n",
    "      3.3 Save model\n",
    "      3.4 Validate model\n",
    "      \n",
    "  Author:\n",
    "    Ashley Gritzman\n",
    "  \"\"\"\n",
    "  \n",
    "  # Set reproduciable random seed\n",
    "  #tf.set_random_seed(1234)\n",
    "    \n",
    "  # Directories\n",
    "  train_dir, train_summary_dir = setup_train_directories()\n",
    "  \n",
    "  # Logger\n",
    "  setup_logger(logger_dir=train_dir, name=\"logger_train.txt\")\n",
    "  \n",
    "  # Hyperparameters\n",
    "  load_or_save_hyperparams(train_dir)\n",
    "  \n",
    "  # Get dataset hyperparameters\n",
    "  logger.info('Using dataset: {}'.format(FLAGS.dataset))\n",
    "  dataset_size_train  = get_dataset_size_train(FLAGS.dataset)\n",
    "  dataset_size_val  = get_dataset_size_validate(FLAGS.dataset)\n",
    "  build_arch      = get_dataset_architecture(FLAGS.dataset)\n",
    "  num_classes     = get_num_classes(FLAGS.dataset)\n",
    "  create_inputs_train = get_create_inputs(FLAGS.dataset, mode=\"train\")\n",
    "  create_inputs_val   = get_create_inputs(FLAGS.dataset, mode=\"validate\")\n",
    "\n",
    "  \n",
    " #*****************************************************************************\n",
    " # 1. BUILD GRAPHS\n",
    " #*****************************************************************************\n",
    "\n",
    "  #----------------------------------------------------------------------------\n",
    "  # GRAPH - TRAIN\n",
    "  #----------------------------------------------------------------------------\n",
    "  logger.info('BUILD TRAIN GRAPH')\n",
    "  g_train = tf.Graph()\n",
    "  with g_train.as_default(), tf.device('/cpu:0'):\n",
    "    \n",
    "    # Get global_step\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "    # Get batches per epoch\n",
    "    num_batches_per_epoch = int(dataset_size_train / FLAGS.batch_size)\n",
    "\n",
    "    # In response to a question on OpenReview, Hinton et al. wrote the \n",
    "    # following:\n",
    "    # \"We use an exponential decay with learning rate: 3e-3, decay_steps: 20000,     # decay rate: 0.96.\"\n",
    "    # https://openreview.net/forum?id=HJWLfGWRb&noteId=ryxTPFDe2X\n",
    "    lrn_rate = tf.train.exponential_decay(learning_rate = FLAGS.lrn_rate, \n",
    "                        global_step = global_step, \n",
    "                        decay_steps = 20000, \n",
    "                        decay_rate = 0.96)\n",
    "    tf.summary.scalar('learning_rate', lrn_rate)\n",
    "    opt = tf.train.AdamOptimizer(learning_rate=lrn_rate)\n",
    "\n",
    "    # Get batch from data queue. Batch size is FLAGS.batch_size, which is then \n",
    "    # divided across multiple GPUs\n",
    "    input_dict = create_inputs_train()\n",
    "    batch_x = input_dict['image']\n",
    "    batch_labels = input_dict['label']\n",
    "    \n",
    "    # AG 03/10/2018: Split batch for multi gpu implementation\n",
    "    # Each split is of size FLAGS.batch_size / FLAGS.num_gpus\n",
    "    # See: https://github.com/naturomics/CapsNet-Tensorflow/blob/master/\n",
    "    # dist_version/distributed_train.py\n",
    "    splits_x = tf.split(\n",
    "        axis=0, \n",
    "        num_or_size_splits=FLAGS.num_gpus, \n",
    "        value=batch_x)\n",
    "    splits_labels = tf.split(\n",
    "        axis=0, \n",
    "        num_or_size_splits=FLAGS.num_gpus, \n",
    "        value=batch_labels)\n",
    "\n",
    "    \n",
    "    #--------------------------------------------------------------------------\n",
    "    # MULTI GPU - TRAIN\n",
    "    #--------------------------------------------------------------------------\n",
    "    # Calculate the gradients for each model tower\n",
    "    tower_grads = []\n",
    "    tower_losses = []\n",
    "    tower_logits = []\n",
    "    reuse_variables = None\n",
    "    for i in range(FLAGS.num_gpus):\n",
    "      with tf.device('/gpu:%d' % i):\n",
    "        with tf.name_scope('tower_%d' % i) as scope:\n",
    "          logger.info('TOWER %d' % i)\n",
    "          #with slim.arg_scope([slim.model_variable, slim.variable],\n",
    "          # device='/cpu:0'):\n",
    "          with slim.arg_scope([slim.variable], device='/cpu:0'):\n",
    "            loss, logits = tower_fn(\n",
    "                build_arch, \n",
    "                splits_x[i], \n",
    "                splits_labels[i], \n",
    "                scope, \n",
    "                num_classes, \n",
    "                reuse_variables=reuse_variables,\n",
    "                is_train=True)\n",
    "          \n",
    "          # Don't reuse variable for first GPU, but do reuse for others\n",
    "          reuse_variables = True\n",
    "          \n",
    "          # Compute gradients for one GPU\n",
    "          grads = opt.compute_gradients(loss)\n",
    "          \n",
    "          # Keep track of the gradients across all towers.\n",
    "          tower_grads.append(grads)\n",
    "          \n",
    "          # Keep track of losses and logits across for each tower\n",
    "          tower_logits.append(logits)\n",
    "          tower_losses.append(loss)\n",
    "          \n",
    "          # Loss for each tower\n",
    "          tf.summary.scalar(\"loss\", loss)\n",
    "    \n",
    "    # We must calculate the mean of each gradient. Note that this is the\n",
    "    # synchronization point across all towers.\n",
    "    grad = average_gradients(tower_grads)\n",
    "    \n",
    "    # See: https://stackoverflow.com/questions/40701712/how-to-check-nan-in-\n",
    "    # gradients-in-tensorflow-when-updating\n",
    "    grad_check = ([tf.check_numerics(g, message='Gradient NaN Found!') \n",
    "                      for g, _ in grad if g is not None] \n",
    "                  + [tf.check_numerics(loss, message='Loss NaN Found')])\n",
    "    \n",
    "    # Apply the gradients to adjust the shared variables\n",
    "    with tf.control_dependencies(grad_check):\n",
    "      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "      with tf.control_dependencies(update_ops):\n",
    "        train_op = opt.apply_gradients(grad, global_step=global_step)\n",
    "    \n",
    "    # Calculate mean loss     \n",
    "    loss = tf.reduce_mean(tower_losses)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    logits = tf.concat(tower_logits, axis=0)\n",
    "    acc = met.accuracy(logits, batch_labels)\n",
    "    \n",
    "    # Prepare predictions and one-hot labels\n",
    "    probs = tf.nn.softmax(logits=logits)\n",
    "    labels_oh = tf.one_hot(batch_labels, num_classes)\n",
    "    \n",
    "    # Group metrics together\n",
    "    # See: https://cs230-stanford.github.io/tensorflow-model.html\n",
    "    trn_metrics = {'loss' : loss,\n",
    "             'labels' : batch_labels, \n",
    "             'labels_oh' : labels_oh,\n",
    "             'logits' : logits,\n",
    "             'probs' : probs,\n",
    "             'acc' : acc,\n",
    "             }\n",
    "    \n",
    "    # Reset and read operations for streaming metrics go here\n",
    "    trn_reset = {}\n",
    "    trn_read = {}\n",
    "    \n",
    "    # Logging\n",
    "    tf.summary.scalar('trn_loss', loss)\n",
    "    tf.summary.scalar('trn_acc', acc)\n",
    "\n",
    "    # Set Saver\n",
    "    # AG 26/09/2018: Save all variables including Adam so that we can continue \n",
    "    # training from where we left off\n",
    "    # max_to_keep=None should keep all checkpoints\n",
    "    saver = tf.train.Saver(tf.global_variables(), max_to_keep=None)\n",
    "    \n",
    "    # Display number of parameters\n",
    "    train_params = np.sum([np.prod(v.get_shape().as_list())\n",
    "              for v in tf.trainable_variables()]).astype(np.int32)\n",
    "    logger.info('Trainable Parameters: {}'.format(train_params))\n",
    "        \n",
    "    # Set summary op\n",
    "    trn_summary = tf.summary.merge_all()\n",
    "    \n",
    "  \n",
    "  #----------------------------------------------------------------------------\n",
    "  # GRAPH - VALIDATION\n",
    "  #----------------------------------------------------------------------------\n",
    "  logger.info('BUILD VALIDATION GRAPH')\n",
    "  g_val = tf.Graph()\n",
    "  with g_val.as_default():\n",
    "    # Get global_step\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "    num_batches_val = int(dataset_size_val / FLAGS.batch_size * FLAGS.val_prop)\n",
    "    \n",
    "    # Get data\n",
    "    input_dict = create_inputs_val()\n",
    "    batch_x = input_dict['image']\n",
    "    batch_labels = input_dict['label']\n",
    "    \n",
    "    # AG 10/12/2018: Split batch for multi gpu implementation\n",
    "    # Each split is of size FLAGS.batch_size / FLAGS.num_gpus\n",
    "    # See: https://github.com/naturomics/CapsNet-\n",
    "    # Tensorflow/blob/master/dist_version/distributed_train.py\n",
    "    splits_x = tf.split(\n",
    "        axis=0, \n",
    "        num_or_size_splits=FLAGS.num_gpus, \n",
    "        value=batch_x)\n",
    "    splits_labels = tf.split(\n",
    "        axis=0, \n",
    "        num_or_size_splits=FLAGS.num_gpus, \n",
    "        value=batch_labels)\n",
    "    \n",
    "    \n",
    "    #--------------------------------------------------------------------------\n",
    "    # MULTI GPU - VALIDATE\n",
    "    #--------------------------------------------------------------------------\n",
    "    # Calculate the logits for each model tower\n",
    "    tower_logits = []\n",
    "    reuse_variables = None\n",
    "    for i in range(FLAGS.num_gpus):\n",
    "      with tf.device('/gpu:%d' % i):\n",
    "        with tf.name_scope('tower_%d' % i) as scope:\n",
    "          with slim.arg_scope([slim.variable], device='/cpu:0'):\n",
    "            loss, logits = tower_fn(\n",
    "                build_arch, \n",
    "                splits_x[i], \n",
    "                splits_labels[i], \n",
    "                scope, \n",
    "                num_classes, \n",
    "                reuse_variables=reuse_variables, \n",
    "                is_train=False)\n",
    "\n",
    "          # Don't reuse variable for first GPU, but do reuse for others\n",
    "          reuse_variables = True\n",
    "          \n",
    "          # Keep track of losses and logits across for each tower\n",
    "          tower_logits.append(logits)\n",
    "          \n",
    "          # Loss for each tower\n",
    "          tf.summary.histogram(\"val_logits\", logits)\n",
    "    \n",
    "    # Combine logits from all towers\n",
    "    logits = tf.concat(tower_logits, axis=0)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    val_loss = mod.spread_loss(logits, batch_labels)\n",
    "    val_acc = met.accuracy(logits, batch_labels)\n",
    "    \n",
    "    # Prepare predictions and one-hot labels\n",
    "    val_probs = tf.nn.softmax(logits=logits)\n",
    "    val_labels_oh = tf.one_hot(batch_labels, num_classes)\n",
    "    \n",
    "    # Group metrics together\n",
    "    # See: https://cs230-stanford.github.io/tensorflow-model.html\n",
    "    val_metrics = {'loss' : val_loss,\n",
    "                   'labels' : batch_labels, \n",
    "                   'labels_oh' : val_labels_oh,\n",
    "                   'logits' : logits,\n",
    "                   'probs' : val_probs,\n",
    "                   'acc' : val_acc,\n",
    "                   }\n",
    "    \n",
    "    # Reset and read operations for streaming metrics go here\n",
    "    val_reset = {}\n",
    "    val_read = {}\n",
    "    \n",
    "    tf.summary.scalar(\"val_loss\", val_loss)\n",
    "    tf.summary.scalar(\"val_acc\", val_acc)\n",
    "      \n",
    "    # Saver\n",
    "    saver = tf.train.Saver(max_to_keep=None)\n",
    "    \n",
    "    # Set summary op\n",
    "    val_summary = tf.summary.merge_all()\n",
    "     \n",
    "      \n",
    "  #****************************************************************************\n",
    "  # 2. SESSIONS\n",
    "  #****************************************************************************\n",
    "          \n",
    "  #----- SESSION TRAIN -----#\n",
    "  # Session settings\n",
    "  sess_train = tf.Session(config=tf.ConfigProto(allow_soft_placement=True, \n",
    "                                                log_device_placement=False), \n",
    "                          graph=g_train)\n",
    "\n",
    "  # Debugger\n",
    "  # AG 05/06/2018: Debugging using either command line or TensorBoard\n",
    "  if FLAGS.debugger is not None:\n",
    "    # sess = tf_debug.LocalCLIDebugWrapperSession(sess)\n",
    "    sess_train = tf_debug.TensorBoardDebugWrapperSession(sess_train, \n",
    "                                                         FLAGS.debugger)\n",
    "    \n",
    "  with g_train.as_default():\n",
    "    sess_train.run([tf.global_variables_initializer(),\n",
    "                    tf.local_variables_initializer()])\n",
    "    \n",
    "    # Restore previous checkpoint\n",
    "    # AG 26/09/2018: where should this go???\n",
    "    if FLAGS.load_dir is not None:\n",
    "      load_dir_checkpoint = os.path.join(FLAGS.load_dir, \"train\", \"checkpoint\")\n",
    "      prev_step = load_training(saver, sess_train, load_dir_checkpoint)\n",
    "    else:\n",
    "      prev_step = 0\n",
    "\n",
    "  # Create summary writer, and write the train graph\n",
    "  summary_writer = tf.summary.FileWriter(train_summary_dir, \n",
    "                                         graph=sess_train.graph)\n",
    "\n",
    "  \n",
    "  #----- SESSION VALIDATION -----#\n",
    "  sess_val = tf.Session(config=tf.ConfigProto(allow_soft_placement=True,\n",
    "                                              log_device_placement=False),\n",
    "                        graph=g_val)\n",
    "  with g_val.as_default():\n",
    "    sess_val.run([tf.local_variables_initializer(), \n",
    "                  tf.global_variables_initializer()])\n",
    "\n",
    "\n",
    "  #****************************************************************************\n",
    "  # 3. MAIN LOOP\n",
    "  #****************************************************************************\n",
    "  SUMMARY_FREQ = 100\n",
    "  SAVE_MODEL_FREQ = num_batches_per_epoch # 500\n",
    "  VAL_FREQ = num_batches_per_epoch # 500\n",
    "  PROFILE_FREQ = 5\n",
    "  \n",
    "  for step in range(prev_step, FLAGS.epoch * num_batches_per_epoch + 1): \n",
    "  #for step in range(0,3):\n",
    "    # AG 23/05/2018: limit number of iterations for testing\n",
    "    # for step in range(100):\n",
    "    epoch_decimal = step/num_batches_per_epoch\n",
    "    epoch = int(np.floor(epoch_decimal))\n",
    "    \n",
    "\n",
    "    # TF queue would pop batch until no file\n",
    "    try: \n",
    "      # TRAIN\n",
    "      with g_train.as_default():\n",
    "    \n",
    "          # With profiling\n",
    "          if (FLAGS.profile is True) and ((step % PROFILE_FREQ) == 0): \n",
    "            logger.info(\"Train with Profiling\")\n",
    "            run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "            run_metadata = tf.RunMetadata()\n",
    "          # Without profiling\n",
    "          else:\n",
    "            run_options = None\n",
    "            run_metadata = None\n",
    "          \n",
    "          # Reset streaming metrics\n",
    "          if step % (num_batches_per_epoch/4) == 1:\n",
    "            logger.info(\"Reset streaming metrics\")\n",
    "            sess_train.run([trn_reset])\n",
    "          \n",
    "          # MAIN RUN\n",
    "          tic = time.time()\n",
    "          train_op_v, trn_metrics_v, trn_summary_v = sess_train.run(\n",
    "              [train_op, trn_metrics, trn_summary], \n",
    "              options=run_options, \n",
    "              run_metadata=run_metadata)\n",
    "          toc = time.time()\n",
    "          \n",
    "          # Read streaming metrics\n",
    "          trn_read_v = sess_train.run(trn_read)\n",
    "          \n",
    "          # Write summary for profiling\n",
    "          if run_options is not None: \n",
    "            summary_writer.add_run_metadata(\n",
    "                run_metadata, 'step{:d}'.format(step))\n",
    "          \n",
    "          # Logging\n",
    "          logger.info('TRN'\n",
    "                + ' e-{:d}'.format(epoch)\n",
    "                + ' stp-{:d}'.format(step) \n",
    "                + ' {:.2f}s'.format(toc - tic) \n",
    "                + ' loss: {:.4f}'.format(trn_metrics_v['loss'])\n",
    "                + ' acc: {:.2f}%'.format(trn_metrics_v['acc']*100)\n",
    "                 )\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "      sess_train.close()\n",
    "      sess_val.close()\n",
    "      sys.exit()\n",
    "      \n",
    "    except tf.errors.InvalidArgumentError as e:\n",
    "      logger.warning('%d iteration contains NaN gradients. Discard.' % step)\n",
    "      logger.error(str(e))\n",
    "      continue\n",
    "      \n",
    "    else:\n",
    "      # WRITE SUMMARY\n",
    "      if (step % SUMMARY_FREQ) == 0:\n",
    "        logger.info(\"Write Train Summary\")\n",
    "        with g_train.as_default():\n",
    "          # Summaries from graph\n",
    "          summary_writer.add_summary(trn_summary_v, step)\n",
    "          \n",
    "      # SAVE MODEL\n",
    "      if (step % SAVE_MODEL_FREQ) == 100:  \n",
    "        logger.info(\"Save Model\")\n",
    "        with g_train.as_default():\n",
    "          train_checkpoint_dir = train_dir + '/checkpoint'\n",
    "          if not os.path.exists(train_checkpoint_dir):\n",
    "            os.makedirs(train_checkpoint_dir)\n",
    "\n",
    "          # Save ckpt from train session\n",
    "          ckpt_path = os.path.join(train_checkpoint_dir, 'model.ckpt')\n",
    "          saver.save(sess_train, ckpt_path, global_step=step)\n",
    "      \n",
    "      # VALIDATE MODEL\n",
    "      if (step % VAL_FREQ) == 100:    \n",
    "        #----- Validation -----#\n",
    "        with g_val.as_default():\n",
    "          logger.info(\"Start Validation\")\n",
    "          \n",
    "          # Restore ckpt to val session\n",
    "          latest_ckpt = tf.train.latest_checkpoint(train_checkpoint_dir)\n",
    "          saver.restore(sess_val, latest_ckpt)\n",
    "          \n",
    "          # Reset accumulators\n",
    "          accuracy_sum = 0\n",
    "          loss_sum = 0\n",
    "          sess_val.run(val_reset)\n",
    "          \n",
    "          for i in range(num_batches_val):\n",
    "            val_metrics_v, val_summary_str_v = sess_val.run(\n",
    "                [val_metrics, val_summary])\n",
    "             \n",
    "            # Update\n",
    "            accuracy_sum += val_metrics_v['acc']\n",
    "            loss_sum += val_metrics_v['loss']\n",
    "            \n",
    "            # Read\n",
    "            val_read_v = sess_val.run(val_read)\n",
    "            \n",
    "            # Get checkpoint number\n",
    "            ckpt_num = re.split('-', latest_ckpt)[-1]\n",
    "\n",
    "            # Logging\n",
    "            logger.info('VAL ckpt-{}'.format(ckpt_num) \n",
    "                        + ' bch-{:d}'.format(i) \n",
    "                        + ' cum_acc: {:.2f}%'.format(accuracy_sum/(i+1)*100) \n",
    "                        + ' cum_loss: {:.4f}'.format(loss_sum/(i+1))\n",
    "                       )\n",
    "          \n",
    "          # Average across batches\n",
    "          ave_acc = accuracy_sum / num_batches_val\n",
    "          ave_loss = loss_sum / num_batches_val\n",
    "           \n",
    "          logger.info('VAL ckpt-{}'.format(ckpt_num) \n",
    "                      + ' avg_acc: {:.2f}%'.format(ave_acc*100) \n",
    "                      + ' avg_loss: {:.4f}'.format(ave_loss)\n",
    "                     )\n",
    "          \n",
    "          logger.info(\"Write Val Summary\")\n",
    "          summary_val = tf.Summary()\n",
    "          summary_val.value.add(tag=\"val_acc\", simple_value=ave_acc)\n",
    "          summary_val.value.add(tag=\"val_loss\", simple_value=ave_loss)\n",
    "          summary_writer.add_summary(summary_val, step)\n",
    "          \n",
    "  # Close (main loop)\n",
    "  sess_train.close()\n",
    "  sess_val.close()\n",
    "  sys.exit()\n",
    "\n",
    "  \n",
    "def tower_fn(build_arch, \n",
    "             x, \n",
    "             y, \n",
    "             scope, \n",
    "             num_classes, \n",
    "             is_train=True, \n",
    "             reuse_variables=None):\n",
    "  \"\"\"Model tower to be run on each GPU.\n",
    "  \n",
    "  Author:\n",
    "    Ashley Gritzman 27/11/2018\n",
    "    \n",
    "  Args: \n",
    "    build_arch:\n",
    "    x: split of batch_x allocated to particular GPU\n",
    "    y: split of batch_y allocated to particular GPU\n",
    "    scope:\n",
    "    num_classes:\n",
    "    is_train:\n",
    "    reuse_variables: False for the first GPU, and True for subsequent GPUs\n",
    "  Returns:\n",
    "    loss: mean loss across samples for one tower (scalar)\n",
    "    scores: \n",
    "      If the architecture is a capsule network, then the scores are the output \n",
    "      activations of the class caps.\n",
    "      If the architecture is the CNN baseline, then the scores are the logits of \n",
    "      the final layer.\n",
    "      (samples_per_tower, n_classes)\n",
    "      (64/4=16, 5)\n",
    "  \"\"\"\n",
    "  \n",
    "  with tf.variable_scope(tf.get_variable_scope(), reuse=reuse_variables):\n",
    "    output = build_arch(x, is_train, num_classes=num_classes)\n",
    "    scores = output['scores']\n",
    "    \n",
    "  loss = mod.total_loss(scores, y)\n",
    "\n",
    "  return loss, scores\n",
    "\n",
    "\n",
    "def average_gradients(tower_grads):\n",
    "  \"\"\"Compute average gradients across all towers.\n",
    "  \n",
    "  Calculate the average gradient for each shared variable across all towers.\n",
    "  Note that this function provides a synchronization point across all towers.\n",
    "  \n",
    "  Credit:\n",
    "    https://github.com/naturomics/CapsNet-\n",
    "    Tensorflow/blob/master/dist_version/distributed_train.py\n",
    "  Args:\n",
    "    tower_grads: \n",
    "      List of lists of (gradient, variable) tuples. The outer list is over \n",
    "      individual gradients. The inner list is over the gradient calculation for       each tower.\n",
    "  Returns:\n",
    "    average_grads:\n",
    "      List of pairs of (gradient, variable) where the gradient has been \n",
    "      averaged across all towers.\n",
    "  \"\"\"\n",
    "  \n",
    "  average_grads = []\n",
    "  for grad_and_vars in zip(*tower_grads):\n",
    "  # Note that each grad_and_vars looks like the following:\n",
    "  #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n",
    "    grads = []\n",
    "    for g, _ in grad_and_vars:\n",
    "      # Add 0 dimension to the gradients to represent the tower.\n",
    "      expanded_g = tf.expand_dims(g, 0)\n",
    "\n",
    "      # Append on a 'tower' dimension which we will average over below.\n",
    "      grads.append(expanded_g)\n",
    "\n",
    "    # Average over the 'tower' dimension.\n",
    "    grad = tf.concat(axis=0, values=grads)\n",
    "    grad = tf.reduce_mean(grad, 0)\n",
    "\n",
    "    # Keep in mind that the Variables are redundant because they are shared\n",
    "    # across towers. So .. we will just return the first tower's pointer to\n",
    "    # the Variable.\n",
    "    v = grad_and_vars[0][1]\n",
    "    grad_and_var = (grad, v)\n",
    "    average_grads.append(grad_and_var)\n",
    "    \n",
    "  return average_grads\n",
    "          \n",
    "\n",
    "def extract_step(path):\n",
    "  \"\"\"Returns the step from the file format name of Tensorflow checkpoints.\n",
    "  \n",
    "  Credit:\n",
    "    Sara Sabour\n",
    "    https://github.com/Sarasra/models/blob/master/research/capsules/\n",
    "    experiment.py\n",
    "  Args:\n",
    "    path: The checkpoint path returned by tf.train.get_checkpoint_state.\n",
    "    The format is: {ckpnt_name}-{step}\n",
    "  Returns:\n",
    "    The last training step number of the checkpoint.\n",
    "  \"\"\"\n",
    "  file_name = os.path.basename(path)\n",
    "  return int(file_name.split('-')[-1])\n",
    "\n",
    "\n",
    "def load_training(saver, session, load_dir):\n",
    "  \"\"\"Loads a saved model into current session or initializes the directory.\n",
    "  \n",
    "  If there is no functioning saved model or FLAGS.restart is set, cleans the\n",
    "  load_dir directory. Otherwise, loads the latest saved checkpoint in load_dir\n",
    "  to session.\n",
    "  \n",
    "  Author:\n",
    "    Ashley Gritzman 26/09/2018\n",
    "  Credit:\n",
    "    Adapted from Sara Sabour\n",
    "    https://github.com/Sarasra/models/blob/master/research/capsules/\n",
    "    experiment.py\n",
    "  Args:\n",
    "    saver: An instance of tf.train.saver to load the model in to the session.\n",
    "    session: An instance of tf.Session with the built-in model graph.\n",
    "    load_dir: The directory which is used to load the latest checkpoint.\n",
    "    \n",
    "  Returns:\n",
    "    The latest saved step.\n",
    "  \"\"\"\n",
    "  \n",
    "  if gfile.Exists(load_dir): \n",
    "    ckpt = tf.train.get_checkpoint_state(load_dir)\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "      saver.restore(session, ckpt.model_checkpoint_path)\n",
    "      prev_step = extract_step(ckpt.model_checkpoint_path)\n",
    "      logger.info(\"Restored checkpoint\")\n",
    "    else:\n",
    "      raise IOError(\"\"\"AG: load_ckpt directory exists but cannot find a valid \n",
    "                    checkpoint to resore, consider using the reset flag\"\"\")\n",
    "  else:\n",
    "    raise IOError(\"AG: load_ckpt directory does not exist\")\n",
    "    \n",
    "  return prev_step\n",
    "\n",
    "\n",
    "def find_checkpoint(load_dir, seen_step):\n",
    "  \"\"\"Finds the global step for the latest written checkpoint to the load_dir.\n",
    "  \n",
    "  Credit:\n",
    "    Sara Sabour\n",
    "    https://github.com/Sarasra/models/blob/master/research/capsules/\n",
    "    experiment.py\n",
    "  Args:\n",
    "    load_dir: The directory address to look for the training checkpoints.\n",
    "    seen_step: Latest step which evaluation has been done on it.\n",
    "  Returns:\n",
    "    The latest new step in the load_dir and the file path of the latest model\n",
    "    in load_dir. If no new file is found returns -1 and None.\n",
    "  \"\"\"\n",
    "  ckpt = tf.train.get_checkpoint_state(load_dir)\n",
    "  if ckpt and ckpt.model_checkpoint_path:\n",
    "    global_step = extract_step(ckpt.model_checkpoint_path)\n",
    "    if int(global_step) != seen_step:\n",
    "      return int(global_step), ckpt.model_checkpoint_path\n",
    "  return -1, None\n",
    "          \n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#   absl.app.run(main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-07 10:32:02 INFO: Parameters saved to file: ../logs/smallNORB/20200407_/train/params/params.json\n",
      "2020-04-07 10:32:02 INFO: Using dataset: smallNORB\n"
     ]
    }
   ],
   "source": [
    "FLAGS(sys.argv)\n",
    "\n",
    "# Directories\n",
    "train_dir, train_summary_dir = setup_train_directories()\n",
    "\n",
    "# Logger\n",
    "setup_logger(logger_dir=train_dir, name=\"logger_train.txt\")\n",
    "\n",
    "# Hyperparameters\n",
    "load_or_save_hyperparams(train_dir)\n",
    "\n",
    "# Get dataset hyperparameters\n",
    "logger.info('Using dataset: {}'.format(FLAGS.dataset))\n",
    "dataset_size_train  = get_dataset_size_train(FLAGS.dataset)\n",
    "dataset_size_val  = get_dataset_size_validate(FLAGS.dataset)\n",
    "build_arch      = get_dataset_architecture(FLAGS.dataset)\n",
    "num_classes     = get_num_classes(FLAGS.dataset)\n",
    "create_inputs_train = get_create_inputs(FLAGS.dataset, mode=\"train\")\n",
    "create_inputs_val   = get_create_inputs(FLAGS.dataset, mode=\"validate\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-07 10:40:31 INFO: Overwrite dataset info from restored data version.\n",
      "2020-04-07 10:40:31 INFO: Reusing dataset smallnorb (/home/dehghani/tensorflow_datasets/smallnorb/2.0.0)\n",
      "2020-04-07 10:40:31 INFO: Constructing tf.data.Dataset for split train, from /home/dehghani/tensorflow_datasets/smallnorb/2.0.0\n"
     ]
    }
   ],
   "source": [
    "# Get batch from data queue. Batch size is FLAGS.batch_size, which is then \n",
    "# divided across multiple GPUs\n",
    "# input_dict = create_inputs_train()\n",
    "# batch_x = input_dict['image']\n",
    "# batch_labels = input_dict['label']\n",
    "import tensorflow_datasets as tfds\n",
    "dataset = tfds.load('smallnorb', split=\"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-07 10:42:46 INFO: input shape: (96, 96, 1)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'truncated_normal_initializer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-e533f5f4c943>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_arch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'scores'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-c93a1ebddf20>\u001b[0m in \u001b[0;36mbuild_arch_smallnorb\u001b[0;34m(input, is_train, num_classes)\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;31m# instead of initializing bias with constant 0, a truncated normal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;31m# initializer is exploited here for higher stability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0mbias_initializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtruncated_normal_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstddev\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;31m# AG 13/11/2018\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'truncated_normal_initializer'"
     ]
    }
   ],
   "source": [
    "output = build_arch(a['image'], False, num_classes=num_classes)\n",
    "scores = output['scores']\n",
    "loss = mod.total_loss(scores, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-07 11:12:43 INFO: input shape: (96, 96, 1)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'variable_scope'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-5b5f136431a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m#----- Convolutional Layer 1 -----#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'relu_conv1'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m   output = slim.conv2d(output, \n\u001b[1;32m     33\u001b[0m   \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'variable_scope'"
     ]
    }
   ],
   "source": [
    "input = a['image']\n",
    "is_train = False\n",
    "logger.info('input shape: {}'.format(input.get_shape()))\n",
    "batch_size = int(input.get_shape()[0])\n",
    "spatial_size = int(input.get_shape()[1])\n",
    "\n",
    "# xavier initialization is necessary here to provide higher stability\n",
    "# initializer = tf.truncated_normal_initializer(mean=0.0, stddev=0.01)\n",
    "# instead of initializing bias with constant 0, a truncated normal \n",
    "# initializer is exploited here for higher stability\n",
    "bias_initializer = tf.random_normal_initializer(mean=0.0, stddev=0.01) \n",
    "\n",
    "# AG 13/11/2018\n",
    "# In response to a question on OpenReview, Hinton et al. wrote the \n",
    "# following:\n",
    "# \"We use a weight decay loss with a small factor of .0000002 rather than \n",
    "# the reconstruction loss.\"\n",
    "# https://openreview.net/forum?id=HJWLfGWRb&noteId=rJeQnSsE3X\n",
    "weights_regularizer = tf.keras.regularizers.l2(0.0000002)\n",
    "\n",
    "# weights_initializer=initializer,\n",
    "\n",
    "#----- Batch Norm -----#\n",
    "# output = tf.nn.batch_normalization(\n",
    "#     input\n",
    "# )\n",
    "\n",
    "batch_norm = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "#----- Convolutional Layer 1 -----#\n",
    "# output = slim.conv2d(output, \n",
    "# num_outputs=FLAGS.A, \n",
    "# kernel_size=[5, 5], \n",
    "# stride=2, \n",
    "# padding='SAME', \n",
    "# scope=scope, \n",
    "# activation_fn=tf.nn.relu)\n",
    "\n",
    "# spatial_size = int(output.get_shape()[1])\n",
    "# assert output.get_shape() == [batch_size, spatial_size, spatial_size, \n",
    "#                             FLAGS.A]\n",
    "# logger.info('relu_conv1 output shape: {}'.format(output.get_shape()))\n",
    "\n",
    "conv1 = tf.keras.layers.Conv2d(filters=FLAGS.A, kernel_size=[5,5], strides=(2, 2), \n",
    "                               padding='same', \n",
    "                               activation=tf.nn.relu, use_bias=True,\n",
    "                               kernel_initializer='glorot_uniform', bias_initializer='zeros',\n",
    "                               kernel_regularizer=weights_regularizer, \n",
    "                               bias_regularizer=weights_regularizer, activity_regularizer=None,\n",
    "                               kernel_constraint=None, bias_constraint=None)\n",
    "\n",
    "#----- Primary Capsules -----#\n",
    "# with tf.variable_scope('primary_caps') as scope:\n",
    "#   pose = slim.conv2d(output, \n",
    "#   num_outputs=FLAGS.B * 16, \n",
    "#   kernel_size=[1, 1], \n",
    "#   stride=1, \n",
    "#   padding='VALID', \n",
    "#   scope='pose', \n",
    "#   activation_fn=None)\n",
    "#   activation = slim.conv2d(\n",
    "#       output, \n",
    "#       num_outputs=FLAGS.B, \n",
    "#       kernel_size=[1, 1], \n",
    "#       stride=1, \n",
    "#       padding='VALID', \n",
    "#       scope='activation', \n",
    "#       activation_fn=tf.nn.sigmoid)\n",
    "\n",
    "#   spatial_size = int(pose.get_shape()[1])\n",
    "#   pose = tf.reshape(pose, shape=[batch_size, spatial_size, spatial_size, \n",
    "#                                  FLAGS.B, 16], name='pose')\n",
    "#   activation = tf.reshape(\n",
    "#       activation, \n",
    "#       shape=[batch_size, spatial_size, spatial_size, FLAGS.B, 1], \n",
    "#       name=\"activation\")\n",
    "\n",
    "#   assert pose.get_shape() == [batch_size, spatial_size, spatial_size, \n",
    "#                               FLAGS.B, 16]\n",
    "#   assert activation.get_shape() == [batch_size, spatial_size, spatial_size,\n",
    "#                                     FLAGS.B, 1]\n",
    "#   logger.info('primary_caps pose shape: {}'.format(pose.get_shape()))\n",
    "#   logger.info('primary_caps activation shape {}'\n",
    "#               .format(activation.get_shape()))\n",
    "\n",
    "#   tf.summary.histogram(\"activation\", activation)\n",
    "\n",
    "pcaps_pos_conv = tf.keras.layers.Conv2d(filters=FLAGS.B * 16, kernel_size=[1,1], strides=(1, 1), \n",
    "                               padding='valid', \n",
    "                               activation=None, use_bias=True,\n",
    "                               kernel_initializer='glorot_uniform', \n",
    "                               bias_initializer='zeros',\n",
    "                               kernel_regularizer=weights_regularizer, \n",
    "                               bias_regularizer=weights_regularizer, \n",
    "                               activity_regularizer=None,\n",
    "                               kernel_constraint=None, bias_constraint=None)\n",
    "p_caps_activation_conv = tf.keras.layers.Conv2d(filters=FLAGS.B, kernel_size=[1,1], strides=(1, 1), \n",
    "                               padding='valid', \n",
    "                               activation=tf.nn.sigmoid, use_bias=True,\n",
    "                               kernel_initializer='glorot_uniform', \n",
    "                               bias_initializer='zeros',\n",
    "                               kernel_regularizer=weights_regularizer, \n",
    "                               bias_regularizer=weights_regularizer, \n",
    "                               activity_regularizer=None,\n",
    "                               kernel_constraint=None, bias_constraint=None)\n",
    "#----- Conv Caps 1 -----#\n",
    "# activation_in: (64, 7, 7, 8, 1) \n",
    "# pose_in: (64, 7, 7, 16, 16) \n",
    "# activation_out: (64, 5, 5, 32, 1)\n",
    "# pose_out: (64, 5, 5, 32, 16)\n",
    "activation, pose = ConvCaps(\n",
    "    kernel = 3, \n",
    "    stride = 2, \n",
    "    ncaps_out = FLAGS.C, \n",
    "    name = 'conv_caps1', \n",
    "    weights_regularizer = weights_regularizer)\n",
    "\n",
    "#----- Conv Caps 2 -----#\n",
    "# activation_in: (64, 7, 7, 8, 1) \n",
    "# pose_in: (64, 7, 7, 16, 1) \n",
    "# activation_out: (64, 5, 5, 32, 1)\n",
    "# pose_out: (64, 5, 5, 32, 16)\n",
    "activation, pose = ConvCaps(\n",
    "    kernel=3, \n",
    "    stride=1, \n",
    "    ncaps_out=FLAGS.D, \n",
    "    name='conv_caps2', \n",
    "    weights_regularizer=weights_regularizer)\n",
    "\n",
    "#----- Class Caps -----#\n",
    "# activation_in: (64, 5, 5, 32, 1)\n",
    "# pose_in: (64, 5, 5, 32, 16)\n",
    "# activation_out: (64, 5)\n",
    "# pose_out: (64, 5, 16) \n",
    "activation_out, pose_out = FcCaps(\n",
    "    ncaps_out=num_classes,\n",
    "    name='class_caps',\n",
    "    weights_regularizer=weights_regularizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
