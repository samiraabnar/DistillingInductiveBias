{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab len:  10034\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from util import constants\n",
    "from util.config_util import get_model_params, get_task_params, get_train_params\n",
    "from tf2_models.trainer import Trainer\n",
    "from absl import app\n",
    "from absl import flags\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "from util.models import MODELS\n",
    "from util.tasks import TASKS\n",
    "\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import seaborn as sns; sns.set()\n",
    "from attention_graph_util import *\n",
    "\n",
    "\n",
    "\n",
    "chkpt_dir='../tf_ckpts'\n",
    "\n",
    "task_name = 'word_sv_agreement_vp'\n",
    "task_params = get_task_params()\n",
    "task_params.batch_size = 1\n",
    "task = TASKS[task_name](task_params, data_dir='../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model config: small_gpt_v9\n",
      "{'embedding_dim': 128, 'resid_pdrop': 0.4, 'embd_pdrop': 0.2, 'attn_pdrop': 0.6, 'initializer_range': 0.05}\n",
      "model config: small_lstm_v4\n",
      "{'hidden_dim': 256, 'embedding_dim': 256, 'depth': 2, 'hidden_dropout_rate': 0.8, 'input_dropout_rate': 0.2, 'initializer_range': 0.1}\n",
      "Restored student from ../tf_ckpts/word_sv_agreement_vp/offline_pure_distill_4_cosinerestart_slow_teacher_cl_lstm_em-256_h-256_d-2_hdrop-0.8_indrop-0.2_small_lstm_v4_0.001_samira_offlineteacher_v5_student_cl_bert_h-128_d-6_rdrop-0.4_adrop-0.6_indrop-0.2_small_gpt_v9_samira_fd5/ckpt-60\n",
      "(1, 2)\n"
     ]
    }
   ],
   "source": [
    "# LSTM to Transformer VP\n",
    "exp_name='samira_fd5'\n",
    "teacher_exp_name='0.001_samira_offlineteacher_v5'\n",
    "teacher_config='small_lstm_v4'\n",
    "model_name='cl_bert'\n",
    "teacher_model='cl_lstm'\n",
    "model_config='small_gpt_v9'\n",
    "distill_config='pure_distill_4_cosinerestart_slow'\n",
    "distill_mode='offline'\n",
    "\n",
    "# exp_name='samira_fd7'\n",
    "# teacher_exp_name='0.001_samira_offlineteacher_v5'\n",
    "# teacher_config='small_lstm_v4'\n",
    "# model_name='cl_bert'\n",
    "# teacher_model='cl_lstm'\n",
    "# model_config='short_gpt_v9'\n",
    "# distill_config='pure_distill_4_cosinerestart_slow'\n",
    "# distill_mode='offline'\n",
    "\n",
    "\n",
    "cl_token = task.databuilder.sentence_encoder().encode(constants.bos)\n",
    "hparams=get_model_params(task, model_name, model_config)\n",
    "hparams.output_attentions = True\n",
    "hparams.output_embeddings = True\n",
    "hparams.output_hidden_states = True\n",
    "model_1 = MODELS[model_name](hparams=hparams, cl_token=cl_token)\n",
    "teacher_model = MODELS[teacher_model](hparams=get_model_params(task, teacher_model, teacher_config), cl_token=cl_token)\n",
    "\n",
    "\n",
    "ckpt_dir = os.path.join(chkpt_dir, task.name,\n",
    "                              '_'.join([distill_mode,distill_config,\n",
    "                                        \"teacher\", teacher_model.model_name, \n",
    "                                        teacher_config,\n",
    "                                        teacher_exp_name,\n",
    "                                       \"student\",model_1.model_name,\n",
    "                                        str(model_config),\n",
    "                                        exp_name]))\n",
    "\n",
    "ckpt = tf.train.Checkpoint(net=model_1)\n",
    "manager = tf.train.CheckpointManager(ckpt, ckpt_dir, max_to_keep=None)\n",
    "\n",
    "ckpt.restore(manager.latest_checkpoint)\n",
    "if manager.latest_checkpoint:\n",
    "  print(\"Restored student from {}\".format(manager.latest_checkpoint))\n",
    "\n",
    "model_1.compile(loss=task.get_loss_fn(), metrics=task.metrics())\n",
    "x, y = iter(task.valid_dataset).next()\n",
    "out = model_1(x)\n",
    "print(out.shape)\n",
    "#model_1.evaluate(task.test_dataset, steps=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM to Transformer VP\n",
    "exp_name='samira_fd5'\n",
    "teacher_exp_name='0.001_samira_offlineteacher_v5'\n",
    "teacher_config='small_lstm_v4'\n",
    "model_name='cl_bert'\n",
    "teacher_model='cl_lstm'\n",
    "model_config='small_gpt_v9'\n",
    "distill_config='pure_distill_4_cosinerestart_slow'\n",
    "distill_mode='offline'\n",
    "\n",
    "cl_token = task.databuilder.sentence_encoder().encode(constants.bos)\n",
    "hparams=get_model_params(task, model_name, model_config)\n",
    "hparams.output_attentions = True\n",
    "hparams.output_embeddings = True\n",
    "hparams.output_hidden_states = True\n",
    "model_2 = MODELS[model_name](hparams=hparams, cl_token=cl_token)\n",
    "teacher_model = MODELS[teacher_model](hparams=get_model_params(task, teacher_model, teacher_config), cl_token=cl_token)\n",
    "\n",
    "\n",
    "ckpt_dir = os.path.join(chkpt_dir, task.name,\n",
    "                              '_'.join([distill_mode,distill_config,\n",
    "                                        \"teacher\", teacher_model.model_name, \n",
    "                                        teacher_config,\n",
    "                                        teacher_exp_name,\n",
    "                                       \"student\",model_2.model_name,\n",
    "                                        str(model_config),\n",
    "                                        exp_name]))\n",
    "\n",
    "ckpt = tf.train.Checkpoint(net=model_2)\n",
    "manager = tf.train.CheckpointManager(ckpt, ckpt_dir, max_to_keep=None)\n",
    "\n",
    "ckpt.restore(manager.latest_checkpoint)\n",
    "if manager.latest_checkpoint:\n",
    "  print(\"Restored student from {}\".format(manager.latest_checkpoint))\n",
    "\n",
    "model_2.compile(loss=task.get_loss_fn(), metrics=task.metrics())\n",
    "x, y = iter(task.valid_dataset).next()\n",
    "out = model_2(x)\n",
    "print(out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_iter = iter(task.valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = v_iter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos> be ready to serve at every opportunity , yet making sure that your fellow servers <eos>\n",
      "[False]\n",
      "(6, 8, 18, 18)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "index = 0\n",
    "sentence =  task.databuilder.sentence_encoder().decode(x[index])#'<bos> the young girl who plays with the older kids is happy <eos>'\n",
    "#sentence = '<bos> the young girl who plays with the older kids <eos>'\n",
    "print(sentence)\n",
    "#'<bos> the boys who play football with the old man <eos>'\n",
    "encoded_sentence = task.databuilder.sentence_encoder().encode(sentence)\n",
    "logits, extra = model_1.detailed_call(np.asarray([encoded_sentence]), training=False)\n",
    "print((np.argmax(logits) == [0]))\n",
    "logits, last_state, presents, attentions, embeddings = extra\n",
    "_attentions = [att.numpy() for att in attentions]\n",
    "attentions_mat = np.asarray(_attentions)[:,0]\n",
    "print(attentions_mat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "att_mat = attentions_mat.sum(axis=1)\n",
    "adj_mat, labels_to_index = get_adjmat(mat=att_mat, input_tokens=['<cls>']+sentence.split())\n",
    "\n",
    "G = draw_attention_graph(adj_mat,labels_to_index, n_layers=attentions_mat.shape[0], length=attentions_mat.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig('raw_attentions.png', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_att_mat = attentions_mat.sum(axis=1)\n",
    "res_att_mat = res_att_mat + 8*np.eye(res_att_mat.shape[1])[None,...]\n",
    "res_att_mat = 8*res_att_mat / res_att_mat.sum(axis=-1)[...,None]\n",
    " \n",
    "res_adj_mat, res_labels_to_index = get_adjmat(mat=res_att_mat, input_tokens=['<cls>']+sentence.split())\n",
    "\n",
    "res_G = draw_attention_graph(res_adj_mat,res_labels_to_index, n_layers=res_att_mat.shape[0], length=res_att_mat.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_nodes = []\n",
    "input_nodes = []\n",
    "for key in labels_to_index:\n",
    "    if 'L6' in key:\n",
    "        output_nodes.append(key)\n",
    "    if labels_to_index[key] < attentions_mat.shape[-1]:\n",
    "        input_nodes.append(key)\n",
    "\n",
    "flow_values = compute_flows(res_G, res_labels_to_index, input_nodes, length=attentions_mat.shape[-1])\n",
    "flow_G = draw_attention_graph(flow_values,labels_to_index, n_layers=attentions_mat.shape[0], length=attentions_mat.shape[-1])\n",
    "#plt.savefig('flow_attentions.png', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_flow = np.maximum(flow_values*8 - res_adj_mat, 0)\n",
    "moreflow_G = draw_attention_graph(more_flow,labels_to_index, n_layers=attentions_mat.shape[0], length=attentions_mat.shape[-1])\n",
    "#plt.savefig('flow-raw_attentions.png', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_flow = np.maximum(res_adj_mat - flow_values*8, 0)\n",
    "moreflow_G = draw_attention_graph(more_flow,labels_to_index, n_layers=attentions_mat.shape[0], length=attentions_mat.shape[-1])\n",
    "#plt.savefig('flow-raw_attentions.png', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "att_sum_heads =  attentions_mat.sum(axis=1)/8\n",
    "joint_attentions = compute_joint_attention(att_sum_heads, add_residual=True)\n",
    "joint_att_adjmat, joint_labels_to_index = get_adjmat(mat=joint_attentions, input_tokens=['<cls>']+sentence.split())\n",
    "\n",
    "G = draw_attention_graph(joint_att_adjmat,joint_labels_to_index, n_layers=joint_attentions.shape[0], length=joint_attentions.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.maximum(joint_att_adjmat*8 - res_adj_mat, 0)\n",
    "draw_attention_graph(A,joint_labels_to_index, n_layers=joint_attentions.shape[0], length=joint_attentions.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.maximum(res_adj_mat - joint_att_adjmat*8, 0)\n",
    "draw_attention_graph(A,joint_labels_to_index, n_layers=joint_attentions.shape[0], length=joint_attentions.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = att_mat.shape[-1]\n",
    "final_layer_attention_raw = flow_values[6*length:,5*length:6*length]#joint_attentions[-1]#att_mat[-1]\n",
    "relevance_attention_raw = final_layer_attention_raw[0]\n",
    "rel_rank_att_raw = np.argsort(relevance_attention_raw)\n",
    "\n",
    "print(rel_rank_att_raw, relevance_attention_raw[rel_rank_att_raw])\n",
    "print(\"least important token:\", (['<cls>']+sentence.split())[rel_rank_att_raw[0]])\n",
    "print(\"most important token:\", (['<cls>']+sentence.split())[rel_rank_att_raw[-1]])\n",
    "print(\"2nd most important token:\", (['<cls>']+sentence.split())[rel_rank_att_raw[-2]])\n",
    "print(\"3rd most important token:\", (['<cls>']+sentence.split())[rel_rank_att_raw[-3]])\n",
    "print(\"4th most important token:\", (['<cls>']+sentence.split())[rel_rank_att_raw[-4]])\n",
    "print(\"5th most important token:\", (['<cls>']+sentence.split())[rel_rank_att_raw[-5]])\n",
    "print(\"6th most important token:\", (['<cls>']+sentence.split())[rel_rank_att_raw[-6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_layer_attention_raw = att_mat[-1]\n",
    "relevance_attention_raw = final_layer_attention_raw[0]\n",
    "rel_rank_att_raw = np.argsort(relevance_attention_raw)\n",
    "\n",
    "print(rel_rank_att_raw, relevance_attention_raw[rel_rank_att_raw])\n",
    "print(\"least important token:\", (['<cls>']+sentence.split())[rel_rank_att_raw[0]])\n",
    "print(\"most important token:\", (['<cls>']+sentence.split())[rel_rank_att_raw[-1]])\n",
    "print(\"2nd most important token:\", (['<cls>']+sentence.split())[rel_rank_att_raw[-2]])\n",
    "print(\"3rd most important token:\", (['<cls>']+sentence.split())[rel_rank_att_raw[-3]])\n",
    "print(\"4th most important token:\", (['<cls>']+sentence.split())[rel_rank_att_raw[-4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_layer_attention_raw = joint_attentions[-1]\n",
    "relevance_attention_raw = final_layer_attention_raw[-1]\n",
    "rel_rank_att_raw = np.argsort(relevance_attention_raw)\n",
    "\n",
    "print(rel_rank_att_raw, relevance_attention_raw[rel_rank_att_raw])\n",
    "print(\"least important token:\", (['<cls>']+sentence.split())[rel_rank_att_raw[0]])\n",
    "print(\"most important token:\", (['<cls>']+sentence.split())[rel_rank_att_raw[-1]])\n",
    "print(\"2nd most important token:\", (['<cls>']+sentence.split())[rel_rank_att_raw[-2]])\n",
    "print(\"3rd most important token:\", (['<cls>']+sentence.split())[rel_rank_att_raw[-3]])\n",
    "print(\"4th most important token:\", (['<cls>']+sentence.split())[rel_rank_att_raw[-4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "sentence =  task.databuilder.sentence_encoder().decode(x[index])#'<bos> the young girl who plays with the older kids is happy <eos>'\n",
    "sentence = '<bos> according to contemporary accounts in the NNP NNPS the college ' + \\\n",
    "           'of cardinals was divided into NNS of charles NN ( VBZ NNP ) and the imperial party ( VBZ NNP ) , '+\\\n",
    "          'but the exact reconstruction of these parties <eos>'\n",
    "sentence = '<bos> the young girl who plays with the older kids <eos>'\n",
    "print(sentence)\n",
    "#'<bos> the boys who play football with the old man <eos>'\n",
    "encoded_sentence = task.databuilder.sentence_encoder().encode(sentence)\n",
    "\n",
    "logits, extra = model_1.detailed_call(np.asarray([encoded_sentence]))\n",
    "print((np.argmax(logits)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 6\n",
      "1 6\n",
      "2 5\n",
      "3 12\n",
      "4 13\n",
      "5 6\n",
      "6 8\n",
      "7 8\n",
      "8 4\n",
      "9 4\n",
      "10 4\n",
      "11 7\n",
      "12 21\n",
      "13 6\n",
      "14 7\n",
      "15 5\n",
      "16 15\n",
      "17 4\n",
      "18 7\n",
      "19 4\n",
      "20 5\n",
      "21 6\n",
      "22 8\n",
      "23 11\n",
      "24 5\n",
      "25 5\n",
      "26 18\n",
      "27 5\n",
      "28 20\n",
      "29 17\n",
      "30 10\n",
      "31 7\n",
      "32 11\n",
      "33 9\n",
      "34 4\n",
      "35 5\n",
      "36 8\n",
      "37 4\n",
      "38 32\n",
      "39 6\n",
      "40 5\n",
      "41 5\n",
      "42 21\n",
      "43 5\n",
      "44 6\n",
      "45 8\n",
      "46 5\n",
      "47 22\n",
      "48 9\n",
      "49 8\n",
      "50 11\n",
      "51 11\n",
      "52 5\n",
      "53 4\n",
      "54 5\n",
      "55 3\n",
      "56 10\n",
      "57 11\n",
      "58 25\n",
      "59 8\n",
      "60 19\n",
      "61 4\n",
      "62 25\n",
      "63 13\n",
      "64 3\n",
      "65 4\n",
      "66 13\n",
      "67 5\n",
      "68 6\n",
      "69 8\n",
      "70 13\n",
      "71 4\n",
      "72 6\n",
      "73 4\n",
      "74 20\n",
      "75 31\n",
      "76 8\n",
      "77 9\n",
      "78 21\n",
      "79 9\n",
      "80 6\n",
      "81 14\n",
      "82 27\n",
      "83 6\n",
      "84 14\n",
      "85 11\n",
      "86 4\n",
      "87 5\n",
      "88 4\n",
      "89 6\n",
      "90 20\n",
      "91 10\n",
      "92 15\n",
      "93 14\n",
      "94 5\n",
      "95 6\n",
      "96 4\n",
      "97 19\n",
      "98 4\n",
      "99 18\n",
      "100 13\n",
      "101 32\n",
      "102 5\n",
      "103 5\n",
      "104 3\n",
      "105 6\n",
      "106 5\n",
      "107 4\n",
      "108 20\n",
      "109 10\n",
      "110 3\n",
      "111 6\n",
      "112 24\n",
      "113 10\n",
      "114 9\n",
      "115 4\n",
      "116 15\n",
      "117 7\n",
      "118 6\n",
      "119 12\n",
      "120 5\n",
      "121 4\n",
      "122 5\n",
      "123 7\n",
      "124 28\n",
      "125 3\n",
      "126 5\n",
      "127 14\n",
      "128 4\n",
      "129 22\n",
      "130 31\n",
      "131 10\n",
      "132 8\n",
      "133 4\n",
      "134 21\n",
      "135 4\n",
      "136 4\n",
      "137 16\n",
      "138 4\n",
      "139 30\n",
      "140 15\n",
      "141 16\n",
      "142 6\n",
      "143 9\n",
      "144 10\n",
      "145 4\n",
      "146 4\n",
      "147 43\n",
      "148 10\n",
      "149 26\n",
      "150 4\n",
      "151 4\n",
      "152 6\n",
      "153 4\n",
      "154 6\n",
      "155 7\n",
      "156 4\n",
      "157 24\n",
      "158 4\n",
      "159 9\n",
      "160 14\n",
      "161 8\n",
      "162 9\n",
      "163 4\n",
      "164 7\n",
      "165 6\n",
      "166 7\n",
      "167 4\n",
      "168 8\n",
      "169 11\n",
      "170 3\n",
      "171 5\n",
      "172 7\n",
      "173 5\n",
      "174 11\n",
      "175 4\n",
      "176 10\n",
      "177 5\n",
      "178 15\n",
      "179 27\n",
      "180 6\n",
      "181 11\n",
      "182 7\n",
      "183 6\n",
      "184 5\n",
      "185 6\n",
      "186 5\n",
      "187 5\n",
      "188 4\n",
      "189 17\n",
      "190 14\n",
      "191 4\n",
      "192 14\n",
      "193 11\n",
      "194 8\n",
      "195 11\n",
      "196 12\n",
      "197 17\n",
      "198 7\n",
      "199 10\n",
      "200 10\n",
      "201 4\n",
      "202 4\n",
      "203 17\n",
      "204 4\n",
      "205 7\n",
      "206 4\n",
      "207 12\n",
      "208 5\n",
      "209 18\n",
      "210 6\n",
      "211 4\n",
      "212 11\n",
      "213 10\n",
      "214 14\n",
      "215 7\n",
      "216 4\n",
      "217 6\n",
      "218 7\n",
      "219 4\n",
      "220 8\n",
      "221 6\n",
      "222 5\n",
      "223 5\n",
      "224 13\n",
      "225 4\n",
      "226 4\n",
      "227 13\n",
      "228 9\n",
      "229 5\n",
      "230 6\n",
      "231 4\n",
      "232 5\n",
      "233 4\n",
      "234 6\n",
      "235 4\n",
      "236 5\n",
      "237 5\n",
      "238 4\n",
      "239 7\n",
      "240 4\n",
      "241 9\n",
      "242 9\n",
      "243 4\n",
      "244 6\n",
      "245 5\n",
      "246 4\n",
      "247 4\n",
      "248 13\n",
      "249 3\n",
      "250 8\n",
      "251 7\n",
      "252 11\n",
      "253 6\n",
      "254 5\n",
      "255 4\n",
      "256 6\n",
      "257 11\n",
      "258 4\n",
      "259 24\n",
      "260 10\n",
      "261 5\n",
      "262 26\n",
      "263 21\n",
      "264 9\n",
      "265 8\n",
      "266 4\n",
      "267 8\n",
      "268 4\n",
      "269 6\n",
      "270 8\n",
      "271 4\n",
      "272 4\n",
      "273 7\n",
      "274 6\n",
      "275 15\n",
      "276 9\n",
      "277 27\n",
      "278 4\n",
      "279 4\n",
      "280 10\n",
      "281 11\n",
      "282 5\n",
      "283 13\n",
      "284 3\n",
      "285 5\n",
      "286 8\n",
      "287 7\n",
      "288 12\n",
      "289 6\n",
      "290 6\n",
      "291 8\n",
      "292 15\n",
      "293 10\n",
      "294 6\n",
      "295 3\n",
      "296 5\n",
      "297 3\n",
      "298 4\n",
      "299 8\n",
      "300 8\n",
      "301 31\n",
      "302 12\n",
      "303 4\n",
      "304 8\n",
      "305 5\n",
      "306 6\n",
      "307 4\n",
      "308 4\n",
      "309 6\n",
      "310 5\n",
      "311 5\n",
      "312 4\n",
      "313 5\n",
      "314 6\n",
      "315 10\n",
      "316 6\n",
      "317 6\n",
      "318 3\n",
      "319 15\n",
      "320 9\n",
      "321 7\n",
      "322 4\n",
      "323 7\n",
      "324 11\n",
      "325 4\n",
      "326 9\n",
      "327 4\n",
      "328 9\n",
      "329 9\n",
      "330 10\n",
      "331 4\n",
      "332 28\n",
      "333 4\n",
      "334 9\n",
      "335 9\n",
      "336 18\n",
      "337 6\n",
      "338 8\n",
      "339 6\n",
      "340 4\n",
      "341 4\n",
      "342 17\n",
      "343 8\n",
      "344 19\n",
      "345 5\n",
      "346 4\n",
      "347 4\n",
      "348 6\n",
      "349 28\n",
      "350 7\n",
      "351 9\n",
      "352 4\n",
      "353 4\n",
      "354 7\n",
      "355 15\n",
      "356 15\n",
      "357 4\n",
      "358 7\n",
      "359 7\n",
      "360 19\n",
      "361 4\n",
      "362 22\n",
      "363 5\n",
      "364 19\n",
      "365 13\n",
      "366 21\n",
      "367 23\n",
      "368 4\n",
      "369 35\n",
      "370 4\n",
      "371 9\n",
      "372 12\n",
      "373 4\n",
      "374 9\n",
      "375 17\n",
      "376 4\n",
      "377 7\n",
      "378 8\n",
      "379 6\n",
      "380 6\n",
      "381 26\n",
      "382 30\n",
      "383 15\n",
      "384 16\n",
      "385 5\n",
      "386 12\n",
      "387 6\n",
      "388 16\n",
      "389 4\n",
      "390 4\n",
      "391 19\n",
      "392 4\n",
      "393 6\n",
      "394 6\n",
      "395 7\n",
      "396 10\n",
      "397 8\n",
      "398 11\n",
      "399 5\n",
      "400 7\n",
      "401 4\n",
      "402 17\n",
      "403 4\n",
      "404 27\n",
      "405 6\n",
      "406 4\n",
      "407 5\n",
      "408 12\n",
      "409 4\n",
      "410 7\n",
      "411 6\n",
      "412 3\n",
      "413 4\n",
      "414 13\n",
      "415 5\n",
      "416 4\n",
      "417 5\n",
      "418 6\n",
      "419 5\n",
      "420 24\n",
      "421 11\n",
      "422 11\n",
      "423 18\n",
      "424 5\n",
      "425 7\n",
      "426 10\n",
      "427 9\n",
      "428 27\n",
      "429 12\n",
      "430 4\n",
      "431 4\n",
      "432 11\n",
      "433 4\n",
      "434 5\n",
      "435 5\n",
      "436 4\n",
      "437 5\n",
      "438 7\n",
      "439 4\n",
      "440 3\n",
      "441 4\n",
      "442 7\n",
      "443 8\n",
      "444 5\n",
      "445 14\n",
      "446 6\n",
      "447 23\n",
      "448 12\n",
      "449 4\n",
      "450 5\n",
      "451 4\n",
      "452 9\n",
      "453 6\n",
      "454 8\n",
      "455 9\n",
      "456 5\n",
      "457 6\n",
      "458 19\n",
      "459 4\n",
      "460 15\n",
      "461 21\n",
      "462 24\n",
      "463 4\n",
      "464 20\n",
      "465 4\n",
      "466 4\n",
      "467 6\n",
      "468 4\n",
      "469 8\n",
      "470 9\n",
      "471 10\n",
      "472 5\n",
      "473 18\n",
      "474 9\n",
      "475 8\n",
      "476 5\n",
      "477 8\n",
      "478 13\n",
      "479 39\n",
      "480 8\n",
      "481 6\n",
      "482 16\n",
      "483 6\n",
      "484 4\n",
      "485 5\n",
      "486 5\n",
      "487 4\n",
      "488 4\n",
      "489 7\n",
      "490 10\n",
      "491 5\n",
      "492 5\n",
      "493 13\n",
      "494 5\n",
      "495 15\n",
      "496 5\n",
      "497 9\n",
      "498 4\n",
      "499 3\n",
      "500 4\n",
      "501 5\n",
      "502 7\n",
      "503 8\n",
      "504 16\n",
      "505 3\n",
      "506 3\n",
      "507 5\n",
      "508 12\n",
      "509 11\n",
      "510 12\n",
      "511 5\n",
      "512 9\n",
      "513 5\n",
      "514 4\n",
      "515 21\n",
      "516 10\n",
      "517 11\n",
      "518 11\n",
      "519 4\n",
      "520 4\n",
      "521 5\n",
      "522 22\n",
      "523 7\n",
      "524 22\n",
      "525 7\n",
      "526 12\n",
      "527 5\n",
      "528 5\n",
      "529 4\n",
      "530 14\n",
      "531 10\n",
      "532 5\n",
      "533 7\n",
      "534 4\n",
      "535 8\n",
      "536 8\n",
      "537 6\n",
      "538 9\n",
      "539 5\n",
      "540 5\n",
      "541 16\n",
      "542 9\n",
      "543 9\n",
      "544 16\n",
      "545 7\n",
      "546 9\n",
      "547 5\n",
      "548 20\n",
      "549 4\n",
      "550 13\n",
      "551 4\n",
      "552 3\n",
      "553 5\n",
      "554 9\n",
      "555 7\n",
      "556 13\n",
      "557 13\n",
      "558 5\n",
      "559 4\n",
      "560 5\n",
      "561 8\n",
      "562 10\n",
      "563 9\n",
      "564 4\n",
      "565 5\n",
      "566 15\n",
      "567 4\n",
      "568 5\n",
      "569 6\n",
      "570 12\n",
      "571 26\n",
      "572 4\n",
      "573 26\n",
      "574 13\n",
      "575 17\n",
      "576 4\n",
      "577 17\n",
      "578 5\n",
      "579 10\n",
      "580 31\n",
      "581 5\n",
      "582 5\n",
      "583 7\n",
      "584 7\n",
      "585 5\n",
      "586 14\n",
      "587 12\n",
      "588 7\n",
      "589 4\n",
      "590 4\n",
      "591 4\n",
      "592 7\n",
      "593 11\n",
      "594 16\n",
      "595 5\n",
      "596 10\n",
      "597 16\n",
      "598 6\n",
      "599 7\n",
      "600 11\n",
      "601 16\n",
      "602 8\n",
      "603 22\n",
      "604 23\n",
      "605 9\n",
      "606 4\n",
      "607 7\n",
      "608 4\n",
      "609 11\n",
      "610 4\n",
      "611 25\n",
      "612 8\n",
      "613 4\n",
      "614 6\n",
      "615 5\n",
      "616 8\n",
      "617 4\n",
      "618 11\n",
      "619 4\n",
      "620 5\n",
      "621 6\n",
      "622 4\n",
      "623 13\n",
      "624 5\n",
      "625 13\n",
      "626 16\n",
      "627 7\n",
      "628 8\n",
      "629 6\n",
      "630 7\n",
      "631 5\n",
      "632 12\n",
      "633 4\n",
      "634 17\n",
      "635 18\n",
      "636 32\n",
      "637 7\n",
      "638 16\n",
      "639 7\n",
      "640 5\n",
      "641 14\n",
      "642 4\n",
      "643 5\n",
      "644 5\n",
      "645 4\n",
      "646 7\n",
      "647 3\n",
      "648 8\n",
      "649 11\n",
      "650 4\n",
      "651 5\n",
      "652 5\n",
      "653 4\n",
      "654 4\n",
      "655 15\n",
      "656 5\n",
      "657 4\n",
      "658 4\n",
      "659 12\n",
      "660 4\n",
      "661 12\n",
      "662 14\n",
      "663 4\n",
      "664 4\n",
      "665 23\n",
      "666 5\n",
      "667 11\n",
      "668 7\n",
      "669 6\n",
      "670 4\n",
      "671 7\n",
      "672 8\n",
      "673 13\n",
      "674 5\n",
      "675 4\n",
      "676 5\n",
      "677 19\n",
      "678 5\n",
      "679 5\n",
      "680 8\n",
      "681 5\n",
      "682 7\n",
      "683 8\n",
      "684 4\n",
      "685 10\n",
      "686 4\n",
      "687 7\n",
      "688 5\n",
      "689 8\n",
      "690 5\n",
      "691 16\n",
      "692 20\n",
      "693 5\n",
      "694 9\n",
      "695 14\n",
      "696 4\n",
      "697 24\n",
      "698 19\n",
      "699 20\n",
      "700 10\n",
      "701 4\n",
      "702 4\n",
      "703 23\n",
      "704 4\n",
      "705 5\n",
      "706 4\n",
      "707 7\n",
      "708 12\n",
      "709 5\n",
      "710 7\n",
      "711 4\n",
      "712 9\n",
      "713 6\n",
      "714 4\n",
      "715 10\n",
      "716 4\n",
      "717 9\n",
      "718 4\n",
      "719 15\n",
      "720 5\n",
      "721 6\n",
      "722 4\n",
      "723 28\n",
      "724 5\n",
      "725 5\n",
      "726 14\n",
      "727 5\n",
      "728 5\n",
      "729 4\n",
      "730 8\n",
      "731 11\n",
      "732 7\n",
      "733 5\n",
      "734 7\n",
      "735 6\n",
      "736 14\n",
      "737 6\n",
      "738 7\n",
      "739 25\n",
      "740 4\n",
      "741 5\n",
      "742 7\n",
      "743 17\n",
      "744 5\n",
      "745 8\n",
      "746 18\n",
      "747 10\n",
      "748 4\n",
      "749 6\n",
      "750 8\n",
      "751 5\n",
      "752 11\n",
      "753 5\n",
      "754 6\n",
      "755 13\n",
      "756 17\n",
      "757 6\n",
      "758 14\n",
      "759 10\n",
      "760 11\n",
      "761 17\n",
      "762 16\n",
      "763 5\n",
      "764 9\n",
      "765 19\n",
      "766 5\n",
      "767 3\n",
      "768 10\n",
      "769 7\n",
      "770 4\n",
      "771 4\n",
      "772 4\n",
      "773 4\n",
      "774 4\n",
      "775 4\n",
      "776 15\n",
      "777 5\n",
      "778 6\n",
      "779 7\n",
      "780 4\n",
      "781 7\n",
      "782 4\n",
      "783 4\n",
      "784 10\n",
      "785 7\n",
      "786 28\n",
      "787 4\n",
      "788 7\n",
      "789 21\n",
      "790 5\n",
      "791 10\n",
      "792 7\n",
      "793 6\n",
      "794 4\n",
      "795 7\n",
      "796 10\n",
      "797 4\n",
      "798 7\n",
      "799 9\n",
      "800 7\n",
      "801 15\n",
      "802 4\n",
      "803 23\n",
      "804 3\n",
      "805 24\n",
      "806 6\n",
      "807 6\n",
      "808 4\n",
      "809 6\n",
      "810 10\n",
      "811 10\n",
      "812 4\n",
      "813 5\n",
      "814 4\n",
      "815 3\n",
      "816 7\n",
      "817 19\n",
      "818 4\n",
      "819 5\n",
      "820 17\n",
      "821 5\n",
      "822 4\n",
      "823 7\n",
      "824 8\n",
      "825 8\n",
      "826 10\n",
      "827 15\n",
      "828 6\n",
      "829 11\n",
      "830 4\n",
      "831 16\n",
      "832 8\n",
      "833 26\n",
      "834 7\n",
      "835 3\n",
      "836 15\n",
      "837 6\n",
      "838 4\n",
      "839 9\n",
      "840 9\n",
      "841 18\n",
      "842 4\n",
      "843 14\n",
      "844 18\n",
      "845 4\n",
      "846 6\n",
      "847 6\n",
      "848 5\n",
      "849 9\n",
      "850 7\n",
      "851 4\n",
      "852 32\n",
      "853 6\n",
      "854 8\n",
      "855 5\n",
      "856 5\n",
      "857 14\n",
      "858 6\n",
      "859 19\n",
      "860 5\n",
      "861 4\n",
      "862 13\n",
      "863 13\n",
      "864 16\n",
      "865 24\n",
      "866 14\n",
      "867 4\n",
      "868 7\n",
      "869 4\n",
      "870 24\n",
      "871 6\n",
      "872 4\n",
      "873 10\n",
      "874 29\n",
      "875 11\n",
      "876 33\n",
      "877 15\n",
      "878 24\n",
      "879 16\n",
      "880 6\n",
      "881 4\n",
      "882 11\n",
      "883 4\n",
      "884 6\n",
      "885 7\n",
      "886 11\n",
      "887 15\n",
      "888 10\n",
      "889 5\n",
      "890 5\n",
      "891 9\n",
      "892 7\n",
      "893 12\n",
      "894 7\n",
      "895 7\n",
      "896 13\n",
      "897 7\n",
      "898 9\n",
      "899 6\n",
      "900 19\n",
      "901 18\n",
      "902 12\n",
      "903 8\n",
      "904 14\n",
      "905 4\n",
      "906 16\n",
      "907 26\n",
      "908 4\n",
      "909 7\n",
      "910 7\n",
      "911 4\n",
      "912 16\n",
      "913 18\n",
      "914 7\n",
      "915 20\n",
      "916 8\n",
      "917 5\n",
      "918 21\n",
      "919 37\n",
      "920 4\n",
      "921 4\n",
      "922 3\n",
      "923 5\n",
      "924 11\n",
      "925 13\n",
      "926 5\n",
      "927 6\n",
      "928 4\n",
      "929 5\n",
      "930 5\n",
      "931 18\n",
      "932 8\n",
      "933 3\n",
      "934 11\n",
      "935 4\n",
      "936 15\n",
      "937 5\n",
      "938 4\n",
      "939 7\n",
      "940 4\n",
      "941 4\n",
      "942 5\n",
      "943 7\n",
      "944 6\n",
      "945 4\n",
      "946 13\n",
      "947 9\n",
      "948 4\n",
      "949 9\n",
      "950 10\n",
      "951 5\n",
      "952 10\n",
      "953 5\n",
      "954 3\n",
      "955 10\n",
      "956 4\n",
      "957 8\n",
      "958 23\n",
      "959 5\n",
      "960 5\n",
      "961 19\n",
      "962 5\n",
      "963 6\n",
      "964 7\n",
      "965 10\n",
      "966 13\n",
      "967 14\n",
      "968 5\n",
      "969 8\n",
      "970 7\n",
      "971 10\n",
      "972 3\n",
      "973 5\n",
      "974 26\n",
      "975 4\n",
      "976 4\n",
      "977 22\n",
      "978 4\n",
      "979 14\n",
      "980 3\n",
      "981 7\n",
      "982 4\n",
      "983 9\n",
      "984 4\n",
      "985 8\n",
      "986 4\n",
      "987 5\n",
      "988 32\n",
      "989 28\n",
      "990 6\n",
      "991 10\n",
      "992 11\n",
      "993 5\n",
      "994 6\n",
      "995 4\n",
      "996 8\n",
      "997 5\n",
      "998 6\n",
      "999 14\n",
      "1000 18\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "all_atts = []\n",
    "all_main_probs = []\n",
    "all_index_probs = []\n",
    "prob_fn = task.get_probs_fn()\n",
    "count = 0\n",
    "for x, y in iter(task.test_dataset):\n",
    "    max_len = x.shape[1]\n",
    "    main_logits, extra = model_1.detailed_call(x, training=False)\n",
    "    outputs, last_state, presents, attentions, embeddings = extra\n",
    "    _attentions = [att.numpy() for att in attentions]\n",
    "    attentions = np.transpose(np.asarray(_attentions), (1,0,2,3,4))\n",
    "    main_probs = prob_fn(main_logits, y, 1)\n",
    "    batch_indexes = tf.range(len(y), dtype=tf.int64)\n",
    "    indexes = tf.concat([batch_indexes[:,None], y[:,None]], axis=1)\n",
    "    correct_main_probs = tf.gather_nd(main_probs, indexes).numpy()\n",
    "\n",
    "    sentences.append(task.databuilder.sentence_encoder().decode(x[0]))\n",
    "    all_atts.extend(attentions)\n",
    "    all_main_probs.extend(correct_main_probs)\n",
    "    all_index_probs.append([])\n",
    "    print(count, max_len)\n",
    "    # This loop can be optimized so that there is only one call...\n",
    "    new_xz = []\n",
    "    for i in np.arange(1,max_len-1):\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        unktoken = task.databuilder.sentence_encoder().encode(constants.unk)\n",
    "        unk = tf.reshape(tf.convert_to_tensor(unktoken, dtype=tf.int64)[None], (-1,1))\n",
    "        unks = tf.tile(unk, (batch_size, 1))\n",
    "        new_x = tf.concat([x[:,:i],unks, x[:,i+1:]], axis=-1)\n",
    "        new_xz.extend(new_x)\n",
    "    \n",
    "    new_x = np.asarray(new_xz)\n",
    "    logits = model_1(new_x, training=False)\n",
    "    probs = prob_fn(logits, y, 1)\n",
    "    \n",
    "    batch_indexes = tf.range(len(probs), dtype=tf.int64)\n",
    "    yz = tf.tile(y, (len(probs),))\n",
    "\n",
    "    indexes = tf.concat([batch_indexes[:,None], yz[:,None]], axis=1)\n",
    "    \n",
    "    correct_probs = tf.gather_nd(probs, indexes).numpy()\n",
    "    all_index_probs[-1].extend(abs(correct_main_probs - correct_probs))\n",
    "    count += 1\n",
    "    if count > 1000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1001 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1001/1001 [1:09:24<00:00,  4.16s/it] \n",
      "  0%|          | 1/1001 [00:00<03:04,  5.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f: 0.1755475728467852 0.13963136005850138\n",
      "Layer:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1001/1001 [1:14:02<00:00,  4.44s/it] \n",
      "  0%|          | 0/1001 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f: 0.2666103637693633 0.13174750965844362\n",
      "Layer:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1001/1001 [54:26<00:00,  3.26s/it] \n",
      "  0%|          | 1/1001 [00:00<02:24,  6.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f: 0.48213614419538353 0.14476770698529756\n",
      "Layer:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1001/1001 [52:59<00:00,  3.18s/it] \n",
      "  0%|          | 1/1001 [00:00<02:23,  6.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f: 0.5683504092257706 0.17421387258166213\n",
      "Layer:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1001/1001 [55:11<00:00,  3.31s/it] \n",
      "  0%|          | 1/1001 [00:00<02:20,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f: 0.562664291253102 0.1770877346129305\n",
      "Layer:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1001/1001 [56:13<00:00,  3.37s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f: 0.5679205284029363 0.17561799076753754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def spearmanr(x, y):\n",
    "    \"\"\" `x`, `y` --> pd.Series\"\"\"\n",
    "    x = pd.Series(x)\n",
    "    y = pd.Series(y)\n",
    "    assert x.shape == y.shape\n",
    "    rx = x.rank(method='dense')\n",
    "    ry = y.rank(method='dense')\n",
    "    d = rx - ry\n",
    "    dsq = np.sum(np.square(d))\n",
    "    n = x.shape[0]\n",
    "    coef = 1. - (6. * dsq) / (n * (n**2 - 1.))\n",
    "    return coef\n",
    "\n",
    "def get_raw_att_relevance(full_att_mat, input_tokens, layer=-1):\n",
    "    cls_index = 0\n",
    "    return full_att_mat[layer].sum(axis=0)[cls_index]\n",
    "    \n",
    "\n",
    "def compute_node_flow(G, labels_to_index, input_nodes, output_nodes,length):\n",
    "    number_of_nodes = len(labels_to_index)\n",
    "    flow_values=np.zeros((number_of_nodes,number_of_nodes))\n",
    "    for key in output_nodes:\n",
    "        if key not in input_nodes:\n",
    "            current_layer = int(labels_to_index[key] / length)\n",
    "            pre_layer = current_layer - 1\n",
    "            u = labels_to_index[key]\n",
    "            for inp_node_key in input_nodes:\n",
    "                v = labels_to_index[inp_node_key]\n",
    "                flow_value = nx.maximum_flow_value(G,u,v)\n",
    "                flow_values[u][pre_layer*length+v ] = flow_value\n",
    "            flow_values[u] /= flow_values[u].sum()\n",
    "            \n",
    "    return flow_values\n",
    "\n",
    "def get_flow_relevance(full_att_mat, input_tokens, layer):\n",
    "    \n",
    "    res_att_mat = full_att_mat.sum(axis=1)/8\n",
    "    res_att_mat = res_att_mat + np.eye(res_att_mat.shape[1])[None,...]\n",
    "    res_att_mat = res_att_mat / res_att_mat.sum(axis=-1)[...,None]\n",
    "\n",
    "    res_adj_mat, res_labels_to_index = get_adjmat(mat=res_att_mat, input_tokens=input_tokens)\n",
    "    \n",
    "    A = res_adj_mat\n",
    "    res_G=nx.from_numpy_matrix(A, create_using=nx.DiGraph())\n",
    "    for i in np.arange(A.shape[0]):\n",
    "        for j in np.arange(A.shape[1]):\n",
    "            nx.set_edge_attributes(res_G, {(i,j): A[i,j]}, 'capacity')\n",
    "\n",
    "\n",
    "    output_nodes = []\n",
    "    input_nodes = []\n",
    "    for key in res_labels_to_index:\n",
    "        if 'L'+str(layer+1) in key:\n",
    "            output_nodes.append(key)\n",
    "        if res_labels_to_index[key] < full_att_mat.shape[-1]:\n",
    "            input_nodes.append(key)\n",
    "    \n",
    "    flow_values = compute_node_flow(res_G, res_labels_to_index, input_nodes, output_nodes, length=full_att_mat.shape[-1])\n",
    "    \n",
    "    n_layers = full_att_mat.shape[0]\n",
    "    length = full_att_mat.shape[-1]\n",
    "    final_layer_attention_raw = flow_values[(layer+1)*length:,layer*length:(layer+1)*length]\n",
    "    cls_index = 0\n",
    "    relevance_attention_raw = final_layer_attention_raw[cls_index]\n",
    "\n",
    "    return relevance_attention_raw\n",
    "    \n",
    "    \n",
    "def get_joint_relevance(full_att_mat, input_tokens, layer):\n",
    "    att_sum_heads =  full_att_mat.sum(axis=1)/8\n",
    "    joint_attentions = compute_joint_attention(att_sum_heads, add_residual=True)\n",
    "    relevance_attentions = joint_attentions[layer][0]\n",
    "    return relevance_attentions\n",
    "\n",
    "from tqdm import tqdm\n",
    "for layer in np.arange(6):\n",
    "    print(\"Layer: \", layer) \n",
    "#     r_corels, r_pvals = [], []\n",
    "#     for i in np.arange(len(all_index_probs)):\n",
    "#         aa = [0, 0]+all_index_probs[i]+[0]\n",
    "#         bb = get_raw_att_relevance(all_atts[i], ['<cls>']+sentences[i].split(), layer=layer)\n",
    "#         corel = spearmanr(aa,bb)\n",
    "#         r_corels.append(corel) \n",
    "\n",
    "#     print(\"r:\", np.mean(r_corels), np.std(r_corels))\n",
    "\n",
    "#     j_corels, j_pvals = [], []\n",
    "#     for i in np.arange(len(all_index_probs)):\n",
    "#         aa = [0, 0]+all_index_probs[i]+[0]\n",
    "#         bb = get_joint_relevance(all_atts[i], ['<cls>']+sentences[i].split(),layer=layer)\n",
    "#         corel = spearmanr(aa,bb)\n",
    "#         j_corels.append(corel) \n",
    "#     print(\"j:\", np.mean(j_corels), np.std(j_corels))\n",
    "\n",
    "    f_corels, f_pvals = [], []\n",
    "    for i in tqdm(np.arange(len(all_index_probs))):\n",
    "        aa = [0, 0]+all_index_probs[i]+[0]\n",
    "        bb = get_flow_relevance(all_atts[i], ['<cls>']+sentences[i].split(),layer=layer)\n",
    "        corel = spearmanr(aa,bb)\n",
    "        f_corels.append(corel) \n",
    "    print(\"f:\", np.mean(f_corels), np.std(f_corels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_index_probs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-ae34944d4d32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'all_probs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_index_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'all_index_probs' is not defined"
     ]
    }
   ],
   "source": [
    "np.save('all_probs', all_index_probs, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_index_probs = np.load('all_probs.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-bd26a82e0a2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#pickle.dump(all_atts, open('all_atts', 'wb'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'all_atts'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mEOFError\u001b[0m: Ran out of input"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "#pickle.dump(all_atts, open('all_atts', 'wb'))\n",
    "pickle.load(open('all_atts', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "r_corels, r_pvals = [], []\n",
    "for i in np.arange(len(all_index_probs)):\n",
    "    aa = [0, 0]+all_index_probs[i]+[0]\n",
    "    bb = get_raw_att_relevance(all_atts[i], ['<cls>']+sentences[i].split())\n",
    "    corel = spearmanr(aa,bb)\n",
    "    r_corels.append(corel) \n",
    "\n",
    "j_corels, j_pvals = [], []\n",
    "for i in np.arange(len(all_index_probs)):\n",
    "    aa = [0, 0]+all_index_probs[i]+[0]\n",
    "    bb = get_joint_relevance(all_atts[i], ['<cls>']+sentences[i].split())\n",
    "    corel = spearmanr(aa,bb)\n",
    "    j_corels.append(corel) \n",
    "\n",
    "f_corels, f_pvals = [], []\n",
    "for i in np.arange(len(all_index_probs)):\n",
    "    aa = [0, 0]+all_index_probs[i]+[0]\n",
    "    bb = get_flow_relevance(all_atts[i], ['<cls>']+sentences[i].split())\n",
    "    corel = spearmanr(aa,bb)\n",
    "    f_corels.append(corel) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_corels, f_pvals = [], []\n",
    "for i in np.arange(len(all_index_probs)):\n",
    "    aa = [0, 0]+all_index_probs[i]+[0]\n",
    "    bb = get_flow_relevance(all_atts[i], ['<cls>']+sentences[i].split())\n",
    "    corel = spearmanr(aa,bb)\n",
    "    f_corels.append(corel) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_corels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray(f_corels) > np.asarray(j_corels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray(f_corels) > np.asarray(r_corels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray(f_corels) > np.asarray(j_corels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_corels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_corels, r_pvals = [], []\n",
    "for i in np.arange(len(all_index_probs)):\n",
    "    aa = [0, 0]+all_index_probs[i]+[0]\n",
    "    bb = get_raw_att_relevance(all_atts[i], ['<cls>']+sentences[i].split())\n",
    "    corel, p_value = spearmanr(aa,bb)\n",
    "    r_corels.append(corel) \n",
    "    r_pvals.append(p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3]\n",
    "np.mean(a)\n",
    "np.std(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "f_corels, f_pvals = [], []\n",
    "for i in np.arange(len(all_index_probs)):\n",
    "    aa = [0, 0]+all_index_probs[i]+[0]\n",
    "    bb = get_flow_relevance(all_atts[i], ['<cls>']+sentences[i].split())\n",
    "    corel = spearmanr(aa,bb)\n",
    "    f_corels.append(corel) \n",
    "print(\"f:\", np.mean(f_corels), np.std(f_corels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
