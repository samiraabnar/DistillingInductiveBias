{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from util import constants\n",
    "from util.config_util import get_model_params, get_task_params, get_train_params\n",
    "from tf2_models.trainer import Trainer\n",
    "from absl import app\n",
    "from absl import flags\n",
    "import numpy as np\n",
    "from util.models import MODELS\n",
    "from util.tasks import TASKS\n",
    "\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reps(inputs, model):\n",
    "    logits, extra = model.detailed_call(inputs)\n",
    "\n",
    "def pairwisedot_product_sim(reps1, reps2):\n",
    "    reps1 = tf.reshape(reps1, (-1, tf.shape(reps1)[-1]))\n",
    "    reps2 = tf.reshape(reps2, (-1, tf.shape(reps2)[-1]))\n",
    "    \n",
    "    pw_dot_product = tf.matmul(reps1, reps2.T)\n",
    "    \n",
    "    return pw_dot_product\n",
    "\n",
    "def dot_product_sim(reps1, reps2):\n",
    "    # Elementwise multiplication\n",
    "    dot_product = tf.multiply(reps1, reps2)\n",
    "    # Sum over last axis to get the dot product similarity between corresponding pairs\n",
    "    dot_product = tf.reduce_sum(dot_product, axis=-1)\n",
    "    \n",
    "    return dot_product\n",
    "\n",
    "def normalized_dot_product_sim(reps1, reps2):\n",
    "    #normalize reps:\n",
    "    reps1 = reps1/tf.norm(reps1)\n",
    "    reps2 = reps2/tf.norm(reps2)\n",
    "    \n",
    "    # Elementwise multiplication\n",
    "    dot_product = tf.multiply(reps1, reps2)\n",
    "    # Sum over last axis to get the dot product similarity between corresponding pairs\n",
    "    dot_product = tf.reduce_sum(dot_product, axis=-1)\n",
    "    \n",
    "    return dot_product\n",
    "    \n",
    "    \n",
    "def second_order_rep_sim(reps1, reps2):\n",
    "    sims1 = pairwisedot_product_sim(reps1, reps1)\n",
    "    sims2 = pairwisedot_product_sim(reps2, reps2)\n",
    "    \n",
    "    so_sims = normalized_dot_product_sim(sims1, sims2)\n",
    "    \n",
    "    return np.mean(so_sims), so_sims\n",
    "\n",
    "def compare_models(inputs, model1, model2):\n",
    "    reps1 = get_reps(inputs, model1)\n",
    "    reps2 = get_reps(inputs, model2)\n",
    "    \n",
    "    similarity_measures = second_order_rep_sim(reps1, reps2)\n",
    "    \n",
    "    return similarity_measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab len:  10034\n",
      "model config: very_big_gpt_v10\n",
      "{'embedding_dim': 512, 'resid_pdrop': 0.4, 'embd_pdrop': 0.2, 'attn_pdrop': 0.6, 'initializer_range': 0.05}\n",
      "Restored model from ../tf_ckpts/word_sv_agreement_lm/lm_gpt2_h-512_d-6_rdrop-0.4_adrop-0.6_indrop-0.2_very_big_gpt_v10_0.0001_lisa_offlineteacher_v1/ckpt-60\n"
     ]
    }
   ],
   "source": [
    "model_name='lm_gpt2'\n",
    "model_config='very_big_gpt_v10'\n",
    "learning_rate=0.0001\n",
    "exp_name='lisa_offlineteacher_v1'\n",
    "task_name = 'word_sv_agreement_lm'\n",
    "chkpt_dir = '../tf_ckpts'\n",
    "\n",
    "task = TASKS[task_name](get_task_params(), data_dir='../data')\n",
    "\n",
    "cl_token = task.databuilder.sentence_encoder().encode(constants.bos)\n",
    "hparams=get_model_params(task, model_name, model_config)\n",
    "hparams.output_attentions = True\n",
    "hparams.output_embeddings = True\n",
    "\n",
    "model = MODELS[model_name](hparams=hparams, cl_token=cl_token)\n",
    "\n",
    "\n",
    "ckpt_dir = os.path.join(chkpt_dir,task.name,\n",
    "                        model.model_name+\"_\"+str(model_config)+\"_\"+str(learning_rate)+\"_\"+exp_name)\n",
    "\n",
    "ckpt = tf.train.Checkpoint(net=model)\n",
    "manager = tf.train.CheckpointManager(ckpt, ckpt_dir, max_to_keep=None)\n",
    "\n",
    "ckpt.restore(manager.latest_checkpoint)\n",
    "if manager.latest_checkpoint:\n",
    "  print(\"Restored model from {}\".format(manager.latest_checkpoint))\n",
    "else:\n",
    "  print(\"Initialized from scratch!\")\n",
    "  print(ckpt_dir)\n",
    "\n",
    "model.compile(loss=task.get_loss_fn(), metrics=task.metrics())\n",
    "#model.evaluate(task.train_dataset, steps=task.n_train_batches)\n",
    "model.evaluate(task.valid_dataset, steps=task.n_valid_batches)\n",
    "#model.evaluate(task.train_dataset, steps=task.n_train_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
