{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/dehghani/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.io as spio\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "import glob\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from util import constants\n",
    "from util.config_util import get_model_params, get_task_params, get_train_params\n",
    "from tf2_models.trainer import Trainer\n",
    "from absl import app\n",
    "from absl import flags\n",
    "import numpy as np\n",
    "from util.models import MODELS\n",
    "from util.tasks import TASKS\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow_datasets as tfds\n",
    "from tfds_data.aff_nist import AffNist\n",
    "from calibration_util import *\n",
    "from notebook_utils import *\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns; sns.set()\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def visualization(x, y,count,index):\n",
    "    x = np.reshape(x, (40, 40))\n",
    "    \n",
    "    plt.subplot(1,count,index)\n",
    "    plt.imshow(x, cmap=cm.Greys_r)\n",
    "    plt.title(y)\n",
    "    plt.axis('off')   \n",
    "\n",
    "def visualization_overlap(x0,x1, y0,y1,count,index):\n",
    "    r = np.reshape(x0, (40, 40))\n",
    "    g = np.reshape(x1, (40, 40))\n",
    "    b = np.zeros_like(r)\n",
    "    rgb = np.stack([r,g,b],-1)\n",
    "    \n",
    "    plt.subplot(1,count,index)\n",
    "    plt.imshow(rgb)\n",
    "    plt.title('R:('+ str(y0)+','+str(y1)+')')\n",
    "    plt.axis('off')   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "chkpt_dir='../tf_ckpts'\n",
    "task_name = 'mnist'\n",
    "mnist = TASKS[task_name](get_task_params(), data_dir='../data')\n",
    "\n",
    "chkpt_dir='../tf_ckpts'\n",
    "task_name = 'affnist'\n",
    "affnist = TASKS[task_name](get_task_params(), data_dir='../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "468.75"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_examples(examples):\n",
    "    pad_length = int((28 - 28) / 2)\n",
    "    return tf.pad(tf.cast(examples['image'], dtype=tf.float32) / 255,\n",
    "                  ([pad_length, pad_length], [pad_length, pad_length],\n",
    "                   [0, 0])), tf.cast(\n",
    "      examples['label'], dtype=tf.int32)\n",
    "\n",
    "cmnist_trans = tfds.builder('mnist_corrupted/translate')\n",
    "cmnist_trans_test = cmnist_trans.as_dataset(split='test')\n",
    "cmnist_trans_test = cmnist_trans_test.map(map_func=lambda x: convert_examples(x))\n",
    "cmnist_trans_test = cmnist_trans_test.batch(128)\n",
    "cmnist_trans.info.splits['test'].num_examples\n",
    "\n",
    "\n",
    "cmnist_trans_train = cmnist_trans.as_dataset(split='train')\n",
    "cmnist_trans_train = cmnist_trans_train.map(map_func=lambda x: convert_examples(x))\n",
    "cmnist_trans_train = cmnist_trans_train.batch(128)\n",
    "cmnist_trans_train = cmnist_trans_train.repeat(100)\n",
    "\n",
    "cmnist_trans.info.splits['train'].num_examples / 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmnist_scale = tfds.builder('mnist_corrupted/scale')\n",
    "cmnist_scale_test = cmnist_scale.as_dataset(split='test')\n",
    "cmnist_scale_test = cmnist_scale_test.map(map_func=lambda x: convert_examples(x))\n",
    "cmnist_scale_test = cmnist_scale_test.batch(128)\n",
    "cmnist_scale.info.splits['test'].num_examples\n",
    "\n",
    "cmnist_scale_train = cmnist_scale.as_dataset(split='train')\n",
    "cmnist_scale_train = cmnist_scale_train.map(map_func=lambda x: convert_examples(x))\n",
    "cmnist_scale_train = cmnist_scale_train.batch(128)\n",
    "cmnist_scale_train = cmnist_scale_train.repeat(100)\n",
    "\n",
    "cmnist_scale.info.splits['train'].num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 10\n",
    "for x,y in affnist.test_dataset:\n",
    "    print(x.shape)\n",
    "    #visualization(x[1].numpy(),y[1].numpy(),1,1)\n",
    "    plt.imshow(x[0,...,0])\n",
    "    plt.show()\n",
    "    count -= 1\n",
    "    if count <= 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 10\n",
    "for x,y in mnist.valid_dataset:\n",
    "    print(x.shape)\n",
    "    #visualization(x[1].numpy(),y[1].numpy(),1,1)\n",
    "    plt.imshow(x[0,...,0])\n",
    "    plt.show()\n",
    "    count -= 1\n",
    "    if count <= 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model config: ff_mnist4\n",
      "<util.model_configs.ModelConfig object at 0x7f4264363790> {'embedding_dim': 512, 'hidden_dim': [512, 128, 32], 'input_dim': 784, 'output_dim': 10, 'depth': 3, 'proj_depth': 1, 'fc_dim': [], 'hidden_dropout_rate': 0.1, 'input_dropout_rate': 0.0, 'initializer_range': None, 'kernel_size': [(3, 3)], 'filters': [32], 'maxout_size': [32], 'pool_size': [(2, 2)], 'output_hidden_states': False, 'output_embeddings': False, 'routings': 3}\n"
     ]
    }
   ],
   "source": [
    "config={'exp_name':'test',\n",
    "    'model_config':'ff_mnist4',\n",
    "    'task_name':'mnist',\n",
    "    'model_name':'cl_vff',\n",
    "    'chkpt_dir':'../tf_ckpts',\n",
    "    'learning_rate': 0.001\n",
    "    }\n",
    "\n",
    "task = TASKS[config['task_name']](get_task_params(batch_size=128), data_dir='../data')\n",
    "\n",
    "hparams = get_model_params(task, config['model_name'], config['model_config'])\n",
    "print(hparams, hparams.__dict__)\n",
    "\n",
    "#model, _ = get_model(config, task, hparams, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaFF(tf.keras.models.Sequential):\n",
    "  def __init__(self, hparams, scope=\"cl_vff\", *inputs, **kwargs):\n",
    "    if 'cl_token' in kwargs:\n",
    "      del kwargs['cl_token']\n",
    "\n",
    "    super(VanillaFF, self).__init__()\n",
    "    self.scope = scope\n",
    "    self.hparams = hparams\n",
    "\n",
    "    self.model_name = '_'.join([self.scope,\n",
    "                                'h-' + '.'.join([str(x) for x in self.hparams.hidden_dim]),\n",
    "                                'd-' + str(self.hparams.depth),\n",
    "                                'hdrop-' + str(self.hparams.hidden_dropout_rate),\n",
    "                                'indrop-' + str(self.hparams.input_dropout_rate)])\n",
    "\n",
    "    self.regularizer = tf.keras.regularizers.l1_l2(l1=0.00001,\n",
    "                                                   l2=0.00001)\n",
    "    self.create_vars()\n",
    "    self.rep_index = 1\n",
    "    self.rep_layer = -1\n",
    "\n",
    "\n",
    "\n",
    "  def create_vars(self):\n",
    "    self.flat = tf.keras.layers.Flatten()\n",
    "    # self.batch_norm = tf.keras.layers.BatchNormalization()\n",
    "    # self.batch_norm.trainable = True\n",
    "    self.indrop = tf.keras.layers.Dropout(self.hparams.input_dropout_rate)\n",
    "    self.activation = tf.keras.layers.Activation('relu')\n",
    "\n",
    "    self.hidden_layers = []\n",
    "    self.hidden_batch_norms = []\n",
    "    self.hidden_dropouts = []\n",
    "    for i in np.arange(self.hparams.depth):\n",
    "      self.hidden_layers.append(tf.keras.layers.Dense(self.hparams.hidden_dim[i],\n",
    "                                     activation=None, #'relu',\n",
    "                                     kernel_regularizer=self.regularizer))\n",
    "      #self.hidden_batch_norms.append(tf.keras.layers.BatchNormalization())\n",
    "      #self.hidden_batch_norms[i].trainable = True\n",
    "      self.hidden_dropouts.append(tf.keras.layers.Dropout(self.hparams.hidden_dropout_rate))\n",
    "\n",
    "    self.final_dense = tf.keras.layers.Dense(self.hparams.output_dim,\n",
    "                                   kernel_regularizer=self.regularizer)\n",
    "\n",
    "\n",
    "  def call(self, inputs, padding_symbol=None, training=None, **kwargs):\n",
    "    x = self.flat(inputs, **kwargs)\n",
    "    # x = self.batch_norm(x, training=training, **kwargs)\n",
    "    x = self.indrop(x, training=training, **kwargs)\n",
    "\n",
    "    for i in np.arange(self.hparams.depth):\n",
    "      x = self.hidden_layers[i](x, training=training, **kwargs)\n",
    "      x = self.activation(x)\n",
    "      #x = self.hidden_batch_norms[i](x, training=training, **kwargs)\n",
    "      x = self.hidden_dropouts[i](x, training=training, **kwargs)\n",
    "\n",
    "    logits = self.final_dense(x, training=training, **kwargs)\n",
    "\n",
    "    return logits\n",
    "\n",
    "\n",
    "  def detailed_call(self, inputs, padding_symbol=None, training=None, **kwargs):\n",
    "    layer_activations = []\n",
    "    x = self.flat(inputs, **kwargs)\n",
    "    x = self.indrop(x, training=None, **kwargs)\n",
    "    layer_activations.append(x)\n",
    "\n",
    "    for i in np.arange(self.hparams.depth):\n",
    "      x = self.hidden_layers[i](x, training=training, **kwargs)\n",
    "      x = self.activation(x)\n",
    "      x = self.hidden_batch_norms[i](x, training=training, **kwargs)\n",
    "      x = self.hidden_dropouts[i](x, training=training, **kwargs)\n",
    "      layer_activations.append(x)\n",
    "\n",
    "    pnltimt = x\n",
    "    logits = self.final_dense(x, training=None, **kwargs)\n",
    "\n",
    "    return logits, pnltimt, layer_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 28, 28, 1) (128,)\n",
      "(128, 10)\n",
      "Model: \"vanilla_ff\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "activation (Activation)      multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  401920    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  65664     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  4128      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              multiple                  330       \n",
      "=================================================================\n",
      "Total params: 472,042\n",
      "Trainable params: 472,042\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# hparams.depth=4\n",
    "# hparams.hidden_dims=[512, 512, 64, 32]\n",
    "# hparams.hidden_dropout_rate = 0.2\n",
    "hparams.fc_dim = []\n",
    "hparams.depth = 3\n",
    "hparams.hidden_dims = [128, 64, 64]\n",
    "hparams.hidden_dropout_rate =  0.3\n",
    "hparams.input_dropout_rate = 0.0\n",
    "model = VanillaFF(hparams)\n",
    "for x,y in task.train_dataset:\n",
    "    print(x.shape, y.shape)\n",
    "    break\n",
    "out = model(inputs=x)\n",
    "print(out.shape)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss=task.get_loss_fn(),\n",
    "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Unrecognized keyword arguments: {'test_steps': 468, 'test_data': <DatasetV1Adapter shapes: ((None, 28, 28, 1), (None,)), types: (tf.float32, tf.int32)>}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-cd65c5c455bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m                   \u001b[0mtest_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmnist_trans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_examples\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                   \u001b[0mtest_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcmnist_trans_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                   \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m                   )\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/reflect/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    793\u001b[0m       \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nb_epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 795\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unrecognized keyword arguments: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    796\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_compile_was_called\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_call_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fit'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Unrecognized keyword arguments: {'test_steps': 468, 'test_data': <DatasetV1Adapter shapes: ((None, 28, 28, 1), (None,)), types: (tf.float32, tf.int32)>}"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "              loss=task.get_loss_fn(),\n",
    "              metrics=task.metrics())\n",
    "\n",
    "history = model.fit(task.train_dataset,\n",
    "                  epochs=100,\n",
    "                  steps_per_epoch=task.n_train_batches,\n",
    "                  validation_data=task.test_dataset,\n",
    "                  validation_steps=task.n_test_batches,\n",
    "                  verbose=2\n",
    "                  )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(task.test_dataset, steps=task.n_test_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(cmnist_trans_test, steps=cmnist_trans.info.splits['test'].num_examples/128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(cmnist_scale_test, steps=cmnist_scale.info.splits['test'].num_examples/128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 468 steps\n",
      "468/468 [==============================] - 2s 5ms/step - loss: 0.7374 - classification_loss: 0.5918 - sparse_categorical_accuracy: 0.8276\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.4187 - classification_loss: 0.2759 - sparse_categorical_accuracy: 0.9245\n",
      "79/78 [==============================] - 0s 4ms/step - loss: 3.8620 - classification_loss: 3.7187 - sparse_categorical_accuracy: 0.1197\n",
      "79/78 [==============================] - 0s 3ms/step - loss: 1.8512 - classification_loss: 1.7082 - sparse_categorical_accuracy: 0.3928\n",
      "mnist: [0.4186604210199454, 0.27587843, 0.9244792]\n",
      "trans: [3.9046855812072754, 3.7186553, 0.1197]\n",
      "scale: [1.8717201343536376, 1.7082068, 0.3928]\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(task.train_dataset,\n",
    "                  epochs=1,\n",
    "                  steps_per_epoch=task.n_train_batches)\n",
    "mnist = model.evaluate(task.test_dataset, steps=task.n_test_batches)\n",
    "trans = model.evaluate(cmnist_trans_test, steps=cmnist_trans.info.splits['test'].num_examples/128)\n",
    "scale = model.evaluate(cmnist_scale_test, steps=cmnist_scale.info.splits['test'].num_examples/128)\n",
    "\n",
    "print(\"mnist:\", mnist)\n",
    "print(\"trans:\", trans)\n",
    "print(\"scale:\", scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfds.load('mnist_corrupted/scale', split='test')\n",
    "tfds.load('mnist_corrupted/translate', split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
