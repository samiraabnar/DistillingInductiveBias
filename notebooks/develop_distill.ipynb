{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.config_util import get_task_params, get_model_params\n",
    "import os\n",
    "from tasks.sv_agreement import SvAgreementLM, WordSvAgreementLM\n",
    "from tf2_models.lm_transformer import LmGPT2\n",
    "from util.config_util import get_model_params, get_task_params, get_train_params\n",
    "from tf2_models.lm_lstm import LmLSTM\n",
    "from absl import flags\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm \n",
    "\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_string('task', 'word_sv_agreement_lm', 'sv_agreement_lm | word_sv_agreement_lm')\n",
    "\n",
    "\n",
    "flags.DEFINE_string('teacher_exp_name', 'trial4', 'experiment directory')\n",
    "flags.DEFINE_string('teacher_model', 'lm_lstm', 'lm_lstm | lm_gpt2')\n",
    "\n",
    "flags.DEFINE_string('student_exp_name', 'trial1', 'experiment directory')\n",
    "flags.DEFINE_string('student_model', 'lm_lstm', 'lm_lstm | lm_gpt2')\n",
    "\n",
    "flags.DEFINE_string('f', None,'kernel')\n",
    "\n",
    "FLAGS(sys.argv)\n",
    "\n",
    "hparams = flags.FLAGS\n",
    "\n",
    "\n",
    "MODELS = {\"lm_lstm\": LmLSTM,\n",
    "          \"lm_gpt2\": LmGPT2}\n",
    "\n",
    "TASKS = {\n",
    "  'sv_agreement_lm': SvAgreementLM,\n",
    "  'word_sv_agreement_lm': WordSvAgreementLM,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistillLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, padding_symbol=0, tmp=1.0,\n",
    "                 **kwargs):\n",
    "        super(DistillLoss, self).__init__(**kwargs)\n",
    "        self.tmp = tf.constant(tmp, dtype=tf.float32)\n",
    "        self.padding_symbol = tf.constant(padding_symbol, dtype=tf.int32)\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "      y_true = tf.cast(tf.squeeze(y_true), dtype=tf.float32)\n",
    "      sequence_mask = tf.cast(y_true[...,self.padding_symbol] != 1.0, dtype=tf.float32)\n",
    "      sequence_mask = sequence_mask / tf.reduce_sum(sequence_mask)\n",
    "      return tf.reduce_sum(tf.compat.v2.nn.softmax_cross_entropy_with_logits(logits=y_pred/self.tmp,\n",
    "                                                                      labels=y_true,\n",
    "                                                                      name='loss') * sequence_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab len:  10034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.\n"
     ]
    }
   ],
   "source": [
    "log_dir = \"../logs\"\n",
    "chkpt_dir = \"../tf_ckpts\"\n",
    "\n",
    "# Create task\n",
    "task = TASKS[hparams.task](get_task_params(), data_dir='../data')\n",
    "\n",
    "# Create the Model\n",
    "teacher_model = MODELS[hparams.teacher_model](hparams=get_model_params(task, hparams.teacher_model))\n",
    "student_model = MODELS[hparams.student_model](hparams=get_model_params(task, hparams.student_model))\n",
    "\n",
    "teacher_log_dir = os.path.join(log_dir, task.name, teacher_model.model_name + \"_\" + hparams.teacher_exp_name)\n",
    "teacher_ckpt_dir = os.path.join(chkpt_dir, task.name, teacher_model.model_name + \"_\" + hparams.teacher_exp_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_model.compile(\n",
    "                      optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "                      loss=DistillLoss(tmp=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../tf_ckpts/word_sv_agreement_lm/lm_lstm_h-512_d-3_hdrop-0.5_indrop-0.2_trial4\n",
      "Restored from ../tf_ckpts/word_sv_agreement_lm/lm_lstm_h-512_d-3_hdrop-0.5_indrop-0.2_trial4/ckpt-18\n"
     ]
    }
   ],
   "source": [
    "print(teacher_ckpt_dir)\n",
    "teacher_ckpt = tf.train.Checkpoint(net=teacher_model)\n",
    "teacher_manager = tf.train.CheckpointManager(teacher_ckpt, teacher_ckpt_dir, max_to_keep=2)\n",
    "teacher_ckpt.restore(teacher_manager.latest_checkpoint)\n",
    "if teacher_manager.latest_checkpoint:\n",
    "  print(\"Restored from {}\".format(teacher_manager.latest_checkpoint))\n",
    "else:\n",
    "  print(\"Initializing from scratch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_model.optimizer\n",
    "student_model.trainable_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply teacher and student\n",
    "train_iter = iter(task.valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at step 0: 5.463839530944824\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 0: 5.45330810546875\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 0: 5.35497522354126\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 0: 5.404634475708008\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 0: 5.398497581481934\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 0: 5.464436054229736\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 0: 5.433465957641602\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 0: 5.383969306945801\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 0: 5.3649983406066895\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 0: 5.466148853302002\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 0: 5.416903495788574\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 0: 5.425065517425537\n",
      "Seen so far: 64 samples\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-8ae6f743597a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mmasked_teacher_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_probs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mteacher_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmasked_teacher_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0;31m# Log every 200 batches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/indist/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/indist/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    492\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m~/anaconda3/envs/indist/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/indist/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/indist/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/indist/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/envs/indist/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "@tf.function(experimental_relax_shapes=True)\n",
    "def get_logits(x):\n",
    "    return student_model(x) \n",
    "  \n",
    "@tf.function(experimental_relax_shapes=True)\n",
    "def train_step(x,y):\n",
    "    with tf.GradientTape() as tape:\n",
    "       logits = student_model(x)\n",
    "       loss_value = student_model.loss(y_pred=logits, y_true=y)\n",
    "\n",
    "    grads = tape.gradient(loss_value, student_model.trainable_weights)\n",
    "    student_model.optimizer.apply_gradients(zip(grads, student_model.trainable_weights))\n",
    "    return loss_value\n",
    "\n",
    "@tf.function(experimental_relax_shapes=True)\n",
    "def get_probs(logits, y, tmp):\n",
    "    teacher_probs = tf.nn.softmax(logits/tmp, axis=-1)\n",
    "    sequence_mask = tf.cast(y != 0, dtype=tf.float32)\n",
    "    masked_teacher_probs = teacher_probs * sequence_mask[...,None] + tf.eye(tf.shape(teacher_probs)[-1])[0] * (1 - sequence_mask[...,None])\n",
    "\n",
    "    return masked_teacher_probs\n",
    "\n",
    "soft_targets = []\n",
    "inputs = []\n",
    "tmp = tf.constant(1, dtype=tf.float32)\n",
    "\n",
    "step = 0\n",
    "for  (x,y) in train_iter:\n",
    "    x = tf.convert_to_tensor(x, dtype=tf.int64)\n",
    "    y = tf.convert_to_tensor(x, dtype=tf.int64)\n",
    "    \n",
    "    teacher_logits = teacher_model(x)\n",
    "    masked_teacher_probs = get_probs(teacher_logits, y, tmp)\n",
    "    \n",
    "    loss_value = train_step(x,masked_teacher_probs)\n",
    "    # Log every 200 batches.\n",
    "    if step % 10 == 0:\n",
    "        print('Training loss (for one batch) at step %s: %s' % (step, float(loss_value)))\n",
    "        print('Seen so far: %s samples' % ((step + 1) * 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_probs = tf.nn.softmax(teacher_logits/1.0, axis=-1)\n",
    "print(teacher_probs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_probs = teacher_probs * sequence_mask[...,None] + tf.eye(tf.shape(teacher_probs)[-1])[0] * (1 - sequence_mask[...,None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_probs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masked_probs(logits, labels, temperature, padding_symbol=0):\n",
    "  teacher_probs = tf.nn.softmax(logits / temperature, axis=-1)\n",
    "  sequence_mask = tf.cast(labels != padding_symbol, dtype=tf.float32)\n",
    "  masked_teacher_probs = teacher_probs * sequence_mask[..., None] + tf.eye(tf.shape(teacher_probs)[-1])[0] * (\n",
    "      1 - sequence_mask[..., None])\n",
    "\n",
    "  return masked_teacher_probs\n",
    "\n",
    "def get_topk_mask(inputs, k):\n",
    "    values, indices = tf.nn.top_k(inputs, k=k, sorted=False)\n",
    "\n",
    "    temp_indices = tf.meshgrid(*[tf.range(d) for d in (tf.unstack(\n",
    "           tf.shape(inputs)[:(inputs.get_shape().ndims - 1)]) + [k])], indexing='ij')\n",
    "    temp_indices = tf.stack(temp_indices[:-1] + [indices], axis=-1)\n",
    "    full_indices = tf.reshape(temp_indices, [-1, inputs.get_shape().ndims])\n",
    "    values = tf.reshape(values, [-1])\n",
    "\n",
    "    mask_st = tf.SparseTensor(indices=tf.cast(\n",
    "          full_indices, dtype=tf.int64), values=tf.ones_like(values), dense_shape=inputs.shape)\n",
    "    mask = tf.sparse.to_dense(tf.sparse.reorder(mask_st))\n",
    "    \n",
    "    return mask\n",
    "\n",
    "def get_topk_masked_probs(logits, labels, temperature, k=100, padding_symbol=0):\n",
    "  topk_mask =(1 - tf.cast(get_topk_mask(logits, k), dtype=tf.float32)) * -10e8\n",
    "  teacher_probs = tf.nn.softmax((logits+topk_mask) / temperature, axis=-1)\n",
    "  sequence_mask = tf.cast(labels != padding_symbol, dtype=tf.float32)\n",
    "  masked_teacher_probs = teacher_probs * sequence_mask[..., None] + tf.eye(tf.shape(teacher_probs)[-1])[0] * (\n",
    "      1 - sequence_mask[..., None])\n",
    "\n",
    "  return masked_teacher_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.32073522 0.6911475  0.1812065  0.52984554 0.40381116 0.7097301\n",
      "  0.6959685  0.01675281]\n",
      " [0.8634111  0.3515953  0.78308344 0.03304483 0.11540707 0.8430617\n",
      "  0.70706254 0.7702668 ]\n",
      " [0.5482357  0.7089918  0.07130605 0.31061053 0.09298615 0.2160058\n",
      "  0.99783164 0.7269276 ]]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "logits = np.float32(np.random.random(size=(3,8)))\n",
    "print(logits)\n",
    "labels = np.float32(np.eye(8, dtype=np.float32)[0:3])\n",
    "print(labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.5050872  0.         0.         0.         0.         0.4949128\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.56731486 0.43268517]], shape=(3, 8), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "probs = get_topk_masked_probs(tf.convert_to_tensor(logits), tf.convert_to_tensor(np.argmax(labels, axis=-1)), 1.0, 2)\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(probs, axis=-1) == np.argmax(logits,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 4\n",
    "arr = tf.random.normal(shape=(5, 3, 8))\n",
    "values, indices = tf.nn.top_k(arr, k=K, sorted=False)\n",
    "\n",
    "temp_indices = tf.meshgrid(*[tf.range(d) for d in (tf.unstack(\n",
    "       tf.shape(arr)[:(arr.get_shape().ndims - 1)]) + [K])], indexing='ij')\n",
    "temp_indices = tf.stack(temp_indices[:-1] + [indices], axis=-1)\n",
    "full_indices = tf.reshape(temp_indices, [-1, arr.get_shape().ndims])\n",
    "values = tf.reshape(values, [-1])\n",
    "\n",
    "mask_st = tf.SparseTensor(indices=tf.cast(\n",
    "      full_indices, dtype=tf.int64), values=tf.ones_like(values), dense_shape=arr.shape)\n",
    "mask = tf.sparse.to_dense(tf.sparse.reorder(mask_st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=427, shape=(5, 3, 8), dtype=float32, numpy=\n",
       "array([[[0., 1., 1., 0., 1., 0., 0., 1.],\n",
       "        [0., 1., 0., 1., 0., 1., 0., 1.],\n",
       "        [0., 0., 1., 1., 1., 0., 0., 1.]],\n",
       "\n",
       "       [[0., 1., 1., 1., 0., 0., 0., 1.],\n",
       "        [0., 1., 1., 0., 0., 1., 1., 0.],\n",
       "        [1., 0., 1., 0., 1., 1., 0., 0.]],\n",
       "\n",
       "       [[1., 1., 0., 0., 1., 1., 0., 0.],\n",
       "        [0., 1., 0., 1., 1., 1., 0., 0.],\n",
       "        [1., 0., 1., 0., 1., 0., 0., 1.]],\n",
       "\n",
       "       [[0., 0., 1., 0., 1., 0., 1., 1.],\n",
       "        [0., 1., 1., 0., 0., 0., 1., 1.],\n",
       "        [0., 1., 1., 0., 0., 1., 1., 0.]],\n",
       "\n",
       "       [[1., 1., 0., 0., 1., 0., 0., 1.],\n",
       "        [1., 0., 0., 1., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 1., 0., 0., 1., 0.]]], dtype=float32)>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
